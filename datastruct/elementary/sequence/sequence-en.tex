\ifx\wholebook\relax \else
% ------------------------ 

\documentclass{article}
%------------------- Other types of document example ------------------------
%
%\documentclass[twocolumn]{IEEEtran-new}
%\documentclass[12pt,twoside,draft]{IEEEtran}
%\documentstyle[9pt,twocolumn,technote,twoside]{IEEEtran}
%
%-----------------------------------------------------------------------------
%\input{../../../common.tex}
\input{../../../common-en.tex}

\setcounter{page}{1}

\begin{document}

\fi
%--------------------------

% ================================================================
%                 COVER PAGE
% ================================================================

\title{Sequences, The last brick}

\author{Liu~Xinyu
\thanks{{\bfseries Liu Xinyu } \newline
  Email: liuxinyu95@gmail.com \newline}
  }

\markboth{Sequences}{AlgoXY}

\maketitle

\ifx\wholebook\relax
\chapter{Sequences, The last brick}
\numberwithin{Exercise}{chapter}
\fi

% ================================================================
%                 Introduction
% ================================================================
\section{Introduction}
\label{introduction}
In the first chapter of this book, which introduced binary search tree
as the `hello world' data structure, we mentioned that neither queue
nor array is simple if realized not only in imperative way, but also
in functional approach. In previous chapter, we explained functional
queue, which achieves the similar performance as its imperative counterpart.
In this chapter, we'll dive into the topic of array-like data structures.

We have introduced several data structures in this book so far, and 
it seems that functional approaches typically bring more expressive
and elegant solution. However, there are some areas, people haven't
found competitive purely functional solutions which can match the imperative
ones. For instance, the Ukkonen linear time suffix tree construction
algorithm. another examples is Hashing table. Array is also among them.

Array is trivial in imperative settings, it enables randomly accessing
any elements with index in constant $O(1)$ time. However, this performance
target can't be achieved directly in purely functional settings as
there is only list can be used.

In this chapter, we are going to abstract the concept of array to sequences.
Which support the following features

\begin{itemize}
\item Element can be inserted to or removed from the head of the sequence quickly in $O(1)$ time;
\item Element can be inserted to or removed from the head of the sequence quickly in $O(1)$ time;
\item Support concatenate two sequences quickly (faster than linear time);
\item Support randomly access and update any element quickly;
\item Support split at any position quickly;
\end{itemize}

We call these features abstract sequence properties, and it easy to see
the fact that even array (here means plain-array) in imperative settings
can't meet them all at the same time.

We'll provide three solutions in this chapter. Firstly, we'll introduce
a solution based on binary tree forest and numeric representation;
Secondly, we'll show a catenable list solution; Finally, we'll give
the finger tree solution.

Most of the results are based on Chris, Okasaki's work in \cite{okasaki-book}. 

% ================================================================
%                 Binary random access list
% ================================================================
\section{Binary random access list}
\index{Sequence!Binary random access list}

\subsection{Review of plain-array and list}

Let's review the performance of plain-array and singly linked-list so that we know
how they perform in different cases.

\begin{tabular}{l | c | r}
  \hline
  operation & Array & Linked-list \\
  \hline
  operation on head & $O(N)$ & $O(1)$ \\
  operation on tail & $O(1)$ & $O(N)$ \\
  access at random position & $O(1)$ & average $O(N)$ \\
  remove at given position & average $O(N)$ & $O(1)$ \\
  concatenate & $O(N_2)$ & $O(N_1)$ \\
  \hline
\end{tabular}

Because we hold the head of linked list, operations on head such as insert and remove perform
in constant time; while we need traverse to the end to perform remove or append on tail; Given
a position $i$, it need traverse $i$ elements to access it. Once we are in that position,
removing element from there is just bound to constant time by modifying some pointers. 
In order to concatenate two
linked-lists, we need traverse to the end of the first one, and link it to the second one, which
is bound to the length of the first linked-list;

On the other hand, for array, we must prepare free cell for insert a new element to the head of array, and
we need release the first cell after the first element is removed, all these two operations are
achieved by shifting all the rest elements forward or backward, which costs linear time. While the
operations on the tail of array are trivial constant time ones. Array also support access random
poistion $i$ by nature; However, remove the element at that position cause shifting all elements
after it one position ahead. In order to concatenate two arrays, we need copy all elements from the
second one to the end of the first one (ignore the memory re-allocation details), which is proportion
to the length of the second array.

In the chapter about binomial heaps, we have explained the idea of using forest, which is a list of trees
to design the data structure. It brings us the merit that, for any given number $N$, by representing it
in binary number, we know how many binomial trees need to hold them. That each bit of 1 represent a binomial
tree of that rank of bit. We can go one step ahead, if we have a $N$ nodes binomial heap, for any given
index $1 < i < N$, we can quickly know which binomial tree in the heap hold the $i$-th node.

\subsection{Represent sequence by trees}
\index{Binary Random Access List!Definition}

One solution to realize a random-access sequence is to manage the sequence with a forest of complete binary
trees. Figure \ref{fig:bi-tree-sequence} shows how we attach such trees to a sequence of numbers.

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=1.0]{img/bi-tree-sequence.ps}
  \caption{A sequence of 6 elements can be represented in a forest.} \label{fig:bi-tree-sequence}
\end{figure}

Here two trees $t_1$ and $t_2$ are used to represent sequence $\{x_1, x_2, x_3, x_4, x_5, x_6\}$. 
The size of binary tree $t_1$ is 2. The first two elements $\{x_1, x_2\}$ are leaves of $t_1$;
the size of binary tree $t_2$ is 4. The next foure elements $\{x_3, x_4, x_5, x_6\}$ are leaves
of $t_2$. 

For a complete binary tree, we define the depth as 0 if the tree only has a leaf, 
The tree is denoted as as $t_i$ if its depth is $i+1$. It's obvious that there
are $2^i$ leaves in $t_i$.

For any sequence contains $N$ elements, it can be turned to a forest of compelte binary trees in this manner.
First we represent $N$ in binary number like below.

\be
N = 2^0 e_0 + 2^1 e_1 + ... + 2^M e_M
\ee

Where $e_i$ is either 1 or 0, so $N=(e_M e_{M-1} ... e_1 e_0)_2$. If $e_i \neq 0$, we then need a 
complete binary tree with size $2^i$,  For example in figure \ref{fig:bi-tree-sequence}, as the
length of sequence is 6, which is $(110)2$ in binary. The lowest bit is 0, so we needn't a tree
of size 1; the second bit is 1, so we need a tree of size 2, which has depth of 2; the highest
bit is also 1, thus we need a tree of size 4, which has depth of 3.

This method represents the sequence $\{x_1, x_2, ..., x_N\}$ to a list of trees $\{t_0, t_1, ..., t_M\}$
where $t_i$ is either empty if $e_i = 0$ or a complete binary tree if $e_i = 1$.
We call this representaion as {\em Binary Random Access List}.

We can reused the definition of binary tree, for example, the following Haskell program defines
the binary random access list as well as tree.

\lstset{language=Haskell}
\begin{lstlisting}
data Tree a = Leaf a
            | Node Int (Tree a) (Tree a)  -- size, left, right

type BRAList a = [Tree a]
\end{lstlisting}

The only difference from the typical binary tree is that we augment the size information to the tree.
This enable us to get the size without calculation at every time. For instance.

\begin{lstlisting}
size (Leaf _) = 1
size (Node sz _ _) = sz
\end{lstlisting}

\subsection{Insertion to the head of the sequence}
\index{Binary Random Access List!Insertion}
The new forest representation of sequence enables many operation effectively. For example, the
operation of inserting a new element $y$ in front of sequence can be realized as the following.

\begin{enumerate}
\item Create a tree $t'$, with $y$ as the only one leaf;
\item Examine the first tree in the forest, compare its size with $t'$, if its size is greater than $t'$,
we just let $t'$ be the new head of the forest, since the forest is a linked-list of tree, insert
$t'$ to its head is trivial operation, which is bound to constant $O(1)$ time;
\item Otherwise, if the size of first tree in the forest is equal to $t'$, let's denote this tree
in the forest as $t_i$, we can construct a new binary tree $t'_{i+1}$ by linking $t_i$ and $t'$ as
its left and right children. After that, we recursively try to insert $t'_{i+1}$ to the forest.
\end{enumerate}

Figuer \ref{fig:bralist-1} and \ref{fig:bralist-2} illustrate the steps of inserting element $x_1, x_2, ..., x_6$ to a empty tree.

\begin{figure}[htbp]
  \centering
  \subfloat[A singleton leaf of $x_1$]{\hspace{0.2\textwidth}\includegraphics[scale=0.5]{img/bralst-1.ps}\hspace{0.2\textwidth}}
  \subfloat[Insert $x_2$. It causes linking, results a tree of height 1.]{\hspace{0.2\textwidth}\includegraphics[scale=0.5]{img/bralst-2.ps}\hspace{0.2\textwidth}} \\
  \subfloat[Insert $x_3$. the result is two trees, $t_1$ and $t_2$]{\hspace{0.2\textwidth}\includegraphics[scale=0.5]{img/bralst-3.ps}\hspace{0.2\textwidth}}
  \subfloat[Insert $x_4$. It first causes linking two leafs to a binary tree, then it performs linking again, which results a final tree of height 2.]{\includegraphics[scale=0.5]{img/bralst-4.ps}}
  \caption{Steps of inserting elements to an empty list, 1} \label{fig:bralist-1}
\end{figure}

\begin{figure}[htbp]
  \centering
  \subfloat[Insert $x_5$. The forest is a leaf ($t_0$) and $t_2$.]{\includegraphics[scale=0.5]{img/bralst-5.ps}}
  \subfloat[Insert $x_6$. It links two leaf to $t_1$. ]{\includegraphics[scale=0.5]{img/bralst-6.ps}} \\
  \caption{Steps of inserting elements to an empty list, 2} \label{fig:bralist-2}
\end{figure}

As there are at most $M$ trees in the forest, and $M$ is bound to $O(\lg N)$, so the insertion to
head algorithm is ensured to perform in $O(\lg N)$ even in worst case. We'll prove the amortized
performance is $O(1)$ later.

Let's formalize the algorithm to equations. we define the function of insert a element in front of
a sequence as $insert(S, x)$.

\be
insert(S, x) = insertTree(S, (leaf x))
\ee

This function just wrap element $x$ to a singleton tree with a leaf, and call $insertTree$ to insert
this tree to the forest. Suppose the forest $F=\{ t_1, t_2, ...\}$ if it's not empty, and $F' = \{ t_2, t_3, ...\}$
is the rest of trees without the first one.

\be
insertTree(F, t) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{ t \} & F = \Phi \\
  \{ t \} \cup F & size(t) < size(t_1) \\
  insertTree(F', link(t, t_1) & otherwise
  \end{array}
\right .
\ee

Where function $link$ create a new tree from two small trees with same size. Suppose function
$tree(s, t_1, t_2)$ create a tree, set its size as $s$, makes $t_1$ as the left child, and $t_2$ as
the right child, linking can be realized as below.

\be
link(t_1, t_2) = tree(size(t_1) + size(t_2), t_1, t_2)
\ee

The relative Haskell programs can be given by translating these equations.

\begin{lstlisting}
cons :: a -> BRAList a -> BRAList a
cons x ts = insertTree ts (Leaf x) 

insertTree :: BRAList a -> Tree a -> BRAList a
insertTree [] t = [t]
insertTree (t':ts) t = if size t < size t' then  t:t':ts
                       else insertTree ts (link t t')

-- Precondition: rank t1 = rank t2
link :: Tree a -> Tree a -> Tree a
link t1 t2 = Node (size t1 + size t2) t1 t2
\end{lstlisting}

Here we use the Lisp tradition to name the function that insert an element before a list as `cons'.

\subsubsection{Remove the element from the head of the sequence}

It's not complex to realize the inverse operation of `cons', which can remove element from the
head of the sequence.

\begin{itemize}
\item If the first tree in the forest is a singleton leaf, remove this tree from the forest;
\item otherwise, we can halve the first tree by unlinking its two children, so the first tree
in the forest becomes two trees, we recursively halve the first tree until it turns to be a 
leaf.
\end{itemize}

TODO: Add figures to illustrate the remove process step by step.

If we assume the sequence isn't empty, so that we can skip the error handling such as trying 
to remove an element from an empty sequence, this can be expressed with the following equation.
We denote the forest $F = \{t_1, t_2, ... \}$ and the trees without the first one as
$F' = \{ t_2, t_3, ...\}$

\be
extractTree(F) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (t_1, F') & t_1 {\quad} \mbox{is leaf} \\
  extractTree(\{t_l, t_r\} \cup F') & otherwise
  \end{array}
\right .
\ee

where $\{ t_l, t_r \} = unlink(t_1)$ are the two children of $t_1$.

It can be translated to Haskell programs like below.

\begin{lstlisting}
extractTree (t@(Leaf x):ts) = (t, ts)
extractTree (t@(Node _ t1 t2):ts) = extractTree (t1:t2:ts)
\end{lstlisting}

With this function defined, it's convenient to give `head' and `tail', the former returns
the first element in the sequence, the latter return the rest.

\be
head(S) = key(first(extractTree(S)))
\ee

\be
tail(S) = second(extractTree(S))
\ee

Where function `first' returns the first element in a paired-value (as known as tuple); 
`second' returns the second element respectively. function `key' is used to access the
elements inside a leaf. Below are Haskell programs corresponding to these two euqations.

\begin{lstlisting}
head' ts = x where (Leaf x, _) = extractTree ts
tail' = snd . extractTree
\end{lstlisting}

Note that as `head' and `tail' functions have already been defined in Haskell standard
library, we given them apostrophes to make them distinct. (another option is to hide
the standard ones by importing. We skip the details as they are language specific).

\subsubsection{Random access the element in binary}
\index{Binary Random Access List!Random access}

As trees in the forest help managing the elements in blocks, giving an arbitrary index,
it's easy to locate which tree this element are stored, after that performing a search 
in the tree yields the result. As all trees are binary (more accurate, complete binary
tree), the search is essentially binary search, which is bound to the logarithme
of the tree size. This brings us a faster random access capability than linear search
in linked-list setting.

Given an index $i$, and a sequence $S$, which is actually a forest of trees, the 
algorithm is executed as the following \footnote{We following the
tradition that the index $i$ starts from 1 in algorithm description; while it starts from
0 in most programming languages}.

\begin{enumerate}
\item Compare $i$ with the size of the first tree $T_1$ in the forest, if $i$ is 
less than or equal to the size, the element exists in $T_1$, perform looking up in $T_1$;
\item Otherwise, decrease $i$ by the size of $T_1$, and repeat the previous step
in the rest of the trees in the forest.
\end{enumerate}

This algorithm can be represented as the below equation.

\be
get(S, i) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  lookupTree(T_1, i) & i \leq size(T_1) \\
  get(S', i-size(T_1)) & otherwise
  \end{array}
\right .
\ee

Where $S' = \{ T_2, T_3, ... \}$ is the rest of trees without the first one in the
forest. Note that we don't handle out of bound error cases, this is left as an exercise
to the reader.

Function `lookupTree' is just a binary search, if the index $i$ is 1, we just return the root
of the tree, otherwise, we halve the tree by unlinking, if $i$ is less than or equal to the size
of the halved tree, we recursively perform looking up the left tree, otherwise, we
look up the right tree.

\be
lookupTree(T, i) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  root(T) & i = 1 \\
  lookupTree(left(T)) & i \leq \lfloor \frac{size(T)}{2} \rfloor \\
  lookupTree(right(T)) & otherwise
  \end{array}
\right .
\ee

Where function `left' returns the left tree $T_l$ of $T$, while `right` returns $T_r$.

The corresponding Haskell program is given as below.

\begin{lstlisting}
getAt (t:ts) i = if i < size t then lookupTree t i
                 else getAt ts (i - size t)

lookupTree (Leaf x) 0 = x
lookupTree (Node sz t1 t2) i = if i < sz `div` 2 then lookupTree t1 i
                               else lookupTree t2 (i - sz `div` 2)
\end{lstlisting}

Figure \ref{fig:get-at-example} illustrates the steps of looking up the 4-th element
in a sequence of size 6. It first examine the first tree, since the size is 2 which is 
smaller than 4, so it goes on looking up for the second tree for the 4-2, which is the
2nd element in the rest of the forest. As the size of the next tree is 4, which is 
greater than 2, so the element to be searched should be located in this tree.
It then examines the left sub tree since the new index 2 is not greater than the half size 4/2=2;
The process next visit the right grand-child, and the final result is returned.

\begin{figure}[htbp]
  \centering
  \subfloat[$getAt(S, 4))$, $4 > size(t_1) = 2$]{\includegraphics[scale=0.5]{img/bralst-6.ps}}
  \subfloat[$getAt(S', 4-2) \Rightarrow lookupTree(t_2, 2)$]{\hspace{0.2\textwidth}\includegraphics[scale=0.5]{img/bralst-4.ps}\hspace{0.2\textwidth}} \\
  \subfloat[$ 2 \leq \lfloor size(t_2)/2 \rfloor \Rightarrow lookupTree(left(t_2), 2)$]{\hspace{0.2\textwidth}\includegraphics[scale=0.5]{img/bralst-4l.ps}\hspace{0.2\textwidth}}
  \subfloat[$lookupTree(right(left(t_2)), 1)$, $x_3$ is returned.]{\hspace{0.2\textwidth}\includegraphics[scale=0.5]{img/bralst-4lr.ps}\hspace{0.2\textwidth}}
  \caption{Steps of locating the 4-th element in a sequence.} \label{fig:get-at-example}
\end{figure}

By using the similar idea, we can update element at any arbitrary position $i$.
We first compare the size of the first tree $T_1$ in the forest with $i$, if it is
less than $i$, it means the element to be updated doesn't exist in the first
tree. We recurisvely examine the next tree in the forest, comparing it with
$i - |T_1|$, where $|T_1|$ represents the size of the first tree. Otherwise
if this size is greater than or equal to $i$, the element is in the tree, 
we halve the tree recursively until to get a leaf, at this stage, we can
replace the element of this leaf with a new one.

\be
set(S, i, x) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{ updateTree(T_1, i, x) \} \cup S' & i < |T_1| \\
  \{T_1\} \cup set(S', i - |T_1|, x)
  \end{array}
\right .
\ee

Where $S' = \{ T_2, T_3, ...\}$ is the rest of the trees in the forest without
the first one.

Function $setTree(T, i, x)$ performs a tree search and replace the $i$-th element
with the given value $x$.

\be
setTree(T, i, x) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  leaf(x) & i = 0 \land |T| = 1 \\
  tree(|T|, setTree(T_l, i, x), t_r) & i < \lfloor \frac{|T|}{2} \rfloor \\
  tree(|T|, t_l, setTree(T_r, i - \lfloor \frac{|T|}{2} \rfloor, x))
  \end{array}
\right .
\ee

Where $T_l$ and $T_r$ are left and right sub tree of $T$ respectively. The following
Haskell program translates the equation accordingly.

\begin{lstlisting}
setAt :: BRAList a -> Int -> a -> BRAList a
setAt (t:ts) i x = if i < size t then (updateTree t i x):ts
                   else t:setAt ts (i-size t) x

updateTree :: Tree a -> Int -> a -> Tree a
updateTree (Leaf _) 0 x = Leaf x
updateTree (Node sz t1 t2) i x = 
    if i < sz `div` 2 then Node sz (updateTree t1 i x) t2
    else Node sz t1 (updateTree t2 (i - sz `div` 2) x)
\end{lstlisting}

As the nature of complete binary search tree, for a sequence with $N$ elements, which
is represented by binary random
access list, the number of trees in the forest is bound to $O(\lg N)$. Thus it takes
$O(\lg N)$ time to locate the tree for arbitrary index $i$, that contains the element.
the followed tree seach is bound the heights of the tree, which is $O(\lg N)$ as well.
So the total performance of random access is $O(\lg N)$. 

\begin{Exercise}
\begin{enumerate}
\item The random access algorithm given in this section doesn't handle the error such as
out of bound index at all. Modify the algorithm to handle these cases, and implement
it in your favorate programming langauge.

\item It's quite possible to realize the binary random access list in imperative settings,
which is benefited with fast operation on the head of the sequence. the random access
can be realized in two steps: firstly locate the tree, secondly use the capability of
constant random access of array. Write a program to implement it in your favorate imperative
programming language.
\end{enumerate}
\end{Exercise}

\section{Numeric representation for binary random access list}
\index{Sequence!numeric representation for binary access list}
In previous section, we mentioned that for any sequence with $N$ elements, we can 
represent $N$ in binary fomat so that $N = 2^0e_0 + 2^1e_1 + ... + 2^Me_M$. Where $e_i$
is the $i$-th bit, which can be either 0 or 1. If $e_i \neq 0$ it means that there is
a complete binary tree with size $2^i$.

This fact indicates us that there is an explicit relationship between the binary
form of $N$ and the forest. Insertion a new element on the head can be simulated
by increasing the binary number by one; while remove an element from the head mimics
the decreasing of the coresponding binary number by one. This is as known as 
{\em numeric representation} \cite{okasaki-book}.

In order to represent the binary access list with binary number, we can define two
states for a bit. That $Zero$ means there is no such a tree with size which is conresponding
to the bit, while $One$, means such tree exists in the forest. And we can attach
the tree with the state if it is $One$.

The following Haskell program for instance defines such states.

\begin{lstlisting}
data Digit a = Zero
             | One (Tree a)

type RAList a = [Digit a]
\end{lstlisting}

Here we reuse the definition of complete binary tree and attach it to the state $One$.
Note that we cached the size information in the tree as well.

With digit defined, forest can be treated as a list of digits. Let's see how insert
a new element can be realized as binary number increasing. Suppose function $one(t)$
creates a $One$ state and attaches tree $t$ to it. And function $getTree(s)$ get the
tree which is attached to the $One$ state $s$. 
The sequence $S$ is a list of digit
of state that $S = \{ s_1, s_2, ... \}$, and $S'$ is the rest of digits with the first
one removed.

\be
insertTree(S, t) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{ one(t) \} & S = \Phi \\
  \{ one(t) \} \cup S' & s_1 = Zero \\
  \{ Zero \} \cup insertTree(S', link(t, getTree(s_1))) & otherwise
  \end{array}
\right .
\ee

When we insert a new tree $t$ to a forest $S$ of binary digits, If the forest is empty, we 
just create a $One$ state, attach the tree to it, and make this state the only 
digit of the binary number. This is just like $0 + 1 = 1$; 

Otherwise if the forest isn't empty, we need examine the first digit of the binary number. If the first digit
is $Zero$, we just create a $One$ state, attach the tree, and replace the $Zero$ state
with the new created $One$ state. This is just like $(...digits...0)_2 + 1 = (...digits...1)_2$.
For example $6 + 1 = (110)_2 + 1 = (111)_2 = 7$. 

The last case is that the first
digit is $One$, here we make assumption that the tree $t$ to be inserted has the 
same size with the tree attached to this $One$ state at this stage. 
This can be ensured by calling this function from inserting a leaf, so that the size
of the tree to be inserted grows in a series of $1, 2, 4, ..., 2^i, ...$. In such
case, we need link these two trees (one is $t$, the other is the tree attached to 
the $One$ state), and recursively insert the linked result to the rest of the digits.
Note that the previous $One$ state has to be replaced with a $Zero$ state. This
is just like $(...digits...1)_2 + 1 = (...digits'...0)_2$, where $(...digits'...)_2 = (...digits...)_2+1$.
For example $7 + 1 = (111)_2 + 1 = (1000)_2 = 8$

Translating this algorithm to Haskell yeilds the following program.

\begin{lstlisting}
insertTree :: RAList a -> Tree a -> RAList a
insertTree [] t = [One t]
insertTree (Zero:ts) t = One t : ts
insertTree (One t' :ts) t = Zero : insertTree ts (link t t')
\end{lstlisting}

All the other functions, including $link(), cons()$ etc. are as same as before.

Next let's see how remove element from a seqence can be represented as binary number
deduction. If the sequence is a singleton $One$ state attached with a leaf.
After removal, it becomes empty. This is just like $1 - 1 = 0$;

Otherwise, we examine the first digit, if
it is $One$ state, it will be replaced with a $Zero$ state to indicate that
this tree will be no longer exist in the forest as it being removed.
This is just like $(...digits...1)_2 - 1 = (...digits...0)_2$. For example
$7 - 1 = (111)_2 - 1 = (110)_2 = 6$;

If the first digit in the sequence is a $Zero$ state, we have to borrow
from the further digits for removal. We recursively extract a tree from
the rest digits, and halve the extracted tree to its two children. Then
the $Zero$ state will be replaced with a $One$ state attached with the 
right children, and the left children is removed. This is something like
$(...digits...0)_2 - 1 = (...digits'...1)_2$, where 
$(...digits'...)_2 = (...digits)_2 - 1$. For example 
$4 - 1 = (100)_2 - 1 = (11)_2 = 3$.The following equation
illustrated this algorithm.

\be
extractTree(S) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (t, \Phi) & S = \{ one(t) \} \\
  (t, S') & s_1 = one(t) \\
  (t_l, \{ one(t_r) \} \cup S'' & otherwise
  \end{array}
\right .
\ee

Where $(t', S'') = extractTree(S')$, $t_l$ and $t_r$ are left and right
sub-trees of $t'$. All other functions, including $head()$, $tail()$ are
as same as before.

Numeric representation doesn't change the performance of binary random
access list, readers can refer to \cite{okasaki-ralist} for detailed
discussion. Let's take for example, analyze the average performance (or amortized) of insertion
on head algorithm by using aggregation analysis.

Considering the process of inserting $N = 2^m$ elements to an empty binary random access list.
The numeric representation of the forest can be listed as the following.

\begin{tabular}{l | r}
  \hline
  i & forest (MSB ... LSB) \\
  \hline
  0 & 0, 0, ..., 0, 0 \\
  1 & 0, 0, ..., 0, 1 \\
  2 & 0, 0, ..., 1, 0 \\
  3 & 0, 0, ..., 1, 1 \\
  ... & ... \\
  $2^m-1$ & 1, 1, ..., 1, 1 \\
  $2^m$ & 1, 0, 0, ..., 0, 0 \\
  \hline
  bits changed & 1, 1, 2, ... $2^{m-1}$. $2^m$ \\
  \hline
\end{tabular}

The LSB of the forest changed every time when there is a new element inserted,
it costs $2^m$ units of computation; The next bit changes every two times due
to a linking operation, so it costs $2^{m-1}$ untis; the bit next to MSB of 
the forest changed only one time which links all previous trees to a big tree 
as the only one in the forest. This happens at the half time of the total
insertion process, and after the last element is inserted, the MSB flips to 1.

Sum these costs up yield to the total cost $T = 1 + 1 + 2 + 3 + ... + 2^{m_1} + 2^m = 2^{m+1}$
So the average cost for one insertion is

\be
O(T/N) = O(\frac{2^{m+1}}{2^m}) = O(1)
\ee

Which proves that the insertion algorithm performs in amortized $O(1)$ constant time.
The proof for deletion are left as an exercise to the reader.

\subsection{Imperative binary access list}

It's trivial to implement the binary access list by using binary trees, and the
recusion can be eliminated by update the focused tree in loops. This is left
as an excercise to the reader. In this section, we'll show some different imperative
implementations by using the properties of numeric representation.

Remind the chapter about binary heap, binar heaps, which can be represented by 
implicit array. We can use similar approach that use an array of 1 element to 
represent the leaf; use an array of 2 elements to represent a binary tree of height 1;
and use an array of $2^m$ to represent a complete binary tree of height $m$.

This brings us the capability of access any element with index directly instead
of divide and conquer tree search. However, the tree linking operation has to
be implemented as array copying as the expense.

The following ANSI C code defines such a forest.

\lstset{language=C}
\begin{lstlisting}
#define M 32      
typedef int Key;  

struct List {
  int n;
  Key* tree[M];
};
\end{lstlisting}

Where $n$ is the number of the elemenets stored in this forest.
Of course we can't avoid limiting the max number of trees by using dynamic arrays, for
example as the following ISO C++ code.

\lstset{language=C++}
\begin{lstlisting}
template<tyename Key>
struct List {
  int n;
  vector<vector<key> > tree;
};
\end{lstlisting}

For illustration purpose only, we use ANSI C here, readers can find the complete ISO C++
example programs along with this book.

Let's review the insertion process, if the first tree is empty (a $Zero$ digit), we simply
set the first tree as a leaf of the new element to be inserted; otherwise, the insertion
will cause tree linking anyway, and such linking may be recursive until it reach a
position (digit) that the coresponding tree is empty. The numeric representation
reveals an important fact that if the first, second, ..., $i-1$-th trees are all
exist, and the $i$-th tree is empty, the result is creating a tree of size $2^i$,
and all the elements togehter with the new element to be inserted are stored in
this new created tree. What's more, all trees after position $i$ are kept same
as before.

Is there any good methods to locate this $i$ position? As we can use binary number
to represent the forest of $N$ element, after a new element is inserted, $N$
increased to $N+1$. Compare the binary form of $N$ and $N+1$, we find that
all bits before $i$ change from 1 to 0, the $i$-th bit flip from 0 to 1, and
all the bits after $i$ keep same. So we can use bitwise exclusive or ($\oplus$) to detect 
this bit. Here is the algorithm.

\begin{algorithmic}
\Function{Number-Of-Bits}{$N$}
  \State $i \gets 0$
  \While{$\lfloor \frac{N}{2} \rfloor \neq 0$}
    \State $ N \gets \lfloor \frac{N}{2} \rfloor \neq 0 $
    \State $ i \gets i \times 2$
  \EndWhile
  \State \Return $i$
\EndFunction
\Statex
\State $i \gets $ \Call{Number-Of-Bits}{$N \oplus (N + 1)$}
\end{algorithmic}

And it can be easily implemented with bit shifting, for example the below ANSI C
code.

\begin{lstlisting}
int nbits(int n) {
  int i=0;
  while(n >>= 1)
    ++i;
  return i;
}
\end{lstlisting}

So the imperative insertion algorithm can be realized by first locating the bit which
flip from 0 to 1, then creating a new array of size $2^i$ to represent a complete 
binary tree, and moving content of all trees before this bit to this array as well
as the new element to be inserted.

\begin{algorithmic}
\Function{Insert}{$L, x$}
  \State $i \gets $ \Call{Number-Of-Bits}{$N \oplus (N + 1)$}
  \State \Call{Tree}{$L$}[$i+1$] $\gets $ \Call{Create-Array}{$2^i$}
  \State $l \gets 1$
  \State  \Call{Tree}{$L$}[$i+1$][$l$]  $\gets x$
  \For{$j \in [1, i]$}
    \For{$k \in [1, 2^j]$}
      \State $l \gets l + 1$
      \State \Call{Tree}{$L$}[$i+1$][$l$]  $\gets$ \Call{Tree}{$L$}[$j$][$k$]
    \EndFor
    \State \Call{Tree}{$L$}[$j$] $\gets$ NIL
  \EndFor
  \State \Call{Size}{$L$} $\gets$ \Call{Size}{$L$} + 1
  \State \Return $L$
\EndFunction
\end{algorithmic}

The corresponding ANSI C program is given as the following.

\begin{lstlisting}
struct List insert(struct List a, Key x) {
  int i, j, sz;
  Key* xs;
  i = nbits( (a.n+1) ^ a.n ); 
  xs = a.tree[i] = (Key*)malloc(sizeof(Key)*(1<<i));
  for(j=0, *xs++ = x, sz = 1; j<i; ++j, sz <<= 1) {
    memcpy((void*)xs, (void*)a.tree[j], sizeof(Key)*(sz));
    xs += sz;
    free(a.tree[j]);
    a.tree[j] = NULL;
  }
  ++a.n;
  return a;
}
\end{lstlisting}

However, the performance in theory isn't as good as before. This is because the 
linking opreation downgrade from $O(1)$ constant time to linear array copying.

We can again calculate the average (amortized) performance by using aggregation 
analysis. When insert $N = 2^m$ elements to an empty list which is represented
by implicit binary trees in arrays, the numeric presentation of the forest of
arrays are as same as before except for the cost of bit flipping.

\begin{tabular}{l | r}
  \hline
  i & forest (MSB ... LSB) \\
  \hline
  0 & 0, 0, ..., 0, 0 \\
  1 & 0, 0, ..., 0, 1 \\
  2 & 0, 0, ..., 1, 0 \\
  3 & 0, 0, ..., 1, 1 \\
  ... & ... \\
  $2^m-1$ & 1, 1, ..., 1, 1 \\
  $2^m$ & 1, 0, 0, ..., 0, 0 \\
  \hline
  bit change cost & $1 \times 2^m$, $1 \times 2^{m-1}$, $2 \times 2^{m-2}$, ... $2^{m-2} \times 2$, $2^{m-1} \times 1$ \\
  \hline
\end{tabular}

The LSB of the forest changed every time when there is a new element inserted, however,
it creates leaf tree and performs copying only it changes from 0 to 1, so the
cost is half of $N$ unit, which is $2^{m-1}$;
The next bit flips as half as the LSB. Each time the bit
gets flipped to 1, it copies the first tree as well as the new element to the second tree.
the the cost of fliping a bit to 1 in this bit is 2 units, but not 1; For the MSB, it only
flips to 1 at the last time, but the cost of flipping this bit, is copying all the previous
trees to fill the array of size $2^m$.

Suming all to cost and distributing them to the $N$ times of insertion yield the
amortized performance as below.

\be
\begin{array}{rcl}
O(T/N) & = & O(\frac{1 \times 2^m + 1 \times 2^{m-1} + 2 \times 2^{m-2} + ... + 2^{m-1} \times 1}{2^m}) \\
       & = & O(1 + \frac{m}{2}) \\
       & = & O(m)
\end{array}
\ee

As $m = O(\lg N)$, so the amortized performance downgrade from constant time to logarithm,
although it is still faster than the normal array insertion which is $O(N)$ in average.

The random accessing gets a bit faster because we can use array indexing instead of tree
search.

\begin{algorithmic}
\Function{Get}{$L, i$}
  \For{each $t \in $ \Call{Trees}{$L$}}
    \If{$t \neq$ NIL}
      \If{$i \leq $ \Call{Size}{$t$}}
        \State \Return $t$[i]
      \Else
        \State $i \gets i -$ \Call{Size}{$t$}
      \EndIf
    \EndIf
  \EndFor
\EndFunction
\end{algorithmic}

Here we skip the error handling such as out of bound indexing etc. The ANSI C program
of this algorithm is like the following.

\begin{lstlisting}
Key get(struct List a, int i) {
  int j, sz;
  for(j = 0, sz = 1; j < M; ++j, sz <<= 1)
    if(a.tree[j]) {
      if(i < sz)
	break;
      i -= sz;
    }
  return a.tree[j][i];
}
\end{lstlisting}

The imperative removal and random mutating algorithms are left as exercises to the reader.

\begin{Exercise}
\begin{enumerate}
\item Please implement the random access algorithms, including looking up and updating,
for binary random access list with numeric representation in your favorate programming
language.

\item Prove that the amortized performance of deletion is $O(1)$ constant time by
using aggregation analysis.

\item Design the implement the binary random access list by implicit array in your
favorate imperative programming language.
\end{enumerate}
\end{Exercise}

\section{Imperative paird-array list}
\index{Sequence!Paired-array list}

\subsection{Definition}
In previous chapter about queue, a symmetric solution of paired-array is presented.
It is capable to operate on both end of the list. Because the nature of array supports
fast random access. It can be also used to realize a fast random access sequence
in imperative setting.

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=1.0]{img/palist.ps}
  \caption{A paired-array list, which is consist of 2 arrays linking in head-head manner.} \label{fig:palist}
\end{figure}

Figure \ref{fig:palist} shows the design of paired-array list. Tow arrays are linked in head-head manner.
To insert a new element on the head of the sequence, the element is appended at the end of front list;
To append a new element on the tail of the sequence, the element is appended at the end of rear list;

Here is a ISO C++ code snippet to define the this data structure.

\lstset{language=C++}
\begin{lstlisting}
template<typename Key>
struct List {
  int n, m;
  vector<Key> front;
  vector<Key> rear;

  List() : n(0), m(0) {}
  int size() { return n + m; }
};
\end{lstlisting}

Here we use vector provides in standard library to cover the dynamic memory management issues, so
that we can concentrate on the algorithm design.

\subsection{Insertion and appending}
Suppose function \textproc{Front}($L$) returns the front array, while \textproc{Rear}($L$) returns the 
rear array. For illustration purpose, we assume the arrays are dynamic allocated. inserting and appending
can be realized as the following.

\begin{algorithmic}
\Function{Insert}{$L, x$}
  \State $F \gets $ \Call{Front}{$L$}
  \State \Call{Size}{$F$} $\gets $ \Call{Size}{$F$} + 1
  \State $F$[\Call{Size}{$F$}] $\gets x$
\EndFunction
\Statex
\Function{Append}{$L, x$}
  \State $R \gets $ \Call{Rear}{$L$}
  \State \Call{Size}{$R$} $\gets $ \Call{Size}{$R$} + 1
  \State $R$[\Call{Size}{$R$}] $\gets x$
\EndFunction
\end{algorithmic}

As all the above operations manipulate the front and rear array on tail, they are all constant $O(1)$ time. And the following are the corresponding ISO C++ programs.

\begin{lstlisting}
template<typename Key>
void insert(List<Key>& xs, Key x) {
  ++xs.n;
  xs.front.push_back(x);
}

template<typename Key>
void append(List<Key>& xs, Key x) {
  ++xs.m;
  xs.rear.push_back(x);
}
\end{lstlisting}

\subsection{random access}
As the inner data structure is array (dynamic array as vector), which support random access
by nature, it's trivial to implement constant time indexing algorithm. 

\begin{algorithmic}
\Function{Get}{$L, i$}
  \State $F \gets $ \Call{Front}{$L$}
  \State $N \gets $ \Call{Size}{$F$}
  \If{$i \leq N $}
    \State \Return $F$[$N-i+1$]
  \Else
    \State \Call{Rear}{$L$}[$i-N$]
  \EndIf
\EndFunction
\end{algorithmic}

Here the index $i \in [1, |L|]$ starts from 1. If it is not greater than the size of front
array, the element is stored in front. However, as front and rear arrays are connect head-to-head,
so the elements in front array are in reverse order. We need locate the element by substract
the size of front array by $i$; If the index $i$ is greater than the size of front array,
the element is stored in rear array. Since elements are stored in normal order in rear,
we just need substract the index $i$ by an offset which is the size of front array.

Here is the ISO C++ program implements this algorithm.

\begin{lstlisting}
template<typename Key>
Key get(List<Key>& xs, int i) {
  if( i < xs.n )
    return xs.front[xs.n-i-1];
  else
    return xs.rear[i-xs.n];
}
\end{lstlisting}

The random mutating algorithm is left as an exercise for the reader.

\subsection{removing and balancing}

Removing isn't as simple as insertion and appending. This is because we must handle the 
condition that one array (either front or rear) becomes empty due to removal, while the
other still contains elements. In extreme case, the list turns to be quite unbalanced.
So we must fix it to resume the balance.

One idea is to trigger this fixing when either front or rear array becomes empty. We just
cut the other array in half, and reverse the first half to form the new pair. The algorithm
is decribed as the following.

\begin{algorithmic}
\Function{Balance}{$L$}
  \State $F \gets$ \Call{Front}{$L$}, $R \gets$ \Call{Rear}{$L$}
  \State $N \gets$ \Call{Size}{$F$}, $M \gets$ \Call{Size}{$R$}
  \If{ $F = \Phi$}
    \State $F \gets$ \Call{Reverse}{$R$[1 ... $\lfloor \frac{M}{2} \rfloor$]}
    \State $R \gets R[\lfloor \frac{M}{2} \rfloor + 1 ... M]$
  \ElsIf{ $R = \Phi$ }
    \State $R \gets$ \Call{Reverse}{$F$[1 ... $\lfloor \frac{N}{2} \rfloor$]}
    \State $F \gets F[\lfloor \frac{N}{2} \rfloor + 1 ... N]$
  \EndIf
\EndFunction
\end{algorithmic}

Actually, the operations are symmetric for the case front is empty and the case that
rear is empty. Another approach is to swap the front and rear for one symmetric case
and recursive resumes the balance, then swap the front and rear back. For example 
the below ISO C++ program use this method.

\begin{lstlisting}
template<typename Key>
void balance(List<Key>& xs) {
  if(xs.n == 0) {
    back_insert_iterator<vector<Key> > i(xs.front);
    reverse_copy(xs.rear.begin(), xs.rear.begin() + xs.m/2, i);
    xs.rear.erase(xs.rear.begin(), xs.rear.begin() +xs.m/2);
    xs.n = xs.m/2;
    xs.m -= xs.n;
  }
  else if(xs.m == 0) {
    swap(xs.front, xs.rear);
    swap(xs.n, xs.m);
    balance(xs);
    swap(xs.front, xs.rear);
    swap(xs.n, xs.m);
  }
}
\end{lstlisting}

With \textproc{Balance} algorithm defined, it's trivial to implement remove algorithm
both on head and on tail.

\begin{algorithmic}
\Function{Remove-Head}{$L$}
  \State \Call{Balance}{$L$}
  \State $F \gets $ \Call{Front}{$L$}
  \If{$F = \Phi$}
    \State \Call{Remove-Tail}{$L$}
  \Else
    \State \Call{Size}{$F$} $\gets $ \Call{Size}{$F$} - 1
  \EndIf
\EndFunction
\Statex
\Function{Remove-Tail}{$L$}
  \State \Call{Balance}{$L$}
  \State $R \gets $ \Call{Rear}{$L$}
  \If{$R = \Phi$}
    \State \Call{Remove-Head}{$L$}
  \Else
    \State \Call{Size}{$R$} $\gets $ \Call{Size}{$R$} - 1
  \EndIf
\EndFunction
\end{algorithmic}

There is an edge case for each, that is even after balancing, the array targeted to 
perform removal is still empty. This happens that there is only one element stored 
in the paired-array list. The solution is just remove this singleton left element,
and the overall list results empty. Below is the ISO C++ program implements this 
algorithm.

\begin{lstlisting}
template<typename Key>
void remove_head(List<Key>& xs) {
  balance(xs);
  if(xs.front.empty())
    remove_tail(xs); //remove the singleton elem in rear
  else {
    xs.front.pop_back();
    --xs.n;
  }
}

template<typename Key>
void remove_tail(List<Key>& xs) {
  balance(xs);
  if(xs.rear.empty())
    remove_head(xs); //remove the singleton elem in front
  else {
    xs.rear.pop_back();
    --xs.m;
  }
}
\end{lstlisting}

It's obvious that the worst case performance is $O(N)$ where $N$ is the number of elements
stored in paired-array list. This happens when balancing is triggered, and both reverse
and shifting are linear operation. However, the amortized performance of removal is still
$O(1)$, the proof is left as exercise to the reader.

\begin{Exercise}
\begin{enumerate}
\item Implement the random mutating algorithm in your favorate imperative programming language.
\item We utilized vector provided in standard library to manage memory dynamically, try to realize
a version using plain array and manage the memory allocation manually. Compare this version and
consider how does this affect the performance?
\item Prove that the amortized performance of removal is $O(1)$ for paired-array list.
\end{enumerate}
\end{Exercise}

\section{Concatenate-able list}
\index{Sequence!Concatenate-able list}
By using binary random access list, we realized sequence data structure which
support $O(\lg N)$ time insertion and removal on head, as well as random accessing element 
with a given index.

However, it's not so easy to concatenate two lists. As both lists are forests of
complete binary trees, we can't merely merge them (Since forests are essentailly
list of trees, and for any size, there is at most one tree of that size. Even
cacatenate forests directly is not fast). One solution is to push the element
from the first one by one to a stack and then pop those elements and insert
them to the head of the second one by using `cons' function. Of course the
stack can be implicitly used in recursion manner, for instance:

\be
concat(s_1, s_2) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  s_2 & s_1 = \Phi \\
  cons(head(s_1), concat(tail(s_1), s_2)) & otherwise
  \end{array}
\right .
\ee

Where function $cons()$, $head()$ and $tail()$ are defined in previous section.

If the length of the two sequence is $N$, and $M$, This method takes $O(N \lg N)$
time repeatedly push all elements from the first sequence to stacks, and then
takes $\Omega(N \lg (N + M))$ to insert the elements in front of the second sequence.
Note that $\Omega$ means the upper limit, There is detailed definiton for it in 
\cite{CLRS}.

We have already implemented the real-time queue in previous chapter. It supports
$O(1)$ time pop and push. If we can turn the sequence concatenation to a kind
of pushing operation to queue, the performance will be improved to $O(1)$ as well.
Okasaki gave such realization in \cite{okasaki-book}, which can concatenate
lists in constant time.

To represent a concatenate-able list, the data structure designe by Okasaki is 
essentially a K-ary tree. The root of the tree stores the first elements in the 
list. So that we can access it in constant $O(1)$ time. The sub-trees or children
are all small concatenate-able lists, which is managed by a real-time queue.
Concatenating another list to the end is just adding it as the last child, which
is in turn a queue pushing operation. Appending a new element can be realized
as that, first wrapping the element to a singleton tree, which is a leaf with
no children. Then, concatenate this singleton to finalize appending.

Figure \ref{fig:clist} illustrates this data structure.

\begin{figure}[htbp]
  \centering
  \subfloat[The data structure for list $\{ x_1, x_2, ..., x_n\}$]{\includegraphics[scale=0.5]{img/clist.ps}} \\
  \subfloat[The result after concatenated with list $\{y_1, y_2, ..., y_m\}$]{\includegraphics[scale=0.5]{img/clist1.ps}}
  \caption{Data structure for concatenate-able list} \label{fig:clist}
\end{figure}

Such recurisvely designed data structure can be defined in the following
Haskell code.

\lstset{language=Haskell}
\begin{lstlisting}
data CList a = Empty | CList a (Queue (CList a)) deriving (Show, Eq)
\end{lstlisting}

It means that a concatenate-able list is either empty or a K-ary tree, which
again consists of a queue of concatenate-able sub-lists and a root element.
Here we reuse the realization of real-time queue mentioned in previous
chapter.

Suppose function $clist(x, Q)$ constructs a concatenate-able list from
an element $x$, and a queue of sub-lists $Q$. While function $root(s)$ 
returns the root element of such K-ary tree implemented list. and
function $queue(s)$ returns the queue of sub-lists respectively.
We can implement the algorithm
to concat two lists like this.

\be
concat(s_1, s_2) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  s_1 & s_2 = \Phi \\
  s_2 & s_1 = \Phi \\
  clist(x, push(Q, s_2)) & otherwise
  \end{array}
\right .
\ee

Where $x = root(s_1)$ and $Q = queue(s_1)$. The idea of concatenation is that
if either one of the list to be concatenated is empty, the result is 
just the other list; otherwise, we push the second list as the last
child to the queue of the first list.

Since the push operation is $O(1)$ constant time for a well realized
real-time queue, the performance of concatenation is bound to $O(1)$.

The $concat()$ function can be translated to the below Haskell program.

\begin{lstlisting}
concat x Empty = x
concat Empty y = y
concat (CList x q) y = CList x (push q y)
\end{lstlisting}

Besides the good performance of concatenation, this design also brings
satisified featurs for adding element both on head and tail.

\be
cons(x, s) = concat(clist(x, \Phi), s)
\ee

\be
append(s, x) = concat(s, clist(x, \Phi))
\ee

It's a bit complex to realize the algorithm that removes the first
element from a concatenate-able list. This is because after the root,
which is the first element in the sequence got removed, we have to 
re-construct the rest things, a queue of sub-lists, to a K-ary
tree.

Before diving into the re-construction, let's solve the trivial
part first. Getting the first element is just returning the root
of the K-ary tree.

\be
head(s) = root(s)
\ee

As we mentioned above, after root being removed, there left
all children of the K-ary tree. Note that all of them are also
concatenate-able list, so that one naturual solution is to
concatenate them all together to a big list. 

\be
concatAll(Q) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & Q = \Phi \\
  concat(front(Q), concatAll(pop(Q))) & otherwise
  \end{array}
\right .
\ee

Where function $front()$ just return the first element from a
queue without removing it, while $pop()$ doest the removing work.

If the queue is empty, it means that there is no children at all, so
the result is also an empty list; Otherwise, we pop the first child,
which is a concatenate-able list, from the queue, and recursively
concat all the rest children to a list; finally, we concat this list
behind the already popped first children.

With $concatAll()$ defined, we can then implement the algorithm 
of removing the firest element from a list as below.

\be
tail(s) = linkAll(queue(s))
\ee

The corresponding Haskell program is given like the following.

\begin{lstlisting}
head (CList x _) = x 
tail (CList _ q) = linkAll q

linkAll q | isEmptyQ q = Empty
          | otherwise = link (front q) (linkAll (pop q))
\end{lstlisting}

Function `isEmptyQ' is used to test a queue is empty, it is trivial and 
we omit its definition. Readers can refer to the source code along with
this book.

$linkAll()$ algorithm actually traverses the queue data structure,
and reduces to a final result. This remind us of {\em folding} mentioned
in the chapter of binary search tree. readers can refer to the
appendix of this book for the detailed description of folding.
It's quite possible to define a folding algorithm for queue instead
of list\footnote{Some function programming languageg, such as 
Haskell, deinfed type class, which is a concept of monoid so that
it's easy to support folding on a customized data structure.}
\cite{learn-haskell}.

\be
foldQ(f, e, Q) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  e & Q = \Phi \\
  f(front(Q), foldQ(f, e, pop(Q))) & otherwise
  \end{array}
\right .
\ee

Function $foldQ()$ takes three parameters, a function $f$, which is used
for reducing, an initial value $e$, and the queue $Q$ to be traversed.

Here are some examples to illustrate folding on queue. Suppose
a queue $Q$ contains elements $\{ 1, 2, 3, 4, 5 \}$ from head to tail.

\[
\begin{array}{l}
foldQ(+, 0, Q) = 1 + (2 + (3 + (4 + (5 + 0)))) = 15 \\
foldQ(\times, 1, Q) = 1 \times (2 \times (3 \times (4 \times (5 \times 1)))) = 120 \\
foldQ(\times, 0, Q) = 1 \times (2 \times (3 \times (4 \times (5 \times 0)))) = 0 \\
\end{array}
\]

Function $linkAll$ can be changed by using $foldQ$ accordingly.

\be
linkAll(Q) = foldQ(link, \Phi, Q)
\ee

The Haskell program can be modified as well.

\begin{lstlisting}
linkAll = foldQ link Empty

foldQ :: (a -> b -> b) -> b -> Queue a -> b
foldQ f z q | isEmptyQ q = z
            | otherwise = (front q) `f` foldQ f z (pop q)
\end{lstlisting}

However, the performance of removing can't be ensured in all cases. 
The worst case is that, user keeps appending $N$ elements to a empty
list, and then immediately performs removing. At this time, the K-ary
tree has the first element stored in root. There are $N-1$ children,
all are leaves. So $linkAll()$ algorithm downgrades to $O(N)$ which
is linear time.

The average case is amortized $O(1)$, if the add, append, concatenate
and removing operations are randomly performed. The proof is left as
en exercise to the reader.

\begin{Exercise}
\begin{enumerate}
\item Can you figure out a solution to append an element to the end of a binary
random access list?

\item Prove that the amortized performance of removal operation is
$O(1)$. Hint: using the banker's method.

\item Implement the concatenate-able list in your favorite imperative language.
\end{enumerate}
\end{Exercise}

% =========================================
%  Finger tree
% =========================================
\section{Finger tree}
\index{Sequence!finger tree}
We haven't been able to meet all the performance targets listed at the beginning
of this chapter. 

Binary random access list enables to insert, remove element
on the head of sequence, and random access elements fast. However, it performs
poor when concatenates lists. There is no good way to append element at the
end of binary access list.

Concatenate-able list is capable to concatenates mutliple lists in a fly, and
it performs well for adding new element both on head and tail. However, it doesn't
suppor randomly access element with a given index.

Theset two examples bring us some ideas:

\begin{itemize}
\item In order to support fast manipulation both on head and tail of the sequence, 
there must be some way to easily access the head and tail position;
\item Tree like data structure helps to turn the random access into divide and 
conquer search, if the tree is well balance, the serach can be ensured to
be logarithm time.
\end{itemize}

\subsection{Definition}

Finger tree\cite{finger-tree-1977}, which was first invented in 1977, can help to 
realize efficient sequence. And it is also well implemented in purely functional
settings\cite{finger-tree-2006}.

As we mentioned that the balance of the tree is critical to ensure the performance
for search. One option is to use balanced tree as the under ground data structure
for finger tree. For example the 2-3 tree, which is a special B-tree. (readers
can refer to the chapter of B-tree of this book).

A 2-3 tree either contains 2 children or 3. It can be defined as below in Haskell.

\lstset{language=Haskell}
\begin{lstlisting}
data Node a = Br2 a a | Br3 a a a 
\end{lstlisting}

In imperative settings, node can be defined with a list of sub nodes, which contains
at most 3 children. For instance the following ANSI C code defines node.

\lstset{language=C}
\begin{lstlisting}
union Node {
  Key* keys;  
  union Node* children;
};
\end{lstlisting}

Note in this definition, a node can either contain 2~3 keys, or 2~3 sub nodes.
Where key is the type of elements stored in leaf node.

We mark the left-most none-leaf node as the front finger and the right-most
none-leaf node as the rear finger. Since both figures are essentially 
2-3 trees with all leafs as children, the can be directly represents as 
list of 2 or 3 leafs. Of course a finger tree can be empty or contains
only one one element as leaf.

So the definition of a finger tree is specified like this.

\begin{itemize}
\item A finger tree is either empty;
\item or a singleton leaf;
\item or contains three parts: a left finger which is a list contains at most
3 elements; a sub finger tree; and a right finger which is also a list contains
at most 3 elements.
\end{itemize}

Note that this definition is recursive, so it's quite possible to be translated
to functional settings. The following Haskell definition summary these cases 
for example.

\lstset{language=Haskell}
\begin{lstlisting}
data Tree a = Empty
            | Lf a
            | Tr [a] (Tree (Node a)) [a]
\end{lstlisting}

Figure \ref{fig:ftr-example-1} and \ref{fig:ftr-example-2} show some examples
of figure tree.

In imperative settings, we can define the finger tree in a similiar manner. What's more,
we can add a parent field, so that it's possible to back-track to root from any tree node.
Below ANSI C code defines finger tree accordingly.

\lstset{language=C}
\begin{lstlisting}
struct Tree {
  union Node* front;
  union Node* rear;
  Tree* mid;
  Tree* parent;
};
\end{lstlisting}

We can use NIL pointer to represent an empty tree; and a leaf tree contains only one element
in its front finger, both its rear finger and middle part are empty;

\begin{figure}[htbp]
  \centering
  \subfloat[An empty tree]{\hspace{0.2\textwidth}\includegraphics[scale=0.5]{img/ftr-empty.ps}\hspace{0.2\textwidth}}
  \subfloat[A singleton leaf]{\hspace{0.2\textwidth}\includegraphics[scale=0.5]{img/ftr-leaf.ps}\hspace{0.2\textwidth}} \\
  \subfloat[Front finger and rear finger contain one element for each, the middle part is empty]{\includegraphics[scale=0.5]{img/ftr-ab.ps}}
  \caption{Examples of finger tree, 1} \label{fig:ftr-example-1}
\end{figure}

\begin{figure}[htbp]
  \centering
  \subfloat[After inserting extra 3 elements to front finger, it exceeds the 2-3 tree constraint, which isn't balanced any more]{\includegraphics[scale=0.5]{img/ftr-abcde.ps}}
  \hspace{0.2\textwidth}
  \subfloat[The tree resumes balancing. There are 2 elements in front finger; The middle part is a leaf, which contains a 3-branches 2-3 tree.]{\includegraphics[scale=0.5]{img/ftr-abcdef.ps}}
  \caption{Examples of finger tree, 2} \label{fig:ftr-example-2}
\end{figure}

The first example is an empty finger tree; the second one shows the result after
inserting one element to empty, it becomes a leaf of one node; the third example
shows a finger tree contains 2 elements, one is in front finger, the other is
in rear; 

If we continously insert new elements, to the tree, those elements will be
put in the front finger one by one, until it exeeds the limit of 2-3 tree.
The 4-th example shows such condition, that there are 4 elements in front
finger, which isn't balanced any more.

The last example shows that the finger tree gets fixed so that it resumes
balancing. There are two elements in the front finger. Note that the middle
part is not empty any longer. It's a leaf of a 2-3 tree. The content of the 
leaf is a tree with 3 branches, eash contains an elements.

We can express these 5 examples as the following Haskell expression.

\lstset{language=Haskell}
\begin{lstlisting}
Empty
Lf a
[b] Empty [a]
[e, d, c, b] Empty [a]
[f, e] Lf (Br3 d c b) [a]
\end{lstlisting}

As we mentioned that the definition of finger tree is recursive.
The middle part besides the front and rear finger is a deeper finger tree,
which is defined as $Tree(Node(a))$. Every time we go deeper, the 
$Node()$ is embedded one more level. if the element type of the first level tree
is $a$, the element type for the second level tree is $Node(a)$,
the third level is $Node(Node(a))$, ..., the n-th level is
$Node(Node(Node(...(a))...)) = Node^n(a)$, where $^n$ indicates the $Node()$
is applied $N$ times.

\subsection{Insert element to the head of sequence}

The examples list above actually reveals the typical process that the
elements are inserted one by one to a finger tree. It's possible to summarize
these examples to some cases for insertion on head algorithm.

When we insert an element $x$ to a finger tree $T$,
\begin{itemize}
\item If the tree is empty, the result is a leaf which contains the singleton element $x$;
\item If the tree is a singleton leaf of element $y$, the result is a new finger tree. The front finger contains the new element $x$, the rear finger contains the old element $y$; the middle part is a empty finger tree;
\item If the number of elements store in front finger isn't bigger than the upper limit of 2-3 tree, which is 3, the new element is just insert on the head
of front finger;
\item otherwise, it means that the number of elements stored in front finger exceeds the upper limit of 2-3 tree. the last 3 elements in front finger is wrapped in a 2-3 tree and recursively inserted to the middle part. the new element $x$ is inserted in front of the rest elements in front finger.
\end{itemize}

Suppose that function $leaf(x)$ creates a leaf of element $x$, function
$tree(F, T', R)$ creates a finger tree from three part: $F$ is the front
finger, which is a list contains several elements. Similarity, $R$ is the
rear finger, which is also a list. $T'$ is the middle part which is a 
deeper finger tree. Function $tr3(a, b, c)$ creates a 2-3 tree from 3 
elements $a, b, c$; while $tr2(a, b)$ creates a 2-3 tree from 2 elements
$a$ and $b$.

\be
insertT(x, T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  leaf(x) & T = \Phi \\
  tree(\{x\}, \Phi, \{y\}) & T = leaf(y) \\
  tree(\{x, x_1\}, insertT(tr3(x_2, x_3, x_4), T'), R) & T = tree(\{x_1, x_2, x_3, x_4\}, T', R) \\
  tree(\{x\} \cup F, T', R) & otherwise
  \end{array}
\right .
\ee

The performance of this algorithm is dominated by the recursive case. All the
other cases are constant $O(1)$ time. The recursion depth is proportion to
the height of the tree, so the algorithm is bound to $O(h)$ time, where $h$ is
the height. As we use 2-3 tree to ensure that the tree is well balanced,
$h = O(\lg N)$, where $N$ is the number of elements stored in the finger tree.

More analysis reveal that the amortized performance of $insertT()$ is $O(1)$
because we can amortize the expensive recursion case to other trivial cases.
Please refer to \cite{okasaki-book} and \cite{finger-tree-2006} for the detailed proof.

Translating the algorithm yields the below Haskell program.

\begin{lstlisting}
cons :: a -> Tree a -> Tree a
cons a Empty = Lf a
cons a (Lf b) = Tr [a] Empty [b]
cons a (Tr [b, c, d, e] m r) = Tr [a, b] (cons (Br3 c d e) m) r
cons a (Tr f m r) = Tr (a:f) m r
\end{lstlisting}

Here we use the LISP naming convention to illustrate inserting a
new element to a list. 

The insertion algorithm can also be implemented in imperative approach. Suppose
function \textproc{Tree}() creates an empty tree, that all fields, including
front and rear finger, the middle part inner tree and parent are empty.
Function \textproc{Node}() creates an empty node.

\begin{algorithmic}
\Function{Prepend-Node}{$n, T$}
  \State $r \gets $ \textproc{Tree}()
  \State $p \gets r$
  \State \Call{Connect-Mid}{$p, T$}
  \While{\textproc{Full?}(\Call{Front}{$T$})}
    \State $F \gets $ \Call{Front}{$T$}
    \State \Call{Front}{$T$} $\gets$ $\{n, F[0]\}$
    \State $n \gets$ \textproc{Node}()
    \State \Call{Children}{$n$} $\gets F[1..]$
    \State $p \gets T$
    \State $T \gets$ \Call{Mid}{$T$}
  \EndWhile
  \If{$T =$ NIL}
    \State $T \gets$ \textproc{Tree}()
    \State \Call{Front}{$T$}$\gets \{ n \}$
  \ElsIf{ $|$ \Call{Front}{$T$} $|$ = 1 $\land$ \Call{Rear}{$T$} = $\Phi$}
    \State \Call{Rear}{$T$} $\gets$ \Call{Front}{$T$}
    \State \Call{Front}{$T$} $\gets \{ n \}$
  \Else
    \State \Call{Front}{$T$} $\gets \{ n \} \cup $ \Call{Front}{$T$}
  \EndIf
  \State \Call{Connect-Mid}{$p, T$} $\gets T$
  \State \Return \Call{Flat}{$r$}
\EndFunction
\end{algorithmic}

Where the notation $L[i..]$ means a sub list of $L$ with the first $i-1$
elements removed, that if $L = \{a_1, a_2, ..., a_n\}$, $L[i..] = \{a_i, a_{i+1}, ..., a_n\}$.

Functions \textproc{Front}(), \textproc{Rear}(), \textproc{Mid}(), and \textproc{Parent}()
are used to access the front finger, the rear finger, the middle part inner tree and
the parent tree respectively; Function \textproc{Children}() accesses the children of a
node.

Function \textproc{Connect-Mid}($T_1, T_2$), connect $T_2$ as the inner middle part tree of
$T_1$, and set the parent of $T_2$ as $T_1$ if $T_2$ isn't empty.

In this algorithm, we performs a one pass top-down traverse along the middle part inner tree
if the front finger is full that it can't afford to store any more. The criteria for full
for a 2-3 tree is that the finger contains 3 elements already. In such case, we
extract all the elements except the first one off, wrap them to a new node (one level deeper node),
and continously insert this new node to its middle inner tree. The first element is left
in the front finger, and the element to be inserted is put in front of it, so that this
element becomes the new first one in the front finger.

After this traversal, the algorithm either reach an empty tree, or the tree still has room
to hold more element in its front finger. We create a new leaf for the former case, and
perform a trivial list insert to the front finger for the latter.

During the traversal, we use $p$ to record the parent of the current tree we are processing.
So any new created tree are connected as the middle part inner tree to $p$.

Finally, we return the root of the tree $r$. The last trick of this algorithm is the \textproc{Flat}()
function. In order to simplify the logic, we create an empty `ground' tree and set
it as the parent of the root. We need eliminate this extra `ground' level before return the 
root. This flatten algorithm is realized as the following.

\begin{algorithmic}
\Function{Flat}{$T$}
  \While{$T$ isn't empty}
    \State $T \gets$ \Call{Mid}{$T$}
  \EndWhile
  \If{$T \neq \Phi$}
    \State \Call{Parent}{$T$} $\gets \Phi$
  \EndIf
  \State \Return $T$
\EndFunction
\end{algorithmic}

The while loop test if $T$ is trivial empty, that it's not NIL($=\Phi$), while both its front
and rear fingers are empty. 

Below Python code implements the insertion algorithm for finger tree.

\lstset{language=Python}
\begin{lstlisting}
def insert(x, t):
    return prepend_node(wrap(x), t)

def prepend_node(n, t):
    root = prev = Tree()
    prev.set_mid(t)
    while frontFull(t):
        f = t.front
        t.front = [n] + f[:1]
        n = wraps(f[1:])
        prev = t
        t = t.mid
    if t is None:
        t = leaf(n)
    elif len(t.front)==1 and t.rear == []:
        t = Tree(n.size + t.size, [n], None, t.front)
    else:
        t = Tree(n.size + t.size, [n]+t.front, t.mid, t.rear)
    prev.set_mid(t)
    return flat(root)

def flat(t):
    while t is not None and t.empty():
        t = t.mid
    if t is not None:
        t.parent = None
    return t
\end{lstlisting}

The implementation of function '\verb|set_mid()|', '\verb|frontFull()|', '\verb|wrap()|', 
'\verb|wraps()|', '\verb|empty()|', and tree 
constructor are trivial enough, that we skip the detail of them here. Readers can take this as
exersices.

\subsection{Remove element from the head of sequence}

It's easy to implement the reverse operation
that remove the first element from the list by line by line
reverse the $insertT()$ algorithm.

Let's denote $F = \{f_1, f_2, ...\}$ is the front finger list, 
$M$ is the middle part inner finger tree. $R = \{r_1, r_2, ...\}$ 
is the rear finger list of a finger tree,
and $R' = \{r_2, r_3, ... \}$ is the rest of element with the first one
removed from $R$. 

\be
extractT(T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (x, \Phi) & T = leaf(x) \\
  (x, leaf(y)) & T = tree(\{x\}, \Phi, \{y\}) \\
  (x, tree(\{r_1\}, \Phi, R')) & T = tree(\{x\}, \Phi, R) \\
  (x, tree(toList(F'), M', R)) & T = tree(\{x\}, M, R), (F', M') = extractT(M)\\
  (f_1, tree(\{f_1, f_2, ...\}, M, R)) & otherwise
  \end{array}
\right .
\ee

Where function $toList(T)$ converts a 2-3 tree to plain list as the
following.

\be
toList(T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{x, y\} & T = tr2(x, y) \\
  \{x, y, z\} & T = tr3(x, y, z)
  \end{array}
\right .
\ee

Here we skip the error handling such as trying to remove element from
empty tree etc. If the finger tree is a leaf, the result after removal
is an empty tree; If the finger tree contains two elements, one in the 
front rear, the other in rear, we return the element stored in front rear
as the first element, and the resulted tree after removal is a leaf;
If there is only one element in front finger, the middle part inner tree
is empty, and the rear finger isn't empty, we return the only element
in front finger, and borrow one element from the rear finger to
front; If there is only one element in front finger, however, the
middle part inner tree isn't empty, we can recursively remove a node
from the inner tree, and falttern it to a plain list to replace the
front finger, and remove the orignal only element in front finger;
The last case says that if the front finger contains more than one
element, we can just remove the first element from front finger
and keep all the other part unchanged.

\begin{figure}[htbp]
  \centering
  \subfloat[A sequence of 10 elements represented as a finger tree]{\includegraphics[scale=0.4]{img/ftr-10.ps}} \\
  \subfloat[The first element is removed. There is one element left in front finger.]{\includegraphics[scale=0.4]{img/ftr-9.ps}} \\
  \subfloat[Another element is remove from head. We borrowed one node from the middle part inner tree, change the node, which is a 2-3 tree to a list, and use it as the new front finger. the middle part inner tree becomes a leaf of one 2-3 tree node.]{\hspace{0.2\textwidth}\includegraphics[scale=0.4]{img/ftr-8.ps}\hspace{0.2\textwidth}}
  \caption{Examples of remove 2 elements from the head of a sequence} \label{fig:ftr-uncons-example}
\end{figure}

Figuer \ref{fig:ftr-uncons-example} shows the steps of removing two elements from
the head of a sequence. There are 10 elements stored in the finger tree.
When the first element is removed, there is still one element left in the front finger.
However, when the next element is removed, the front finger is empty. So we `borrow'
one tree node from the middle part inner tree. This is a 2-3 tree. it is converted
to a list of 3 elements, and the list is used as the new finger. the middle part
inner tree change from three parts to a singleton leaf, which contains only one
2-3 tree node. There are three elements stored in this tree node.

Below is the corresponding Haskell program for `uncons'.

\lstset{language=Python}
\begin{lstlisting}
uncons :: Tree a -> (a, Tree a)
uncons (Lf a) = (a, Empty)
uncons (Tr [a] Empty [b]) = (a, Lf b)
uncons (Tr [a] Empty (r:rs)) = (a, Tr [r] Empty rs)
uncons (Tr [a] m r) = (a, Tr (nodeToList f) m' r) where (f, m') = uncons m
uncons (Tr f m r) = (head f, Tr (tail f) m r)
\end{lstlisting}

And the function $nodeToList$ is defined like this.

\begin{lstlisting}
nodeToList :: Node a -> [a]
nodeToList (Br2 a b) = [a, b]
nodeToList (Br3 a b c) = [a, b, c]
\end{lstlisting}

Similar as above, we can define $head$ and $tail$ function from
$uncons$.

\begin{lstlisting}
head = fst . uncons
tail = snd . uncons
\end{lstlisting}

\subsection{append element to the tail of the sequence}

Because finger tree is symmetric, we can give the realization of appending element on tail
by referencing to $insertT()$ algorithm.

\be
appendT(T, x) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  leaf(x) & T = \Phi \\
  tree(\{y\}, \Phi, \{x\}) & T = leaf(y) \\
  tree(F, appendT(M, tr3(x_1, x_2, x_3)), \{x_4, x\}) & T = tree(F, M, \{x_1, x_2, x_3, x_4\}) \\
  tree(F, M, R \cup \{x\}) & otherwise
  \end{array}
\right .
\ee

Generally speaking, if the rear finger is still valid 2-3 tree, that the number of elements
is not greater than 4, the new elements is directly appended to rear finger.
Otherwise, we break the rear finger, take the first 3 element in rear finger to create a 
new 2-3 tree, and recursively append it to the middle part inner tree.
If the finger tree is empty or a singleton leaf, they are handled in the first two cases.

Translating the equation to Haskell yields the below program.

\begin{lstlisting}
snoc :: Tree a -> a -> Tree a
snoc Empty a = Lf a
snoc (Lf a) b = Tr [a] Empty [b]
snoc (Tr f m [a, b, c, d]) e = Tr f (snoc m (Br3 a b c)) [d, e]
snoc (Tr f m r) a = Tr f m (r++[a])
\end{lstlisting}

Function name `snoc' is mirror of 'cons', which indicates the symmetric relationship.

\subsection{remove element from the tail of the sequence}

Similar to $appendT()$, we can realize the algorithm which remove the last element from
finger tree in symmetric manner of $extractT()$.

We denote the none-empty, none-leaf finger tree as $tree(F, M, R)$, where $F$ is the
front finger, $M$ is the middle part inner tree, and $R$ is the rear finger.

\be
removeT(T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\Phi, x) & T = leaf(x) \\
  (leaf(y), x) & T = tree(\{y\}, \Phi, \{x\}) \\
  (tree(init(F), \Phi, last(F)), x) & T = tree(F, \Phi, \{x\}) \land F \neq \Phi \\
  (tree(F, M', toList(R')), x) & T = tree(F, M, \{x\}), (M', R') = removeT(M) \\
  (tree(F, M, init(R)), last(R)) & otherwise
  \end{array}
\right .
\ee

Function $toList(T)$ is used to flattern a 2-3 tree to plain list, which is defined
previously. Function $init(L)$ returns all elements except for the last one in list $L$,
that if $L = \{a_1, a_2, ..., a_{n-1}, a_n\}$, $init(L) = \{a_1, a_2, ..., a_{n-1}\}$.
And Function $last(L)$ returns the last element, so that $init(L) = a_n$. Please
refer to the appendix of this book for their implementation.

Algorithm $removeT()$ can be translated to the following Haskell program, we name
it as `unsnoc' to indicate it's the reverse function of `snoc'.

\begin{lstlisting}
unsnoc :: Tree a -> (Tree a, a)
unsnoc (Lf a) = (Empty, a)
unsnoc (Tr [a] Empty [b]) = (Lf a, b)
unsnoc (Tr f@(_:_) Empty [a]) = (Tr (init f) Empty [last f], a)
unsnoc (Tr f m [a]) = (Tr f m' (nodeToList r), a) where (m', r) = unsnoc m
unsnoc (Tr f m r) = (Tr f m (init r), last r)
\end{lstlisting}

And we can define a special function `last' and 'init' for finger tree which is similar
to their counterpart for list.

\begin{lstlisting}
last = snd . unsnoc
init = fst . unsnoc
\end{lstlisting}

\subsection{concatenate}

Consider the none-trivial case that concatenate two finger tree $T_1 = tree(F_1, M_1, R_1)$ and
$T_2 = tree(F_2, M_2, R_2)$. One natural idea is to use $F_1$ as the new front finger for the 
cacatenated result, and keep $R_2$ is the new rear finger. The rest of work is to merge
$M_1$, $R_1$, $F_2$ and $M_2$ to a new middle part inner tree.

Note that both $R_1$ and $F_2$ are plain lists of node, so the sub-problem is to realize a 
algorithm like this.

\[
merge(M_1, R_1 \cup F_2, M_2) = ?
\]

More observation reveals that both $M_1$ and $M_2$ are also finger trees, except that they
are one level deeper than $T_1$ and $T_2$ in terms of $Node(a)$, where $a$ is the type of 
element stored in the tree. We can recursively use the strategy that keep the front finger
of $M_1$ and the rear finger of $M_2$, then merge the middle part inner tree of $M_1$, $M_2$,
as well as the rear finger of $M_1$ and front finger of $M_2$.

If we denote function $front(T)$ returns the front finger, $rear(T)$ returns the rear finger,
$mid(T)$ returns the middle part inner tree. the above $merge()$ algorithm can be
expressed for none-trivial case as the following.

\be
\begin{array}{l}
merge(M_1, R_1 \cup F_2, M_2) = tree(front(M_1), S, rear(M_2)) \\
S = merge(mid(M_1), rear(M_1) \cup R_1 \cup F_2 \cup front(M_2), mid(M_2))
\end{array}
\label{eq:merge-recursion}
\ee

If we look back to the original concatenate soltuion, which can be expressed as.

\be
concat(T_1, T_2) = tree(F_1, merge(M_1, R_1 \cup R_2, M_2), R_2)
\ee

And compare it with equation \ref{eq:merge-recursion}, it's easy to note the fact that 
concatenating is essentally merging. So we have the final algorithm as this.

\be
concat(T_1, T_2) = merge(T_1, \Phi, T_2)
\ee

By adding edge cases, the $merge()$ algorithm can be completed as below.

\be
merge(T_1, S, T_2) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  foldR(insertT, T_2, S) & T_1 = \Phi \\
  foldL(appendT, T_1, S) & T_2 = \Phi \\
  merge(\Phi, \{x\} \cup S, T_2) & T_1 = leaf(x) \\
  merge(T_1, S \cup \{x\}, \Phi) & T_2 = leaf(x) \\
  tree(F_1, merge(M_1, nodes(R_1 \cup S \cup F_2), M2), R_2) & otherwise
  \end{array}
\right .
\ee

Most of them are straightforward, if any one of $T_1$ or $T_2$ is empty, the algorithm
repeatedly insert/append all elements in $S$ to the other tree; Function $foldL$ and
$foldR$ are kinds of for-each process in imperative settings. The difference is that
$foldL$ process the list $S$ from left to right while $foldR$ process from right to left.

Here are their definition. Suppose list $L=\{ a_1, a_2, ..., a_{n-1}, a_n\}$, 
$L' = \{ a_2, a_3, ..., a_{n-1}, a_n\}$ is the rest of elements except for the first one.

\be
foldL(f, e, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  e & L = \Phi \\
  foldL(f, f(e, a_1), L') & otherwise
  \end{array}
\right .
\ee

\be
foldR(f, e, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  e & L = \Phi \\
  f(a_1, foldR(f, e, L')) & otherwise
  \end{array}
\right .
\ee

They are detailed explained in the appendix of this book.

If either one of the tree is a leaf, we can insert or append the element of this leaf to
$S$, so that it becomes the trivial case of concatenate one empty tree with another.

Function $nodes()$ is used to wrap a list of elements to a list of 2-3 trees. 
This is because the contents of middle part inner tree, compare to the
contents of finger are one level deeper in terms of $Node()$. Consider the
time point that transforms from recursive case to edget case. Let's suppose
$M_1$ is empty at that time, we then need repeatedly insert all elements from
$R_1 \cup S \cup F_2$ to $M_2$. However, we can't directly do the insertion.
If the element type is $a$, we can only insert $Node(a)$ which is 2-3 tree
to $M_2$. This is just like what we did in the $insertT()$ algorithm,
take out the last 3 elements, wrap them in a 2-3 tree, and recursive
perform $insertT()$. Here is the definition of $nodes()$.

\be
nodes(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{tr2(x_1, x_2)\} & L = \{x_1, x_2\} \\
  \{tr3(x_1, x_2, x_3)\} & L = \{x_1, x_2, x_3\} \\
  \{tr2(x_1, x_2), tr2(x_3, x_4)\} & L = \{x_1, x_2, x_3, x_4\} \\
  \{tr3(x_1, x_2, x_3)\} \cup nodes(\{x_4, x_5, ...\}) & otherwise
  \end{array}
\right .
\ee

Function $nodes()$ follows the constraint of 2-3 tree, that if there are
only 2 or 3 elements in the list, it just wrap them in singleton list
contains a 2-3 tree; If there are 4 elements in the lists, it split them
into two trees each is consist of 2 branches; Otherwise, if there are
more elements than 4, it wraps the first three in to one tree with 3 branches,
and recursively call $nodes()$ to process the rest.

The performance of concatenation is determined by merging. Analyze the 
recursive case of merging reveals that the depth of recursion is 
proportion to the smaller height of the two trees. As the tree is
ensured to be balanced by using 2-3 tree. it's height is bound to 
$O(\lg N')$ where $N'$ is the number of elements. The edge
case of merging performs as same as insertion, (It calls $insertT()$
at most 8 times) which is amortized $O(1)$ time, and $O(\lg M)$
at worst case, where $M$ is the difference in height of the two trees.
So the overall performance is bound to $O(\lg N)$, where $N$ is
the total number of elements contains in two finger trees.

The following Haskell program impelements the concatenation algorithm.

\begin{lstlisting}
concat :: Tree a -> Tree a -> Tree a
concat t1 t2 = merge t1 [] t2
\end{lstlisting}

Note that there is `concat' function defined in prelude standard library,
so we need distinct them either by hiding import or take a different name.

\begin{lstlisting}
merge :: Tree a -> [a] -> Tree a -> Tree a
merge Empty ts t2 = foldr cons t2 ts
merge t1 ts Empty = foldl snoc t1 ts
merge (Lf a) ts t2 = merge Empty (a:ts) t2
merge t1 ts (Lf a) = merge t1 (ts++[a]) Empty
merge (Tr f1 m1 r1) ts (Tr f2 m2 r2) = Tr f1 (merge m1 (nodes (r1 ++ ts ++ f2)) m2) r2
\end{lstlisting}

And the implementation of $nodes()$ is as below.

\begin{lstlisting}
nodes :: [a] -> [Node a]
nodes [a, b] = [Br2 a b]
nodes [a, b, c] = [Br3 a b c]
nodes [a, b, c, d] = [Br2 a b, Br2 c d]
nodes (a:b:c:xs) = Br3 a b c:nodes xs
\end{lstlisting}

\begin{Exercise}
\begin{enumerate}
\item Implement the complete finger tree insertion program in your favorite imperative
programming language. Don't check the example programs along with this chapter before
having a try.

\item Try to implement concatenation algorithm without using folding. You can either use
recursive methos, or use imperative for-each method.
\end{enumerate}
\end{Exercise}

\subsection{Random access of finger tree}

\subsubsection{size augmentation}
The strategy to provide fast random access, is to turn the looking up into tree-search.
In order to avoid calculating the size of tree many times, we augment an extra field 
to trees and nodes. The definition should be modified accordingly, for example the
following Haskell definition added size field in its constructor.

\begin{lstlisting}
data Tree a = Empty
            | Lf a
            | Tr Int [a] (Tree (Node a)) [a]
\end{lstlisting}

Suppose the function $tree(s, F, M, R)$ creates a finger tree from size $s$, front
finger $F$, rear finger $R$, and middle part inner tree $M$.
When the size of the tree is needed, we can call a $size(T)$ function. It will be
someting like this.

\[
size(T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  0 & T = \Phi \\
  ? & T = leaf(x) \\
  s & T = tree(s, F, M, R)
  \end{array}
\right .
\]

If the tree is empty, the size is definately zero; and if it can be expressed as $tree(s, F, M, R$,
the size is $s$; however, what if the tree is a singleton leaf? is it 1? No, it 
can be 1 only if $T = leaf(a)$ and $a$ isn't a tree node, but an element stored in finger tree.
In most cases, the size is not 1, because $a$ can be again a tree node. That's why we
put a `?' in above equation.

The correct way is to call some size function on the tree node as the following.

\be
size(T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  0 & T = \Phi \\
  size'(x) & T = leaf(x) \\
  s & T = tree(s, F, M, R)
  \end{array}
\right .
\ee

Note that this isn't a resurive definition since $size \neq size'$, the argument to $size'$
is either a tree node, which is a 2-3 tree, or a plain element stored in the finger tree.
To uniform these two cases, we can anyway wrap the single plain element to a tree node
of only one element. So that we can express all the situation as a tree node augmented
with a size field. The following Haskell program modifies the definition of tree node.

\begin{lstlisting}
data Node a = Br Int [a]
\end{lstlisting}

Suppose function $tr(s, L)$, create such a node (either one element being wrapped or a 2-3 tree)
from a size information $s$, and a list $L$. Here are some example.

\[
\begin{array}{ll}
tr(1, \{x\}) & \text{a tree contains only one element} \\
tr(2, \{x, y\}) & \text{a 2-3 tree contains two elements} \\
tr(3, \{x, y, z\}) & \text{a 2-3 tree contains three elements}
\end{array}
\]

So the function $size'$ can be implemented as returning the size information of a tree node.
We have $size'(tr(s, L)) = s$.

Wrapping an element $x$ is just calling $tr(1, \{x\})$. We can define auxiliary functions
$wrap$ and $unwrap$, for instance.

\be
\begin{array}{l}
wrap(x) = tr(1, \{x\}) \\
unwrap(n) = x \quad:\quad n = tr(1, \{x\})
\end{array}
\ee

As both front finger and rear finger are list of tree nodes, in order to calculate the 
total size of finger, we can provide a $size''(L)$ function, which sums up size of all
nodes stored in the list. Denote $L = \{ a_1, a_2, ... \}$ and $L' = \{ a_2, a_3, ... \}$.

\be
size''(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  0 & L = \Phi \\
  size'(a_1) + size''(L') & otherwise
  \end{array}
\right .
\ee

It's quite OK to define $size''(L)$ by using folding.

\be
size'' = fold \circ + \circ 0
\ee

And we can turn a list of tree nodes into one deeper 2-3 tree and vice-versa.

\be
\begin{array}{l}
wraps(L) = tr(size''(L), L) \\
unwraps(n) = L \quad:\quad n = tr(s, L) \\
\end{array}
\ee

These helper functions are translated to the following Haskell code.

\begin{lstlisting}
size (Br s _) = s

sizeL = sum .(map size)

sizeT Empty = 0
sizeT (Lf a) = size a
sizeT (Tr s _ _ _) = s
\end{lstlisting}

Here here are the wrap and unwrap auxiliary functions.

\begin{lstlisting}
wrap x = Br 1 [x]
unwrap (Br 1 [x]) = x
wraps xs = Br (sizeL xs) xs
unwraps (Br _ xs) = xs
\end{lstlisting}

We omitted their type definitions for illustration purpose.

\subsubsection{Modification due to the augmented size}

The algorithms have been presented so far need to be modified to accomplish with the
augmented size. For example the $insertT()$ function now inserts a tree node instead
of a plain element.

\be
insertT(x, T) = insertT'(wrap(x), T)
\ee

The corresponding Haskell program is changed as below.

\begin{lstlisting}
cons a t = cons' (wrap a) t
\end{lstlisting}

After being wrapped, $x$ is augmented with size infomration of 1. In the implementation
of previous insertion algorithm, function $tree(F, M, R)$ is used to create a finger tree
from a front finger, a middle part inner tree and a rear finger. This function should
also be modified to add size information of these three arguments.

\be
tree'(F, M, R) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  fromL(F) & M = \Phi \land R = \Phi \\
  fromL(R) & M = \Phi \land F = \Phi \\
  tree'(unwraps(F'), M', R) & F = \Phi, (F', M') = extractT'(M) \\
  tree'(F, M', unwraps(R')) & R = \Phi, (M', R') = removeT'(M) \\
  tree(size''(F) + size(M) + size''(R), F, M, R) & otherwise
  \end{array}
\right .
\ee

Where $fromL()$ helps to turn a list of nodes to a finger tree by repeatedly 
inserting all the element one by one to an empty tree.

\[
fromL(L) = foldR(insertT', \Phi, L)
\]

Of course it can be implemented in pure recursive manner without using folding as well.

The last case is the most straightforward one. If none of $F$, $M$, and $R$ is empty,
it adds the size of these three part and construct the tree along with this size information
by calling $tree(s, F, M, R)$ function.
If both the middle part innser tree and one of the finger is empty, the algorithm 
repeatedly insert all elements stored in the other finger to an empty tree, so that
the result is constructed from a list of tree node.
If the middle part inner tree isn't empty, and one of the finger is empty, the 
algorithm `borrows' one tree node from the middle part, either by extract from
head if front finger is empty or remove from tail if rear finger is empty.
Then the algorithm unwraps the `borrowed' tree node to a list, and recursively
call $tree'()$ function to construct the result.

This algorithm can be translated to the following Haskell code for example.

\begin{lstlisting}
tree f Empty [] = foldr cons' Empty f
tree [] Empty r = foldr cons' Empty r
tree [] m r = let (f, m') = uncons' m in tree (unwraps f) m' r
tree f m [] = let (m', r) = unsnoc' m in tree f m' (unwraps r)
tree f m r = Tr (sizeL f + sizeT m + sizeL r) f m r
\end{lstlisting}

Function $tree'()$ helps to minimize the modification. $insertT'()$ can be 
realized by using it like the following.

\be
insertT'(x, T) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  leaf(x) & T = \Phi \\
  tree'(\{x\}, \Phi, \{y\}) & T = leaf(x) \\
  tree'(\{x, x_1\}, insertT'(wraps(\{x_2, x_3, x_4\}), M), R) & T = tree(s, \{x_1, x_2, x_3, x_4\}, M, R) \\
  tree'(\{x\} \cup F, M, R) & otherwise
  \end{array}
\right .
\ee

And it's corresponding Haskell code is a line by line translation.

\begin{lstlisting}
cons' a Empty = Lf a
cons' a (Lf b) = tree [a] Empty [b]
cons' a (Tr _ [b, c, d, e] m r) = tree [a, b] (cons' (wraps [c, d, e]) m) r
cons' a (Tr _ f m r) = tree (a:f) m r
\end{lstlisting}

For simplification purpose, we skipped the detailed description of what are modified in
$extractT()'$, $appendT()$, $removeT()$, and $concat()$ algorithms. They are left as exercises to the
reader.

\subsubsection{Split a finger tree at a given position}

With size information augmented, it's easy to locate a node at given position by performing
a tree search. What's more, as the finger tree is constructed from three part $F$, $M$, and
$R$; and it's nature of recursive, it's also possible to split it into three sub parts with
a given position $i$: the left, the node at $i$, and the right part.

The idea is straight forward. Since we have the size information for $F$, $M$, and $R$. Denote
these three sizes as $S_f$, $S_m$, and $S_r$. if the given position $i \leq S_f$, the node must
be stored in $F$, we can go on seeking the node inside $F$; if $S_f < i \leq S_f + S_m $, the
node must be stored in $M$, we need recursively perform serach in $M$; otherwise, the node
should be in $R$, we need search inside $R$.

If we skip the error handling of trying split an empty tree, there is only one edge case
as below.

\[
splitAt(i, T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\Phi, x, \Phi) & T = leaf(x) \\
  ... & otherwise
  \end{array}
\right .
\]

Spliting a leaf results both the left and right parts empty, the node stored in leaf 
is the resulting node.

The recursive case handles the three sub cases by comparing $i$ with the sizes.
Suppose function $splitAtL(i, L)$ splits a list of nodes at given position $i$
into three parts: $(A, x, B) = splitAtL(i, L)$, where $x$ is the $i$-th node
in $L$, $A$ is a sub list contains all nodes before position $i$, and $B$ is
a sub list contains all rest nodes after $i$.

\be
splitAt(i, T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\Phi, x, \Phi) & T = leaf(x) \\
  (fromL(A), x, tree'(B, M, R) & i \leq S_f, (A, x, B) = splitAtL(i, F) \\
  (tree'(F, M_l, A), x, tree'(B, M_r, R) & S_f < i \leq S_f + S_m \\
  (tree'(F, M, A), x, fromL(B)) & otherwise, (A, x, B) = splitAtL(i-S_f-S_m, R)
  \end{array}
\right .
\ee

Where $M_l, x, M_r, A, B$ are calculated as the following.

\[
\begin{array}{l}
(M_l, t, M_r) = splitAt(i-S_f, M) \\
(A, x, B) = splitAtL(i-S_f-size(M_l), unwraps(t))
\end{array}
\]

And the function $splitAtL()$ is just a linear traverse, since the length of list
is limited not to exceed the constraint of 2-3 tree, the performance is still
ensured to be constant $O(1)$ time. Denote $L = \{x_1, x_2, ... \}$ and 
$L' = \{ x_2, x_3, ...\}$.

\be
splitAtL(i, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\Phi, x_1, \Phi) & i = 0 \land L = \{x_1\} \\
  (\Phi, x_1, L') & i < size'(x_1) \\
  (\{x_1\} \cup A, x, B) & otherwise, (A, x, B) = splitAtL(i-size'(x_1), L')
  \end{array}
\right .
\ee

The solution of splitting is a typical divide and conquer strategy. The performance
of this algorithm is determined by the recursive case of searching in middle part
inner tree. Other cases are all constant time as we've analyzed. The depth of
recursion is proportion to the height of the tree $h$, so the algorithm is bound
to $O(h)$. Because the tree is well balanced (by using 2-3 tree, and all the 
insertion/removal algorithms keep the tree balanced), so $h = O(\lg N)$ where
$N$ is the number of elements stored in finger tree. The overall performance
of spliting is $O(\lg N)$.

Let's first give the Haskell program for $splitAtL()$ function

\begin{lstlisting}
splitNodesAt 0 [x] = ([], x, [])
splitNodesAt i (x:xs) | i < size x = ([], x, xs)
                      | otherwise = let (xs', y, ys) = splitNodesAt (i-size x) xs
                                    in (x:xs', y, ys)
\end{lstlisting}

Then the program for $splitAt()$, as there is already function defined in standard
libary with this name, we slightly change the name by adding a apostrophe.

\begin{lstlisting}
splitAt' _ (Lf x) = (Empty, x, Empty)
splitAt' i (Tr _ f m r)
    | i < szf = let (xs, y, ys) = splitNodesAt i f
                in ((foldr cons' Empty xs), y, tree ys m r)
    | i < szf + szm = let (m1, t, m2) = splitAt' (i-szf) m
                          (xs, y, ys) = splitNodesAt (i-szf - sizeT m1) (unwraps t)
                      in (tree f m1 xs, y, tree ys m2 r)
    | otherwise = let (xs, y, ys) = splitNodesAt (i-szf -szm) r
                  in (tree f m xs, y, foldr cons' Empty ys)
    where
      szf = sizeL f
      szm = sizeT m
\end{lstlisting}

\subsubsection{Random access}

With the help of splitting at any arbitrary position, it's trivial to realize random
access in $O(\lg N)$ time. Denote function $mid(x)$ returns the 2-nd element of a tuple,
$left(x)$, and $right(x)$ return the first element and the 3-rd element of the tuple
respectively.

\be
getAt(S, i) = unwrap(mid(splitAt(i, S)))
\ee

It first splits the sequence at position $i$, then unwraps the node to get the element
stored inside it. When mutate the $i$-th element of sequence $S$ represented by finger tree,
we first split it at $i$, then we replace the middle to what
we want to mutate, and re-construct them to one finger tree by using concatenation.

\be
setAt(S, i, x) = concat(L, insertT(x, R))
\ee

where
\[
(L, y, R) = splitAt(i, S)
\]

What's more, we can also realize a $removeAt(S, i)$ function, which can remove the 
$i$-th element from sequence $S$. The idea is first to split at $i$, unwrap and
return the element of the $i$-th node; then concatenate the left and right to a 
new finger tree. 

\be
removeAt(S, i) = (unwrap(y), concat(L, R))
\ee

These handy algorithms can be translated to the following Haskell program.

\begin{lstlisting}
getAt t i = unwrap x where (_, x, _) = splitAt' i t
setAt t i x = let (l, _, r) = splitAt' i t in concat' l (cons x r)
removeAt t i = let (l, x, r) = splitAt' i t in (unwrap x, concat' l r)
\end{lstlisting}

\begin{Exercise}
\begin{enumerate}
\item Another way to realize $insertT'()$ is to force increasing the size field by one, so
that we needn't write function $tree'()$. Try to realize the algorithm by using this idea.

\item Try to handle the augment size information as well as in $insertT'()$ algorithm for
the following algorithms: $extractT()'$, $appendT()$, $removeT()$, and $concat()$. The `head', `tail',
`init' and `last' functions should be kept unchanged. Don't refer to the downloadable
programs along with this book before you take a try.
\end{enumerate}
\end{Exercise}

% ================================================================
%                 Short summary
% ================================================================
\section{Notes and short summary}

Although we haven't been able to give a purely functional realization to match the
$O(1)$ constant time random access as arrays in imperative settings. The resulting
finger tree data structure achieves an overall well performed sequence.
It manipulates fast in amortized $O(1)$ time both on head an on tail, it can also
concatenates two sequence in logarithmic time as well as break one sequence into 
two sub sequences at any position. While neither arrays in imperative settings
nor linked-list in functional settings satisfies all these goals.
Some functional programming languages adopt this sequence realization in its 
standard library \cite{hackage-ftr}.

Just as the title of this chapter, we've presented the last corner stone of
elementary data structures in both functional and imperative settings.
We needn't concern about being lack of elementary data structures when
solve problems with some typical algorithms.

For example, when writing a MTF (move-to-front) encoding algorithm\cite{mtf-wiki}, with
the help of the sequence data structure explained in this chapter. We can
implement it quite straightforward.

\[
mtf(S, i) = \{x\} \cup S' \\
\]

where $(x, S') = removeAt(S, i)$.

In the next following chapters, we'll first explains some typical divide and conquer
sorting methods, including quick sort, merge sort and their variants; then
some elementary searching algorithms, and string matching algorithms; finally,
we'll give a real-world example of algorithms, BWT (Burrows-Wheeler transform) compressor,
which is one of the best compression tool in the world.

% ================================================================
%                 Appendix
% ================================================================

\begin{thebibliography}{99}

\bibitem{okasaki-book}
Chris Okasaki. ``Purely Functional Data Structures''. Cambridge university press, (July 1, 1999), ISBN-13: 978-0521663502

\bibitem{okasaki-ralist}
Chris Okasaki. ``Purely Functional Random-Access Lists''. Functional Programming Languages and Computer Architecutre, June 1995, pages 86-95.

\bibitem{CLRS}
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein. ``Introduction to Algorithms, Second Edition''. The MIT Press, 2001. ISBN: 0262032937.

\bibitem{learn-haskell}
Miran Lipovaca. ``Learn You a Haskell for Great Good! A Beginner's Guide''. No Starch Press; 1 edition April 2011, 400 pp. ISBN: 978-1-59327-283-8

\bibitem{finger-tree-2006}
Ralf Hinze and Ross Paterson. ``Finger Trees: A Simple General-purpose Data Structure." in Journal of Functional Programming16:2 (2006), pages 197-217. http://www.soi.city.ac.uk/~ross/papers/FingerTree.html

\bibitem{finger-tree-1977}
Guibas, L. J., McCreight, E. M., Plass, M. F., Roberts, J. R. (1977), "A new representation for linear lists". Conference Record of the Ninth Annual ACM Symposium on Theory of Computing, pp. 49C60.

\bibitem{hackage-ftr}
Generic finger-tree structure. http://hackage.haskell.org/packages/archive/fingertree/0.0/doc/html/Data-FingerTree.html

\bibitem{mtf-wiki}
Wikipedia. Move-to-front transform. http://en.wikipedia.org/wiki/Move-to-front\_transform

\end{thebibliography}

\ifx\wholebook\relax \else
\end{document}
\fi
