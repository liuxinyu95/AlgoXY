\ifx\wholebook\relax \else
% ------------------------ 

\documentclass{article}
%------------------- Other types of document example ------------------------
%
%\documentclass[twocolumn]{IEEEtran-new}
%\documentclass[12pt,twoside,draft]{IEEEtran}
%\documentstyle[9pt,twocolumn,technote,twoside]{IEEEtran}
%
%-----------------------------------------------------------------------------
%\input{../../../common.tex}
\input{../../../common-en.tex}

\setcounter{page}{1}

\begin{document}

\fi
%--------------------------

% ================================================================
%                 COVER PAGE
% ================================================================

\title{Sequences, The last brick}

\author{Liu~Xinyu
\thanks{{\bfseries Liu Xinyu } \newline
  Email: liuxinyu95@gmail.com \newline}
  }

\markboth{Sequences}{AlgoXY}

\maketitle

\ifx\wholebook\relax
\chapter{Sequences, The last brick}
\numberwithin{Exercise}{chapter}
\fi

% ================================================================
%                 Introduction
% ================================================================
\section{Introduction}
\label{introduction}
In the first chapter of this book, which introduced binary search tree
as the `hello world' data structure, we mentioned that neither queue
nor array is simple if realized not only in imperative way, but also
in functional approach. In previous chapter, we explained functional
queue, which achieves the similar performance as its imperative counterpart.
In this chapter, we'll dive into the topic of array-like data structures.

We have introduced several data structures in this book so far, and 
it seems that functional approaches typically bring more expressive
and elegant solution. However, there are some areas, people haven't
found competitive purely functional solutions which can match the imperative
ones. For instance, the Ukkonen linear time suffix tree construction
algorithm. another examples is Hashing table. Array is also among them.

Array is trivial in imperative settings, it enables randomly accessing
any elements with index in constant $O(1)$ time. However, this performance
target can't be achieved directly in purely functional settings as
there is only list can be used.

In this chapter, we are going to abstract the concept of array to sequences.
Which support the following features

\begin{itemize}
\item Element can be inserted to or removed from the head of the sequence quickly in $O(1)$ time;
\item Element can be inserted to or removed from the head of the sequence quickly in $O(1)$ time;
\item Support concatenate two sequences quickly (faster than linear time);
\item Support randomly access and update any element quickly;
\item Support split at any position quickly;
\end{itemize}

We call these features abstract sequence properties, and it easy to see
the fact that even array (here means plain-array) in imperative settings
can't meet them all at the same time.

We'll provide three solutions in this chapter. Firstly, we'll introduce
a solution based on binary tree forest and numeric representation;
Secondly, we'll show a catenable list solution; Finally, we'll give
the finger tree solution.

Most of the results are based on Chris, Okasaki's work in \cite{okasaki-book}. 

% ================================================================
%                 Binary random access list
% ================================================================
\section{Binary random access list}
\index{Sequence!Binary random access list}

\subsection{Review of plain-array and list}

Let's review the performance of plain-array and singly linked-list so that we know
how they perform in different cases.

\begin{tabular}{l | c | r}
  \hline
  operation & Array & Linked-list \\
  \hline
  operation on head & $O(N)$ & $O(1)$ \\
  operation on tail & $O(1)$ & $O(N)$ \\
  access at random position & $O(1)$ & average $O(N)$ \\
  remove at given position & average $O(N)$ & $O(1)$ \\
  concatenate & $O(N_2)$ & $O(N_1)$ \\
  \hline
\end{tabular}

Because we hold the head of linked list, operations on head such as insert and remove perform
in constant time; while we need traverse to the end to perform remove or append on tail; Given
a position $i$, it need traverse $i$ elements to access it. Once we are in that position,
removing element from there is just bound to constant time by modifying some pointers. 
In order to concatenate two
linked-lists, we need traverse to the end of the first one, and link it to the second one, which
is bound to the length of the first linked-list;

On the other hand, for array, we must prepare free cell for insert a new element to the head of array, and
we need release the first cell after the first element is removed, all these two operations are
achieved by shifting all the rest elements forward or backward, which costs linear time. While the
operations on the tail of array are trivial constant time ones. Array also support access random
poistion $i$ by nature; However, remove the element at that position cause shifting all elements
after it one position ahead. In order to concatenate two arrays, we need copy all elements from the
second one to the end of the first one (ignore the memory re-allocation details), which is proportion
to the length of the second array.

In the chapter about binomial heaps, we have explained the idea of using forest, which is a list of trees
to design the data structure. It brings us the merit that, for any given number $N$, by representing it
in binary number, we know how many binomial trees need to hold them. That each bit of 1 represent a binomial
tree of that rank of bit. We can go one step ahead, if we have a $N$ nodes binomial heap, for any given
index $1 < i < N$, we can quickly know which binomial tree in the heap hold the $i$-th node.

\subsection{Represent sequence by trees}
\index{Binary Random Access List!Definition}

One solution to realize a random-access sequence is to manage the sequence with a forest of complete binary
trees. Figure \ref{fig:bi-tree-sequence} shows how we attach such trees to a sequence of numbers.

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=1.0]{img/bi-tree-sequence.ps}
  \caption{A sequence of 6 elements can be represented in a forest.} \label{fig:bi-tree-sequence}
\end{figure}

Here two trees $t_1$ and $t_2$ are used to represent sequence $\{x_1, x_2, x_3, x_4, x_5, x_6\}$. 
The size of binary tree $t_1$ is 2. The first two elements $\{x_1, x_2\}$ are leaves of $t_1$;
the size of binary tree $t_2$ is 4. The next foure elements $\{x_3, x_4, x_5, x_6\}$ are leaves
of $t_2$. 

For a complete binary tree, we define the depth as 0 if the tree only has a leaf, 
The tree is denoted as as $t_i$ if its depth is $i+1$. It's obvious that there
are $2^i$ leaves in $t_i$.

For any sequence contains $N$ elements, it can be turned to a forest of compelte binary trees in this manner.
First we represent $N$ in binary number like below.

\be
N = 2^0 e_0 + 2^1 e_1 + ... + 2^M e_M
\ee

Where $e_i$ is either 1 or 0, so $N=(e_M e_{M-1} ... e_1 e_0)_2$. If $e_i \neq 0$, we then need a 
complete binary tree with size $2^i$,  For example in figure \ref{fig:bi-tree-sequence}, as the
length of sequence is 6, which is $(110)2$ in binary. The lowest bit is 0, so we needn't a tree
of size 1; the second bit is 1, so we need a tree of size 2, which has depth of 2; the highest
bit is also 1, thus we need a tree of size 4, which has depth of 3.

This method represents the sequence $\{x_1, x_2, ..., x_N\}$ to a list of trees $\{t_0, t_1, ..., t_M\}$
where $t_i$ is either empty if $e_i = 0$ or a complete binary tree if $e_i = 1$.
We call this representaion as {\em Binary Random Access List}.

We can reused the definition of binary tree, for example, the following Haskell program defines
the binary random access list as well as tree.

\lstset{language=Haskell}
\begin{lstlisting}
data Tree a = Leaf a
            | Node Int (Tree a) (Tree a)  -- size, left, right

type BRAList a = [Tree a]
\end{lstlisting}

The only difference from the typical binary tree is that we augment the size information to the tree.
This enable us to get the size without calculation at every time. For instance.

\begin{lstlisting}
size (Leaf _) = 1
size (Node sz _ _) = sz
\end{lstlisting}

\subsection{Insertion to the head of the sequence}
\index{Binary Random Access List!Insertion}
The new forest representation of sequence enables many operation effectively. For example, the
operation of inserting a new element $y$ in front of sequence can be realized as the following.

\begin{enumerate}
\item Create a tree $t'$, with $y$ as the only one leaf;
\item Examine the first tree in the forest, compare its size with $t'$, if its size is greater than $t'$,
we just let $t'$ be the new head of the forest, since the forest is a linked-list of tree, insert
$t'$ to its head is trivial operation, which is bound to constant $O(1)$ time;
\item Otherwise, if the size of first tree in the forest is equal to $t'$, let's denote this tree
in the forest as $t_i$, we can construct a new binary tree $t'_{i+1}$ by linking $t_i$ and $t'$ as
its left and right children. After that, we recursively try to insert $t'_{i+1}$ to the forest.
\end{enumerate}

TODO: Add figures (step by step insertion) to illustrate the insertion process.

As there are at most $M$ trees in the forest, and $M$ is bound to $O(\lg N)$, so the insertion to
head algorithm is ensured to perform in $O(\lg N)$. 

Let's formalize the algorithm to equations. we define the function of insert a element in front of
a sequence as $insert(S, x)$.

\be
insert(S, x) = insertTree(S, (leaf x))
\ee

This function just wrap element $x$ to a singleton tree with a leaf, and call $insertTree$ to insert
this tree to the forest. Suppose the forest $F=\{ t_1, t_2, ...\}$ if it's not empty, and $F' = \{ t_2, t_3, ...\}$
is the rest of trees without the first one.

\be
insertTree(F, t) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{ t \} & F = \Phi \\
  \{ t \} \cup F & size(t) < size(t_1) \\
  insertTree(F', link(t, t_1) & otherwise
  \end{array}
\right .
\ee

Where function $link$ create a new tree from two small trees with same size. Suppose function
$tree(s, t_1, t_2)$ create a tree, set its size as $s$, makes $t_1$ as the left child, and $t_2$ as
the right child, linking can be realized as below.

\be
link(t_1, t_2) = tree(size(t_1) + size(t_2), t_1, t_2)
\ee

The relative Haskell programs can be given by translating these equations.

\begin{lstlisting}
cons :: a -> BRAList a -> BRAList a
cons x ts = insertTree ts (Leaf x) 

insertTree :: BRAList a -> Tree a -> BRAList a
insertTree [] t = [t]
insertTree (t':ts) t = if size t < size t' then  t:t':ts
                       else insertTree ts (link t t')

-- Precondition: rank t1 = rank t2
link :: Tree a -> Tree a -> Tree a
link t1 t2 = Node (size t1 + size t2) t1 t2
\end{lstlisting}

Here we use the Lisp tradition to name the function that insert an element before a list as `cons'.

\subsubsection{Remove the element from the head of the sequence}

It's not complex to realize the inverse operation of `cons', which can remove element from the
head of the sequence.

\begin{itemize}
\item If the first tree in the forest is a singleton leaf, remove this tree from the forest;
\item otherwise, we can halve the first tree by unlinking its two children, so the first tree
in the forest becomes two trees, we recursively halve the first tree until it turns to be a 
leaf.
\end{itemize}

TOOD: Add figures to illustrate the remove process step by step.

If we assume the sequence isn't empty, so that we can skip the error handling such as trying 
to remove an element from an empty sequence, this can be expressed with the following equation.
We denote the forest $F = \{t_1, t_2, ... \}$ and the trees without the first one as
$F' = \{ t_2, t_3, ...\}$

\be
extractTree(F) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (t_1, F') & t_1 {\quad} \mbox{is leaf} \\
  extractTree(\{t_l, t_r\} \cup F') & otherwise
  \end{array}
\right .
\ee

where $\{ t_l, t_r \} = unlink(t_1)$ are the two children of $t_1$.

It can be translated to Haskell programs like below.

\begin{lstlisting}
extractTree (t@(Leaf x):ts) = (t, ts)
extractTree (t@(Node _ t1 t2):ts) = extractTree (t1:t2:ts)
\end{lstlisting}

With this function defined, it's convenient to give `head' and `tail', the former returns
the first element in the sequence, the latter return the rest.

\be
head(S) = key(first(extractTree(S)))
\ee

\be
tail(S) = second(extractTree(S))
\ee

Where function `first' returns the first element in a paired-value (as known as tuple); 
`second' returns the second element respectively. function `key' is used to access the
elements inside a leaf. Below are Haskell programs corresponding to these two euqations.

\begin{lstlisting}
head' ts = x where (Leaf x, _) = extractTree ts
tail' = snd . extractTree
\end{lstlisting}

Note that as `head' and `tail' functions have already been defined in Haskell standard
library, we given them apostrophes to make them distinct. (another option is to hide
the standard ones by importing. We skip the details as they are language specific).

\subsubsection{Random access the element in binary}
\index{Binary Random Access List!Random access}

As trees in the forest help managing the elements in blocks, giving an arbitrary index,
it's easy to locate which tree this element are stored, after that performing a search 
in the tree yields the result. As all trees are binary (more accurate, complete binary
tree), the search is essentially binary search, which is bound to the logarithme
of the tree size. This brings us a faster random access capability than linear search
in linked-list setting.

Given an index $i$, and a sequence $S$, which is actually a forest of trees, the 
algorithm is executed as the following.

\begin{enumerate}
\item Compare $i$ with the size of the first tree $T_1$ in the forest, if $i$ is 
less than the size, the element exists in $T_1$, perform looking up in $T_1$;
\item Otherwise, decrease $i$ by the size of $T_1$, and repeat the previous step
in the rest of the trees in the forest.
\end{enumerate}

This algorithm can be represented as the below equation.

\be
get(S, i) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  lookupTree(T_1, i) & i < size(T_1) \\
  get(S', i-size(T_1)) & otherwise
  \end{array}
\right .
\ee

Where $S' = \{ T_2, T_3, ... \}$ is the rest of trees without the first one in the
forest. Note that we don't handle out of bound error cases, this is left as an exercise
to the reader.

Function `lookupTree' is just a binary search, if the index $i$ is 0, we just return the root
of the tree, otherwise, we halve the tree by unlinking, if $i$ is less than the size
of the halved tree, we recursively perform looking up the left tree, otherwise, we
look up the right tree.

\be
lookupTree(T, i) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  root(T) & i = 0 \\
  lookupTree(left(T)) & i < \lfloor \frac{size(T)}{2} \rfloor \\
  lookupTree(right(T)) & otherwise
  \end{array}
\right .
\ee

Where function `left' returns the left tree $T_l$ of $T$, while `right` returns $T_r$.

The corresponding Haskell program is given as below.

\begin{lstlisting}
getAt (t:ts) i = if i < size t then lookupTree t i
                 else getAt ts (i - size t)

lookupTree (Leaf x) 0 = x
lookupTree (Node sz t1 t2) i = if i < sz `div` 2 then lookupTree t1 i
                               else lookupTree t2 (i - sz `div` 2)
\end{lstlisting}

TODO: Add figures to illustrate this idea.

By using the similar idea, we can update element at any arbitrary position $i$.
We first compare the size of the first tree $T_1$ in the forest with $i$, if it is
less than $i$, it means the element to be updated doesn't exist in the first
tree. We recurisvely examine the next tree in the forest, comparing it with
$i - |T_1|$, where $|T_1|$ represents the size of the first tree. Otherwise
if this size is greater than or equal to $i$, the element is in the tree, 
we halve the tree recursively until to get a leaf, at this stage, we can
replace the element of this leaf with a new one.

\be
set(S, i, x) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{ updateTree(T_1, i, x) \} \cup S' & i < |T_1| \\
  \{T_1\} \cup set(S', i - |T_1|, x)
  \end{array}
\right .
\ee

Where $S' = \{ T_2, T_3, ...\}$ is the rest of the trees in the forest without
the first one.

Function $setTree(T, i, x)$ performs a tree search and replace the $i$-th element
with the given value $x$.

\be
setTree(T, i, x) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  leaf(x) & i = 0 \land |T| = 1 \\
  tree(|T|, setTree(T_l, i, x), t_r) & i < \lfloor \frac{|T|}{2} \rfloor \\
  tree(|T|, t_l, setTree(T_r, i - \lfloor \frac{|T|}{2} \rfloor, x))
  \end{array}
\right .
\ee

Where $T_l$ and $T_r$ are left and right sub tree of $T$ respectively. The following
Haskell program translates the equation accordingly.

\begin{lstlisting}
setAt :: BRAList a -> Int -> a -> BRAList a
setAt (t:ts) i x = if i < size t then (updateTree t i x):ts
                   else t:setAt ts (i-size t) x

updateTree :: Tree a -> Int -> a -> Tree a
updateTree (Leaf _) 0 x = Leaf x
updateTree (Node sz t1 t2) i x = 
    if i < sz `div` 2 then Node sz (updateTree t1 i x) t2
    else Node sz t1 (updateTree t2 (i - sz `div` 2) x)
\end{lstlisting}

As the nature of complete binary search tree, for a sequence with $N$ elements, which
is represented by binary random
access list, the number of trees in the forest is bound to $O(\lg N)$. Thus it takes
$O(\lg N)$ time to locate the tree for arbitrary index $i$, that contains the element.
the followed tree seach is bound the heights of the tree, which is $O(\lg N)$ as well.
So the total performance of random access is $O(\lg N)$. 

\begin{Exercise}
\begin{enumerate}
\item The random access algorithm given in this section doesn't handle the error such as
out of bound index at all. Modify the algorithm to handle these cases, and implement
it in your favorate programming langauge.

\item It's quite possible to realize the binary random access list in imperative settings,
which is benefited with fast operation on the head of the sequence. the random access
can be realized in two steps: firstly locate the tree, secondly use the capability of
constant random access of array. Write a program to implement it in your favorate imperative
programming language.
\end{enumerate}
\end{Exercise}

\section{Numeric representation for binary random access list}
\index{Sequence!numeric representation for binary access list}
In previous section, we mentioned that for any sequence with $N$ elements, we can 
represent $N$ in binary fomat so that $N = 2^0e_0 + 2^1e_1 + ... + 2^Me_M$. Where $e_i$
is the $i$-th bit, which can be either 0 or 1. If $e_i \neq 0$ it means that there is
a complete binary tree with size $2^i$.

This fact indicates us that there is an explicit relationship between the binary
form of $N$ and the forest. Insertion a new element on the head can be simulated
by increasing the binary number by one; while remove an element from the head mimics
the decreasing of the coresponding binary number by one. This is as known as 
{\em numeric representation} \cite{okasaki-book}.

In order to represent the binary access list with binary number, we can define two
states for a bit. That $Zero$ means there is no such a tree with size which is conresponding
to the bit, while $One$, means such tree exists in the forest. And we can attach
the tree with the state if it is $One$.

The following Haskell program for instance defines such states.

\begin{lstlisting}
data Digit a = Zero
             | One (Tree a)

type RAList a = [Digit a]
\end{lstlisting}

Here we reuse the definition of complete binary tree and attach it to the state $One$.
Note that we cached the size information in the tree as well.

With digit defined, forest can be treated as a list of digits. Let's see how insert
a new element can be realized as binary number increasing. Suppose function $one(t)$
creates a $One$ state and attaches tree $t$ to it. And function $getTree(s)$ get the
tree which is attached to the $One$ state $s$. 
The sequence $S$ is a list of digit
of state that $S = \{ s_1, s_2, ... \}$, and $S'$ is the rest of digits with the first
one removed.

\be
insertTree(S, t) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{ one(t) \} & S = \Phi \\
  \{ one(t) \} \cup S' & s_1 = Zero \\
  \{ Zero \} \cup insertTree(S', link(t, getTree(s_1))) & otherwise
  \end{array}
\right .
\ee

When we insert a new tree $t$ to a forest $S$ of binary digits, If the forest is empty, we 
just create a $One$ state, attach the tree to it, and make this state the only 
digit of the binary number. This is just like $0 + 1 = 1$; 

Otherwise if the forest isn't empty, we need examine the first digit of the binary number. If the first digit
is $Zero$, we just create a $One$ state, attach the tree, and replace the $Zero$ state
with the new created $One$ state. This is just like $(...digits...0)_2 + 1 = (...digits...1)_2$.
For example $6 + 1 = (110)_2 + 1 = (111)_2 = 7$. 

The last case is that the first
digit is $One$, here we make assumption that the tree $t$ to be inserted has the 
same size with the tree attached to this $One$ state at this stage. 
This can be ensured by calling this function from inserting a leaf, so that the size
of the tree to be inserted grows in a series of $1, 2, 4, ..., 2^i, ...$. In such
case, we need link these two trees (one is $t$, the other is the tree attached to 
the $One$ state), and recursively insert the linked result to the rest of the digits.
Note that the previous $One$ state has to be replaced with a $Zero$ state. This
is just like $(...digits...1)_2 + 1 = (...digits'...0)_2$, where $(...digits'...)_2 = (...digits...)_2+1$.
For example $7 + 1 = (111)_2 + 1 = (1000)_2 = 8$

Translating this algorithm to Haskell yeilds the following program.

\begin{lstlisting}
insertTree :: RAList a -> Tree a -> RAList a
insertTree [] t = [One t]
insertTree (Zero:ts) t = One t : ts
insertTree (One t' :ts) t = Zero : insertTree ts (link t t')
\end{lstlisting}

All the other functions, including $link(), cons()$ etc. are as same as before.

Next let's see how remove element from a seqence can be represented as binary number
deduction. If the sequence is a singleton $One$ state attached with a leaf.
After removal, it becomes empty. This is just like $1 - 1 = 0$;

Otherwise, we examine the first digit, if
it is $One$ state, it will be replaced with a $Zero$ state to indicate that
this tree will be no longer exist in the forest as it being removed.
This is just like $(...digits...1)_2 - 1 = (...digits...0)_2$. For example
$7 - 1 = (111)_2 - 1 = (110)_2 = 6$;

If the first digit in the sequence is a $Zero$ state, we have to borrow
from the further digits for removal. We recursively extract a tree from
the rest digits, and halve the extracted tree to its two children. Then
the $Zero$ state will be replaced with a $One$ state attached with the 
right children, and the left children is removed. This is something like
$(...digits...0)_2 - 1 = (...digits'...1)_2$, where 
$(...digits'...)_2 = (...digits)_2 - 1$. For example 
$4 - 1 = (100)_2 - 1 = (11)_2 = 3$.The following equation
illustrated this algorithm.

\be
extractTree(S) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (t, \Phi) & S = \{ one(t) \} \\
  (t, S') & s_1 = one(t) \\
  (t_l, \{ one(t_r) \} \cup S'' & otherwise
  \end{array}
\right .
\ee

Where $(t', S'') = extractTree(S')$, $t_l$ and $t_r$ are left and right
sub-trees of $t'$. All other functions, including $head()$, $tail()$ are
as same as before.

Numeric representation doesn't change the performance of binary random
access list, readers can refer to \cite{okasaki-ralist} for detailed
discussion.

\begin{Exercise}
\begin{enumerate}
\item Please implement the random access algorithms, including looking up and updating,
for binary random access list with numeric representation in your favorate programming
language.
\end{enumerate}
\end{Exercise}

\section{Concatenate-able list}
\index{Sequence!Concatenate-able list}
By using binary random access list, we realized sequence data structure which
support $O(\lg N)$ time insertion and removal on head, as well as random accessing element 
with a given index.

However, it's not so easy to concatenate two lists. As both lists are forests of
complete binary trees, we can't merely merge them (Since forests are essentailly
list of trees, and for any size, there is at most one tree of that size. Even
cacatenate forests directly is not fast). One solution is to push the element
from the first one by one to a stack and then pop those elements and insert
them to the head of the second one by using `cons' function. Of course the
stack can be implicitly used in recursion manner, for instance:

\be
concat(s_1, s_2) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  s_2 & s_1 = \Phi \\
  cons(head(s_1), concat(tail(s_1), s_2)) & otherwise
  \end{array}
\right .
\ee

Where function $cons()$, $head()$ and $tail()$ are defined in previous section.

If the length of the two sequence is $N$, and $M$, This method takes $O(N \lg N)$
time repeatedly push all elements from the first sequence to stacks, and then
takes $\Omega(N \lg (N + M))$ to insert the elements in front of the second sequence.
Note that $\Omega$ means the upper limit, There is detailed definiton for it in 
\cite{CLRS}.

We have already implemented the real-time queue in previous chapter. It supports
$O(1)$ time pop and push. If we can turn the sequence concatenation to a kind
of pushing operation to queue, the performance will be improved to $O(1)$ as well.
Okasaki gave such realization in \cite{okasaki-book}, which can concatenate
lists in constant time.

To represent a concatenate-able list, the data structure designe by Okasaki is 
essentially a K-ary tree. The root of the tree stores the first elements in the 
list. So that we can access it in constant $O(1)$ time. The sub-trees or children
are all small concatenate-able lists, which is managed by a real-time queue.
Concatenating another list to the end is just adding it as the last child, which
is in turn a queue pushing operation. Appending a new element can be realized
as that, first wrapping the element to a singleton tree, which is a leaf with
no children. Then, concatenate this singleton to finalize appending.

Figure \ref{fig:clist} illustrates this data structure.

\begin{figure}[htbp]
  \centering
  \subfloat[The data structure for list $\{ x_1, x_2, ..., x_n\}$]{\includegraphics[scale=0.5]{img/clist.ps}} \\
  \subfloat[The result after concatenated with list $\{y_1, y_2, ..., y_m\}$]{\includegraphics[scale=0.5]{img/clist1.ps}}
  \caption{Data structure for concatenate-able list} \label{fig:clist}
\end{figure}

Such recurisvely designed data structure can be defined in the following
Haskell code.

\begin{lstlisting}
data CList a = Empty | CList a (Queue (CList a)) deriving (Show, Eq)
\end{lstlisting}

It means that a concatenate-able list is either empty or a K-ary tree, which
again consists of a queue of concatenate-able sub-lists and a root element.
Here we reuse the realization of real-time queue mentioned in previous
chapter.

Suppose function $clist(x, Q)$ constructs a concatenate-able list from
an element $x$, and a queue of sub-lists $Q$. While function $root(s)$ 
returns the root element of such K-ary tree implemented list. and
function $queue(s)$ returns the queue of sub-lists respectively.
We can implement the algorithm
to concat two lists like this.

\be
concat(s_1, s_2) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  s_1 & s_2 = \Phi \\
  s_2 & s_1 = \Phi \\
  clist(x, push(Q, s_2)) & otherwise
  \end{array}
\right .
\ee

Where $x = root(s_1)$ and $Q = queue(s_1)$. The idea of concatenation is that
if either one of the list to be concatenated is empty, the result is 
just the other list; otherwise, we push the second list as the last
child to the queue of the first list.

Since the push operation is $O(1)$ constant time for a well realized
real-time queue, the performance of concatenation is bound to $O(1)$.

The $concat()$ function can be translated to the below Haskell program.

\begin{lstlisting}
concat x Empty = x
concat Empty y = y
concat (CList x q) y = CList x (push q y)
\end{lstlisting}

Besides the good performance of concatenation, this design also brings
satisified featurs for adding element both on head and tail.

\be
cons(x, s) = concat(clist(x, \Phi), s)
\ee

\be
append(s, x) = concat(s, clist(x, \Phi))
\ee

It's a bit complex to realize the algorithm that removes the first
element from a concatenate-able list. This is because after the root,
which is the first element in the sequence got removed, we have to 
re-construct the rest things, a queue of sub-lists, to a K-ary
tree.

Before diving into the re-construction, let's solve the trivial
part first. Getting the first element is just returning the root
of the K-ary tree.

\be
head(s) = root(s)
\ee

As we mentioned above, after root being removed, there left
all children of the K-ary tree. Note that all of them are also
concatenate-able list, so that one naturual solution is to
concatenate them all together to a big list. 

\be
concatAll(Q) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & Q = \Phi \\
  concat(front(Q), concatAll(pop(Q))) & otherwise
  \end{array}
\right .
\ee

Where function $front()$ just return the first element from a
queue without removing it, while $pop()$ doest the removing work.

If the queue is empty, it means that there is no children at all, so
the result is also an empty list; Otherwise, we pop the first child,
which is a concatenate-able list, from the queue, and recursively
concat all the rest children to a list; finally, we concat this list
behind the already popped first children.

With $concatAll()$ defined, we can then implement the algorithm 
of removing the firest element from a list as below.

\be
tail(s) = linkAll(queue(s))
\ee

The corresponding Haskell program is given like the following.

\begin{lstlisting}
head (CList x _) = x 
tail (CList _ q) = linkAll q

linkAll q | isEmptyQ q = Empty
          | otherwise = link (front q) (linkAll (pop q))
\end{lstlisting}

Function `isEmptyQ' is used to test a queue is empty, it is trivial and 
we omit its definition. Readers can refer to the source code along with
this book.

$linkAll()$ algorithm actually traverses the queue data structure,
and reduces to a final result. This remind us of {\em folding} mentioned
in the chapter of binary search tree. readers can refer to the
appendix of this book for the detailed description of folding.
It's quite possible to define a folding algorithm for queue instead
of list\footnote{Some function programming languageg, such as 
Haskell, deinfed type class, which is a concept of monoid so that
it's easy to support folding on a customized data structure.}
\cite{learn-haskell}.

\be
foldQ(f, e, Q) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  e & Q = \Phi \\
  f(front(Q), foldQ(f, e, pop(Q))) & otherwise
  \end{array}
\right .
\ee

Function $foldQ()$ takes three parameters, a function $f$, which is used
for reducing, an initial value $e$, and the queue $Q$ to be traversed.

Here are some examples to illustrate folding on queue. Suppose
a queue $Q$ contains elements $\{ 1, 2, 3, 4, 5 \}$ from head to tail.

\[
\begin{array}{l}
foldQ(+, 0, Q) = 1 + (2 + (3 + (4 + (5 + 0)))) = 15 \\
foldQ(\times, 1, Q) = 1 \times (2 \times (3 \times (4 \times (5 \times 1)))) = 120 \\
foldQ(\times, 0, Q) = 1 \times (2 \times (3 \times (4 \times (5 \times 0)))) = 0 \\
\end{array}
\]

Function $linkAll$ can be changed by using $foldQ$ accordingly.

\be
linkAll(Q) = foldQ(link, \Phi, Q)
\ee

The Haskell program can be modified as well.

\begin{lstlisting}
linkAll = foldQ link Empty

foldQ :: (a -> b -> b) -> b -> Queue a -> b
foldQ f z q | isEmptyQ q = z
            | otherwise = (front q) `f` foldQ f z (pop q)
\end{lstlisting}

However, the performance of removing can't be ensured in all cases. 
The worst case is that, user keeps appending $N$ elements to a empty
list, and then immediately performs removing. At this time, the K-ary
tree has the first element stored in root. There are $N-1$ children,
all are leaves. So $linkAll()$ algorithm downgrades to $O(N)$ which
is linear time.

The average case is amortized $O(1)$, if the add, append, concatenate
and removing operations are randomly performed. The proof is left as
en exercise to the reader.

\begin{Exercise}
\begin{enumerate}
\item Can you figure out a solution to append an element to the end of a binary
random access list?

\item Prove that the amortized performance of removal operation is
$O(1)$. Hint: using the banker's method.
\end{enumerate}
\end{Exercise}

\section{Finger tree}
\index{Sequence!finger tree}
We haven't been able to meet all the performance targets listed at the beginning
of this chapter. 

Binary random access list enables to insert, remove element
on the head of sequence, and random access elements fast. However, it performs
poor when concatenates lists. There is no good way to append element at the
end of binary access list.

Concatenate-able list is capable to concatenates mutliple lists in a fly, and
it performs well for adding new element both on head and tail. However, it doesn't
suppor randomly access element with a given index.

Theset two examples bring us some ideas:

\begin{itemize}
\item In order to support fast manipulation both on head and tail of the sequence, 
there must be some way to easily access the head and tail position;
\item Tree like data structure helps to turn the random access into divide and 
conquer search, if the tree is well balance, the serach can be ensured to
be logarithm time.
\end{itemize}

\subsection{Definition}

Finger tree\cite{finger-tree-1977}, which was first invented in 1977, can help to 
realize efficient sequence. And it is also well implemented in purely functional
settings\cite{finger-tree-2006}.

As we mentioned that the balance of the tree is critical to ensure the performance
for search. One option is to use balanced tree as the under ground data structure
for finger tree. For example the 2-3 tree, which is a special B-tree. (readers
can refer to the chapter of B-tree of this book).

A 2-3 tree either contains 2 children or 3. It can be defined as below in Haskell.

\begin{lstlisting}
data Node a = Br2 a a | Br3 a a a 
\end{lstlisting}

We mark the left-most none-leaf node as the front finger and the right-most
none-leaf node as the rear finger. Since both figures are essentially 
2-3 trees with all leafs as children, the can be directly represents as 
list of 2 or 3 leafs. Of course a finger tree can be empty or contains
only one one element as leaf.

So the definition of a finger tree is specified like this.

\begin{itemize}
\item A finger tree is either empty;
\item or a singleton leaf;
\item or contains three parts: a left finger which is a list contains at most
3 elements; a sub finger tree; and a right finger which is also a list contains
at most 3 elements.
\end{itemize}

Note that this definition is recursive, so it's quite possible to be translated
to functional settings. The following Haskell definition summary these cases 
for example.

\begin{lstlisting}
data Tree a = Empty
            | Lf a
            | Tr [a] (Tree (Node a)) [a]
\end{lstlisting}

Figure \ref{fig:ftr-example-1} and \ref{fig:ftr-example-2} show some examples
of figure tree.

\begin{figure}[htbp]
  \centering
  \subfloat[An empty tree]{\hspace{0.2\textwidth}\includegraphics[scale=0.5]{img/ftr-empty.ps}\hspace{0.2\textwidth}}
  \subfloat[A singleton leaf]{\hspace{0.2\textwidth}\includegraphics[scale=0.5]{img/ftr-leaf.ps}\hspace{0.2\textwidth}} \\
  \subfloat[Front finger and rear finger contain one element for each, the middle part is empty]{\includegraphics[scale=0.5]{img/ftr-ab.ps}}
  \caption{Examples of finger tree, 1} \label{fig:ftr-example-1}
\end{figure}

\begin{figure}[htbp]
  \centering
  \subfloat[After inserting extra 3 elements to front finger, it exceeds the 2-3 tree constraint, which isn't balanced any more]{\includegraphics[scale=0.5]{img/ftr-abcde.ps}}
  \hspace{0.2\textwidth}
  \subfloat[The tree resumes balancing. There are 2 elements in front finger; The middle part is a leaf, which contains a 3-branches 2-3 tree.]{\includegraphics[scale=0.5]{img/ftr-abcdef.ps}}
  \caption{Examples of finger tree, 2} \label{fig:ftr-example-2}
\end{figure}

The first example is an empty finger tree; the second one shows the result after
inserting one element to empty, it becomes a leaf of one node; the third example
shows a finger tree contains 2 elements, one is in front finger, the other is
in rear; 

If we continously insert new elements, to the tree, those elements will be
put in the front finger one by one, until it exeeds the limit of 2-3 tree.
The 4-th example shows such condition, that there are 4 elements in front
finger, which isn't balanced any more.

The last example shows that the finger tree gets fixed so that it resumes
balancing. There are two elements in the front finger. Note that the middle
part is not empty any longer. It's a leaf of a 2-3 tree. The content of the 
leaf is a tree with 3 branches, eash contains an elements.

We can express these 5 examples as the following Haskell expression.

\begin{lstlisting}
Empty
Lf a
[b] Empty [a]
[e, d, c, b] Empty [a]
[f, e] Lf (Br3 d c b) [a]
\end{lstlisting}

As we mentioned that the definition of finger tree is recursive.
The middle part besides the front and rear finger is a deeper finger tree,
which is defined as $Tree(Node(a))$. Every time we go deeper, the 
$Node()$ is embedded one more level. if the element type of the first level tree
is $a$, the element type for the second level tree is $Node(a)$,
the third level is $Node(Node(a))$, ..., the n-th level is
$Node(Node(Node(...(a))...)) = Node^n(a)$, where $^n$ indicates the $Node()$
is applied $N$ times.

\subsection{Insert element to the head of sequence}

The examples list above actually reveals the typical process that the
elements are inserted one by one to a finger tree. It's possible to summarize
these examples to some cases for insertion on head algorithm.

When we insert an element $x$ to a finger tree $T$,
\begin{itemize}
\item If the tree is empty, the result is a leaf which contains the singleton element $x$;
\item If the tree is a singleton leaf of element $y$, the result is a new finger tree. The front finger contains the new element $x$, the rear finger contains the old element $y$; the middle part is a empty finger tree;
\item If the number of elements store in front finger isn't bigger than the upper limit of 2-3 tree, which is 3, the new element is just insert on the head
of front finger;
\item otherwise, it means that the number of elements stored in front finger exceeds the upper limit of 2-3 tree. the last 3 elements in front finger is wrapped in a 2-3 tree and recursively inserted to the middle part. the new element $x$ is inserted in front of the rest elements in front finger.
\end{itemize}

Suppose that function $leaf(x)$ creates a leaf of element $x$, function
$tree(F, T', R)$ creates a finger tree from three part: $F$ is the front
finger, which is a list contains several elements. Similarity, $R$ is the
rear finger, which is also a list. $T'$ is the middle part which is a 
deeper finger tree. Function $tr3(a, b, c)$ creates a 2-3 tree from 3 
elements $a, b, c$; while $tr2(a, b)$ creates a 2-3 tree from 2 elements
$a$ and $b$.

\be
insertT(x, T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  leaf(x) & T = \Phi \\
  tree(\{x\}, \Phi, \{y\}) & T = leaf(y) \\
  tree(\{x, x_1\}, insertT(tr3(x_2, x_3, x_4), T'), R) & T = tree(\{x_1, x_2, x_3, x_4\}, T', R) \\
  tree(\{x\} \cup F, T', R) & otherwise
  \end{array}
\right .
\ee

The performance of this algorithm is dominated by the recursive case. All the
other cases are constant $O(1)$ time. The recursion depth is proportion to
the height of the tree, so the algorithm is bound to $O(h)$ time, where $h$ is
the height. As we use 2-3 tree to ensure that the tree is well balanced,
$h = O(\lg N)$, where $N$ is the number of elements stored in the finger tree.

More analysis reveal that the amortized performance of $insertT()$ is $O(1)$
because we can amortize the expensive recursion case to other trivial cases.
Please refer to \cite{okasaki-book} and \cite{finger-tree-2006} for the detailed proof.

Translating the algorithm yields the below Haskell program.

\begin{lstlisting}
cons :: a -> Tree a -> Tree a
cons a Empty = Lf a
cons a (Lf b) = Tr [a] Empty [b]
cons a (Tr [b, c, d, e] m r) = Tr [a, b] (cons (Br3 c d e) m) r
cons a (Tr f m r) = Tr (a:f) m r
\end{lstlisting}

Here we use the LISP naming convention to illustrate inserting a
new element to a list. 

\subsection{Remove element from the head of sequence}

It's easy to implement the reverse operation
that remove the first element from the list by line by line
reverse the $insertT()$ algorithm.

Let's denote $F = \{f_1, f_2, ...\}$ is the front finger list, 
$M$ is the middle part inner finger tree. $R = \{r_1, r_2, ...\}$ 
is the rear finger list of a finger tree,
and $R' = \{r_2, r_3, ... \}$ is the rest of element with the first one
removed from $R$. 

\be
extractT(T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (x, \Phi) & T = leaf(x) \\
  (x, leaf(y)) & T = tree(\{x\}, \Phi, \{y\}) \\
  (x, tree(\{r_1\}, \Phi, R')) & T = tree(\{x\}, \Phi, R) \\
  (x, tree(toList(F'), M', R)) & T = tree(\{x\}, M, R), (F', M') = extractT(M)\\
  (f_1, tree(\{f_1, f_2, ...\}, M, R)) & otherwise
  \end{array}
\right .
\ee

Where function $toList(T)$ converts a 2-3 tree to plain list as the
following.

\be
toList(T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{x, y\} & T = tr2(x, y) \\
  \{x, y, z\} & T = tr3(x, y, z)
  \end{array}
\right .
\ee

Here we skip the error handling such as trying to remove element from
empty tree etc. If the finger tree is a leaf, the result after removal
is an empty tree; If the finger tree contains two elements, one in the 
front rear, the other in rear, we return the element stored in front rear
as the first element, and the resulted tree after removal is a leaf;
If there is only one element in front finger, the middle part inner tree
is empty, and the rear finger isn't empty, we return the only element
in front finger, and borrow one element from the rear finger to
front; If there is only one element in front finger, however, the
middle part inner tree isn't empty, we can recursively remove a node
from the inner tree, and falttern it to a plain list to replace the
front finger, and remove the orignal only element in front finger;
The last case says that if the front finger contains more than one
element, we can just remove the first element from front finger
and keep all the other part unchanged.

TODO: Add figures to illustrate the delete process

Below is the corresponding Haskell program for `uncons'.

\begin{lstlisting}
uncons :: Tree a -> (a, Tree a)
uncons (Lf a) = (a, Empty)
uncons (Tr [a] Empty [b]) = (a, Lf b)
uncons (Tr [a] Empty (r:rs)) = (a, Tr [r] Empty rs)
uncons (Tr [a] m r) = (a, Tr (nodeToList f) m' r) where (f, m') = uncons m
uncons (Tr f m r) = (head f, Tr (tail f) m r)
\end{lstlisting}

And the function $nodeToList$ is defined like this.

\begin{lstlisting}
nodeToList :: Node a -> [a]
nodeToList (Br2 a b) = [a, b]
nodeToList (Br3 a b c) = [a, b, c]
\end{lstlisting}

Similar as above, we can define $head$ and $tail$ function from
$uncons$.

\begin{lstlisting}
head = fst . uncons
tail = snd . uncons
\end{lstlisting}

\subsection{append element to the tail of the sequence}

Because finger tree is symmetric, we can give the realization of appending element on tail
by referencing to $insertT()$ algorithm.

\be
appendT(T, x) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  leaf(x) & T = \Phi \\
  tree(\{y\}, \Phi, \{x\}) & T = leaf(y) \\
  tree(F, appendT(M, tr3(x_1, x_2, x_3)), \{x_4, x\}) & T = tree(F, M, \{x_1, x_2, x_3, x_4\}) \\
  tree(F, M, R \cup \{x\}) & otherwise
  \end{array}
\right .
\ee

Generally speaking, if the rear finger is still valid 2-3 tree, that the number of elements
is not greater than 4, the new elements is directly appended to rear finger.
Otherwise, we break the rear finger, take the first 3 element in rear finger to create a 
new 2-3 tree, and recursively append it to the middle part inner tree.
If the finger tree is empty or a singleton leaf, they are handled in the first two cases.

Translating the equation to Haskell yields the below program.

\begin{lstlisting}
snoc :: Tree a -> a -> Tree a
snoc Empty a = Lf a
snoc (Lf a) b = Tr [a] Empty [b]
snoc (Tr f m [a, b, c, d]) e = Tr f (snoc m (Br3 a b c)) [d, e]
snoc (Tr f m r) a = Tr f m (r++[a])
\end{lstlisting}

Function name `snoc' is mirror of 'cons', which indicates the symmetric relationship.

\subsection{remove element from the tail of the sequence}

Similar to $appendT()$, we can realize the algorithm which remove the last element from
finger tree in symmetric manner of $extractT()$.

We denote the none-empty, none-leaf finger tree as $tree(F, M, R)$, where $F$ is the
front finger, $M$ is the middle part inner tree, and $R$ is the rear finger.

\be
removeT(T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\Phi, x) & T = leaf(x) \\
  (leaf(y), x) & T = tree(\{y\}, \Phi, \{x\}) \\
  (tree(init(F), \Phi, last(F)), x) & T = tree(F, \Phi, \{x\}) \land F \neq \Phi \\
  (tree(F, M', toList(R')), x) & T = tree(F, M, \{x\}), (M', R') = removeT(M) \\
  (tree(F, M, init(R)), last(R)) & otherwise
  \end{array}
\right .
\ee

Function $toList(T)$ is used to flattern a 2-3 tree to plain list, which is defined
previously. Function $init(L)$ returns all elements except for the last one in list $L$,
that if $L = \{a_1, a_2, ..., a_{n-1}, a_n\}$, $init(L) = \{a_1, a_2, ..., a_{n-1}\}$.
And Function $last(L)$ returns the last element, so that $init(L) = a_n$. Please
refer to the appendix of this book for their implementation.

Algorithm $removeT()$ can be translated to the following Haskell program, we name
it as `unsnoc' to indicate it's the reverse function of `snoc'.

\begin{lstlisting}
unsnoc :: Tree a -> (Tree a, a)
unsnoc (Lf a) = (Empty, a)
unsnoc (Tr [a] Empty [b]) = (Lf a, b)
unsnoc (Tr f@(_:_) Empty [a]) = (Tr (init f) Empty [last f], a)
unsnoc (Tr f m [a]) = (Tr f m' (nodeToList r), a) where (m', r) = unsnoc m
unsnoc (Tr f m r) = (Tr f m (init r), last r)
\end{lstlisting}

And we can define a special function `last' and 'init' for finger tree which is similar
to their counterpart for list.

\begin{lstlisting}
last = snd . unsnoc
init = fst . unsnoc
\end{lstlisting}

\subsection{concatenate}

Consider the none-trivial case that concatenate two finger tree $T_1 = tree(F_1, M_1, R_1)$ and
$T_2 = tree(F_2, M_2, R_2)$. One natural idea is to use $F_1$ as the new front finger for the 
cacatenated result, and keep $R_2$ is the new rear finger. The rest of work is to merge
$M_1$, $R_1$, $F_2$ and $M_2$ to a new middle part inner tree.

Note that both $R_1$ and $F_2$ are plain lists of node, so the sub-problem is to realize a 
algorithm like this.

\[
merge(M_1, R_1 \cup F_2, M_2) = ?
\]

More observation reveals that both $M_1$ and $M_2$ are also finger trees, except that they
are one level deeper than $T_1$ and $T_2$ in terms of $Node(a)$, where $a$ is the type of 
element stored in the tree. We can recursively use the strategy that keep the front finger
of $M_1$ and the rear finger of $M_2$, then merge the middle part inner tree of $M_1$, $M_2$,
as well as the rear finger of $M_1$ and front finger of $M_2$.

If we denote function $front(T)$ returns the front finger, $rear(T)$ returns the rear finger,
$mid(T)$ returns the middle part inner tree. the above $merge()$ algorithm can be
expressed for none-trivial case as the following.

\be
\begin{array}{l}
merge(M_1, R_1 \cup F_2, M_2) = tree(front(M_1), S, rear(M_2)) \\
S = merge(mid(M_1), rear(M_1) \cup R_1 \cup F_2 \cup front(M_2), mid(M_2))
\end{array}
\label{eq:merge-recursion}
\ee

If we look back to the original concatenate soltuion, which can be expressed as.

\be
concat(T_1, T_2) = tree(F_1, merge(M_1, R_1 \cup R_2, M_2), R_2)
\ee

And compare it with equation \ref{eq:merge-recursion}, it's easy to note the fact that 
concatenating is essentally merging. So we have the final algorithm as this.

\be
concat(T_1, T_2) = merge(T_1, \Phi, T_2)
\ee

By adding edge cases, the $merge()$ algorithm can be completed as below.

\be
merge(T_1, S, T_2) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  foldR(insertT, T_2, S) & T_1 = \Phi \\
  foldL(appendT, T_1, S) & T_2 = \Phi \\
  merge(\Phi, \{x\} \cup S, T_2) & T_1 = leaf(x) \\
  merge(T_1, S \cup \{x\}, \Phi) & T_2 = leaf(x) \\
  tree(F_1, merge(M_1, nodes(R_1 \cup S \cup F_2), M2), R_2) & otherwise
  \end{array}
\right .
\ee

Most of them are straightforward, if any one of $T_1$ or $T_2$ is empty, the algorithm
repeatedly insert/append all elements in $S$ to the other tree; Function $foldL$ and
$foldR$ are kinds of for-each process in imperative settings. The difference is that
$foldL$ process the list $S$ from left to right while $foldR$ process from right to left.

Here are their definition. Suppose list $L=\{ a_1, a_2, ..., a_{n-1}, a_n\}$, 
$L' = \{ a_2, a_3, ..., a_{n-1}, a_n\}$ is the rest of elements except for the first one.

\be
foldL(f, e, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  e & L = \Phi \\
  foldL(f, f(e, a_1), L') & otherwise
  \end{array}
\right .
\ee

\be
foldR(f, e, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  e & L = \Phi \\
  f(a_1, foldR(f, e, L')) & otherwise
  \end{array}
\right .
\ee

They are detailed explained in the appendix of this book.

If either one of the tree is a leaf, we can insert or append the element of this leaf to
$S$, so that it becomes the trivial case of concatenate one empty tree with another.

Function $nodes()$ is used to wrap a list of elements to a list of 2-3 trees. 
This is because the contents of middle part inner tree, compare to the
contents of finger are one level deeper in terms of $Node()$. Consider the
time point that transforms from recursive case to edget case. Let's suppose
$M_1$ is empty at that time, we then need repeatedly insert all elements from
$R_1 \cup S \cup F_2$ to $M_2$. However, we can't directly do the insertion.
If the element type is $a$, we can only insert $Node(a)$ which is 2-3 tree
to $M_2$. This is just like what we did in the $insertT()$ algorithm,
take out the last 3 elements, wrap them in a 2-3 tree, and recursive
perform $insertT()$. Here is the definition of $nodes()$.

\be
nodes(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{tr2(x_1, x_2)\} & L = \{x_1, x_2\} \\
  \{tr3(x_1, x_2, x_3)\} & L = \{x_1, x_2, x_3\} \\
  \{tr2(x_1, x_2), tr2(x_3, x_4)\} & L = \{x_1, x_2, x_3, x_4\} \\
  \{tr3(x_1, x_2, x_3)\} \cup nodes(\{x_4, x_5, ...\}) & otherwise
  \end{array}
\right .
\ee

Function $nodes()$ follows the constraint of 2-3 tree, that if there are
only 2 or 3 elements in the list, it just wrap them in singleton list
contains a 2-3 tree; If there are 4 elements in the lists, it split them
into two trees each is consist of 2 branches; Otherwise, if there are
more elements than 4, it wraps the first three in to one tree with 3 branches,
and recursively call $nodes()$ to process the rest.

The performance of concatenation is determined by merging. Analyze the 
recursive case of merging reveals that the depth of recursion is 
proportion to the smaller height of the two trees. As the tree is
ensured to be balanced by using 2-3 tree. it's height is bound to 
$O(\lg N')$ where $N'$ is the number of elements. The edge
case of merging performs as same as insertion, (It calls $insertT()$
at most 8 times) which is amortized $O(1)$ time, and $O(\lg M)$
at worst case, where $M$ is the difference in height of the two trees.
So the overall performance is bound to $O(\lg N)$, where $N$ is
the total number of elements contains in two finger trees.

The following Haskell program impelements the concatenation algorithm.

\begin{lstlisting}
concat :: Tree a -> Tree a -> Tree a
concat t1 t2 = merge t1 [] t2
\end{lstlisting}

Note that there is `concat' function defined in prelude standard library,
so we need distinct them either by hiding import or take a different name.

\begin{lstlisting}
merge :: Tree a -> [a] -> Tree a -> Tree a
merge Empty ts t2 = foldr cons t2 ts
merge t1 ts Empty = foldl snoc t1 ts
merge (Lf a) ts t2 = merge Empty (a:ts) t2
merge t1 ts (Lf a) = merge t1 (ts++[a]) Empty
merge (Tr f1 m1 r1) ts (Tr f2 m2 r2) = Tr f1 (merge m1 (nodes (r1 ++ ts ++ f2)) m2) r2
\end{lstlisting}

And the implementation of $nodes()$ is as below.

\begin{lstlisting}
nodes :: [a] -> [Node a]
nodes [a, b] = [Br2 a b]
nodes [a, b, c] = [Br3 a b c]
nodes [a, b, c, d] = [Br2 a b, Br2 c d]
nodes (a:b:c:xs) = Br3 a b c:nodes xs
\end{lstlisting}

\begin{Exercise}
\begin{enumerate}
\item Try to implement concatenation algorithm without using folding. You can either use
recursive methos, or use imperative for-each method.
\end{enumerate}
\end{Exercise}

\subsection{Random access of finger tree}

\subsubsection{size augmentation}
The strategy to provide fast random access, is to turn the looking up into tree-search.
In order to avoid calculating the size of tree many times, we augment an extra field 
to trees and nodes. The definition should be modified accordingly, for example the
following Haskell definition added size field in its constructor.

\begin{lstlisting}
data Tree a = Empty
            | Lf a
            | Tr Int [a] (Tree (Node a)) [a]
\end{lstlisting}

Suppose the function $tree(s, F, M, R)$ creates a finger tree from size $s$, front
finger $F$, rear finger $R$, and middle part inner tree $M$.
When the size of the tree is needed, we can call a $size(T)$ function. It will be
someting like this.

\[
size(T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  0 & T = \Phi \\
  ? & T = leaf(x) \\
  s & T = tree(s, F, M, R)
  \end{array}
\right .
\]

If the tree is empty, the size is definately zero; and if it can be expressed as $tree(s, F, M, R$,
the size is $s$; however, what if the tree is a singleton leaf? is it 1? No, it 
can be 1 only if $T = leaf(a)$ and $a$ isn't a tree node, but an element stored in finger tree.
In most cases, the size is not 1, because $a$ can be again a tree node. That's why we
put a `?' in above equation.

The correct way is to call some size function on the tree node as the following.

\be
size(T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  0 & T = \Phi \\
  size'(x) & T = leaf(x) \\
  s & T = tree(s, F, M, R)
  \end{array}
\right .
\ee

Note that this isn't a resurive definition since $size \neq size'$, the argument to $size'$
is either a tree node, which is a 2-3 tree, or a plain element stored in the finger tree.
To uniform these two cases, we can anyway wrap the single plain element to a tree node
of only one element. So that we can express all the situation as a tree node augmented
with a size field. The following Haskell program modifies the definition of tree node.

\begin{lstlisting}
data Node a = Br Int [a]
\end{lstlisting}

Suppose function $tr(s, L)$, create such a node (either one element being wrapped or a 2-3 tree)
from a size information $s$, and a list $L$. Here are some example.

\[
\begin{array}{ll}
tr(1, \{x\}) & \text{a tree contains only one element} \\
tr(2, \{x, y\}) & \text{a 2-3 tree contains two elements} \\
tr(3, \{x, y, z\}) & \text{a 2-3 tree contains three elements}
\end{array}
\]

So the function $size'$ can be implemented as returning the size information of a tree node.
We have $size'(tr(s, L)) = s$.

Wrapping an element $x$ is just calling $tr(1, \{x\})$. We can define auxiliary functions
$wrap$ and $unwrap$, for instance.

\be
\begin{array}{l}
wrap(x) = tr(1, \{x\}) \\
unwrap(n) = x \quad:\quad n = tr(1, \{x\})
\end{array}
\ee

As both front finger and rear finger are list of tree nodes, in order to calculate the 
total size of finger, we can provide a $size''(L)$ function, which sums up size of all
nodes stored in the list. Denote $L = \{ a_1, a_2, ... \}$ and $L' = \{ a_2, a_3, ... \}$.

\be
size''(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  0 & L = \Phi \\
  size'(a_1) + size''(L') & otherwise
  \end{array}
\right .
\ee

It's quite OK to define $size''(L)$ by using folding.

\be
size'' = fold \circ + \circ 0
\ee

And we can turn a list of tree nodes into one deeper 2-3 tree and vice-versa.

\be
\begin{array}{l}
wraps(L) = tr(size''(L), L) \\
unwraps(n) = L \quad:\quad n = tr(s, L) \\
\end{array}
\ee

These helper functions are translated to the following Haskell code.

\begin{lstlisting}
size (Br s _) = s

sizeL = sum .(map size)

sizeT Empty = 0
sizeT (Lf a) = size a
sizeT (Tr s _ _ _) = s
\end{lstlisting}

Here here are the wrap and unwrap auxiliary functions.

\begin{lstlisting}
wrap x = Br 1 [x]
unwrap (Br 1 [x]) = x
wraps xs = Br (sizeL xs) xs
unwraps (Br _ xs) = xs
\end{lstlisting}

We omitted their type definitions for illustration purpose.

\subsubsection{Modification due to the augmented size}

The algorithms have been presented so far need to be modified to accomplish with the
augmented size. For example the $insertT()$ function now inserts a tree node instead
of a plain element.

\be
insertT(x, T) = insertT'(wrap(x), T)
\ee

The corresponding Haskell program is changed as below.

\begin{lstlisting}
cons a t = cons' (wrap a) t
\end{lstlisting}

After being wrapped, $x$ is augmented with size infomration of 1. In the implementation
of previous insertion algorithm, function $tree(F, M, R)$ is used to create a finger tree
from a front finger, a middle part inner tree and a rear finger. This function should
also be modified to add size information of these three arguments.

\be
tree'(F, M, R) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  foldR(insertT', \Phi, F) & M = \Phi \land R = \Phi \\
  foldR(insertT', \Phi, R) & M = \Phi \land F = \Phi \\
  tree'(unwraps(F'), M', R) & F = \Phi, (F', M') = extractT'(M) \\
  tree'(F, M', unwraps(R')) & R = \Phi, (M', R') = removeT'(M) \\
  tree(size''(F) + size(M) + size''(R), F, M, R) & otherwise
  \end{array}
\right .
\ee

The last case is the most straightforward one. If none of $F$, $M$, and $R$ is empty,
it adds the size of these three part and construct the tree along with this size information
by calling $tree(s, F, M, R)$ function.
If both the middle part innser tree and one of the finger is empty, the algorithm 
repeatedly insert all elements stored in the other finger to an empty tree, so that
the result is constructed from a list of tree node.
If the middle part inner tree isn't empty, and one of the finger is empty, the 
algorithm `borrows' one tree node from the middle part, either by extract from
head if front finger is empty or remove from tail if rear finger is empty.
Then the algorithm unwraps the `borrowed' tree node to a list, and recursively
call $tree'()$ function to construct the result.

This algorithm can be translated to the following Haskell code for example.

\begin{lstlisting}
tree f Empty [] = foldr cons' Empty f
tree [] Empty r = foldr cons' Empty r
tree [] m r = let (f, m') = uncons' m in tree (unwraps f) m' r
tree f m [] = let (m', r) = unsnoc' m in tree f m' (unwraps r)
tree f m r = Tr (sizeL f + sizeT m + sizeL r) f m r
\end{lstlisting}

Function $tree'()$ helps to minimize the modification. $insertT'()$ can be 
realized by using it like the following.

\be
insertT'(x, T) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  leaf(x) & T = \Phi \\
  tree'(\{x\}, \Phi, \{y\}) & T = leaf(x) \\
  tree'(\{x, x_1\}, insertT'(wraps(\{x_2, x_3, x_4\}), M), R) & T = tree(s, \{x_1, x_2, x_3, x_4\}, M, R) \\
  tree'(\{x\} \cup F, M, R) & otherwise
  \end{array}
\right .
\ee

And it's corresponding Haskell code is a line by line translation.

\begin{lstlisting}
cons' a Empty = Lf a
cons' a (Lf b) = tree [a] Empty [b]
cons' a (Tr _ [b, c, d, e] m r) = tree [a, b] (cons' (wraps [c, d, e]) m) r
cons' a (Tr _ f m r) = tree (a:f) m r
\end{lstlisting}

For simplification purpose, we skipped the detailed description of what are modified in
$extractT()'$, $appendT()$, $removeT()$, and $concat()$ algorithms. They are left as exercises to the
reader.

\subsubsection{Split a finger tree at a given position}

With size information augmented, it's easy to locate a node at given position by performing
a tree search. What's more, as the finger tree is constructed from three part $F$, $M$, and
$R$; and it's nature of recursive, it's also possible to split it into three sub parts with
a given position $i$: the left, the node at $i$, and the right part.

The idea is straight forward. Since we have the size information for $F$, $M$, and $R$. Denote
these three sizes as $S_f$, $S_m$, and $S_r$. if the given position $i \leq S_f$, the node must
be stored in $F$, we can go on seeking the node inside $F$; if $S_f < i leq S_f + S_m $, the
node must be stored in $M$, we need recursively perform serach in $M$; otherwise, the node
should be in $R$, we need search inside $R$.

If we skip the error handling of trying split an empty tree, there is only one edge case
as below.

\be
splitAt(i, T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\Phi, x, \Phi) & T = leaf(x) \\
  ... & otherwise
  \end{array}
\right .
\ee

\begin{Exercise}
\begin{enumerate}
\item Another way to realize $insertT'()$ is to force increasing the size field by one, so
that we needn't write function $tree'()$. Try to realize the algorithm by using this idea.

\item Try to handle the augment size information as well as in $insertT'()$ algorithm for
the following algorithms: $extractT()'$, $appendT()$, $removeT()$, and $concat()$. The `head', `tail',
`init' and `last' functions should be kept unchanged. Don't refer to the downloadable
programs along with this book before you take a try.
\end{enumerate}
\end{Exercise}

% ================================================================
%                 Short summary
% ================================================================
\section{Notes and short summary}

TODO: Notes and summary here

% ================================================================
%                 Appendix
% ================================================================

\begin{thebibliography}{99}

\bibitem{okasaki-book}
Chris Okasaki. ``Purely Functional Data Structures''. Cambridge university press, (July 1, 1999), ISBN-13: 978-0521663502

\bibitem{okasaki-ralist}
Chris Okasaki. ``Purely Functional Random-Access Lists''. Functional Programming Languages and Computer Architecutre, June 1995, pages 86-95.

\bibitem{CLRS}
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein. ``Introduction to Algorithms, Second Edition''. The MIT Press, 2001. ISBN: 0262032937.

\bibitem{learn-haskell}
Miran Lipovaca. ``Learn You a Haskell for Great Good! A Beginner's Guide''. No Starch Press; 1 edition April 2011, 400 pp. ISBN: 978-1-59327-283-8

\bibitem{finger-tree-2006}
Ralf Hinze and Ross Paterson. ``Finger Trees: A Simple General-purpose Data Structure." in Journal of Functional Programming16:2 (2006), pages 197-217. http://www.soi.city.ac.uk/~ross/papers/FingerTree.html

\bibitem{finger-tree-1977}
Guibas, L. J., McCreight, E. M., Plass, M. F., Roberts, J. R. (1977), "A new representation for linear lists". Conference Record of the Ninth Annual ACM Symposium on Theory of Computing, pp. 49C60.

\end{thebibliography}

\ifx\wholebook\relax \else
\end{document}
\fi
