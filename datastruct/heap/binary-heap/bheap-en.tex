\ifx\wholebook\relax \else
% ------------------------

\documentclass{article}
%------------------- Other types of document example ------------------------
%
%\documentclass[twocolumn]{IEEEtran-new}
%\documentclass[12pt,twoside,draft]{IEEEtran}
%\documentstyle[9pt,twocolumn,technote,twoside]{IEEEtran}
%
%-----------------------------------------------------------------------------
%\input{../../../common.tex}
\input{../../../common-en.tex}

\setcounter{page}{1}

\begin{document}

\fi
%--------------------------

% ================================================================
%                 COVER PAGE
% ================================================================

\title{Binary Heaps with Functional and imperative implementation}

\author{Larry~LIU~Xinyu
\thanks{{\bfseries Larry LIU Xinyu } \newline
  Email: liuxinyu95@gmail.com \newline}
  }

\markboth{Binary Heaps}{Imperative and Functional}

\maketitle

\ifx\wholebook\relax
\chapter{Binary Heaps with Functional and imperative implementation}

\section{abstract}
\else
\begin{abstract}
\fi
Heap is one of the elementary data structure. It is widely used
to solve some practical problems, such as sorting, prioritized
scheduling, and graph algorithms\cite{wiki-heap}.

Most popular implementations of heap are using a kind of implicit
binary heap by array, which is described in ``Introduction to
Algorithm'' textbook\cite{CLRS}. Examples include C++/STL
heap and Python heapq.

However, heaps can be generalized and implemented with varies
of other data structure besides array. In this post, explicit
binary tree is used to realize heaps. It leads to Leftist heaps, Skew heaps,
and Splay heaps, which are suitable for pure functional implementation as shown
by Okasaki\cite{okasaki-book}.

There are multiple programming languages used, including
C++, Haskell, Python and Scheme/Lisp.

There may be mistakes in the post, please feel free to point out.

This post is generated by \LaTeXe, and provided with GNU FDL(GNU Free Documentation License).
Please refer to http://www.gnu.org/copyleft/fdl.html for detail.

\ifx\wholebook\relax \else
\end{abstract}
\fi

\vspace{3cm}
{\bfseries Keywords:} Binary Heaps, Leftist Heaps, Skew Heaps, Splay Heaps

%{\bfseries Corresponding Author:} Larry LIU Xinyu

\maketitle

% ================================================================
%                 Introduction
% ================================================================
\section{Introduction}
\label{introduction}

Heap is an important elementary data structure. Most of the algorithm
textbooks introduce heap, especially about binary heap and heap sort.

Some popular implementation, such as C++/STL heap and Python heapq
are based on binary heaps (implicit binary heap by array
more precisely). And the fastest heap sort algorithm is also
written with binary heap as proposed by R. W. Floyd
\cite{wiki-heapsort} \cite{rosetta-heapsort}.

In this post, we use a general definition of heap, so that
varies of under-ground data structures can be used for implementation.
And binary heap is also extended to a wide concept under this definition.

A heap is a data structure that satisfies the following {\em heap property}.
\begin{itemize}
\item Top operation always returns the minimum (maximum) element;
\item Pop operation removes the top element from the heap while the heap
property should be kept, so that the new top element is still the
minimum (maximum) one;
\item Insert a new element to heap should keep the heap property. That
the new top is still the minimum (maximum) element;
\item Other operations including merge etc should all keep the heap property.
\end{itemize}

This is a kind of recursive definition, while it doesn't limit the under
ground data structure.

We call the heap with top returns the minimum element {\em min-heap},
while if top returns the maximum element, we call it {\em max-heap}.

In this post, I'll first give the definition of binary heap. Then
I'll review the traditional imperative way of implicit heap by array.
After that, by considering explicit heap by binary trees, I'll
explain the Leftist heap, Skew heap, Splay heap, and provide pure functional implementation
for them based on Okasaki's result\cite{okasaki-book}.

As the last part, the computation complexity will be mentioned, and
I'll show in what situation, Leftist heap performs bad.

I'll introduce some other heaps, including Binomial heaps, Fibonacci
heaps, and Pairing heaps in a separate post.

This article provides example implementation in C++, Haskell, Python, and
Scheme/Lisp languages.

All source code can be downloaded in appendix \ref{appendix}, please
refer to appendix for detailed information about build and run.


% ================================================================
%                 Implicit binary heap by array
% ================================================================
\section{Implicit binary heap by array}
\label{ibheap}

Considering the heap definition in previous section, one option to
implement heap is by using trees. A straightforward solution is
to store the minimum (maximum) element in the root node of the
tree, so for `top' operation, we simply return the root as the
result. And for `pop' operation, we can remove the root and
rebuild the tree from the children.

If the tree which is used to implement heap is a binary tree, we
can call it {\em binary heap}. There are three types of binary
heap implementation explained in this post. All of them are
based on binary tree.

% ================================================================
%                 Definition
% ================================================================
\subsection{Definition}

The first one is implicit binary tree indeed. Consider the problem
how to represent a complete binary tree with array. (For example, try to
represent a complete binary tree in a programming language doesn't support structure
or record data type, so that only array can be used). One solution
is to pack all element from top level (root) down to bottom level (leaves).

Figure \ref{fig:tree-array-map} shows a complete binary tree and
its corresponding array representation.

\begin{figure}[htbp]
       \begin{center}
       	  \includegraphics[scale=0.5]{img/tree-array-map-tree.ps}
          \includegraphics[scale=0.5]{img/tree-array-map-array.ps}
        \caption{Mapping between a complete binary tree and array} \label{fig:tree-array-map}
       \end{center}
\end{figure}

This mapping relationship between tree and array can be denote as
the following equations (Note that the array index starts from 1).

\begin{algorithmic}[1]
\Function{PARENT}{$i$}
  \State \Return $\lfloor \frac{i}{2} \rfloor$
\EndFunction
\Statex
\Function{LEFT}{$i$}
  \State \Return $2i$
\EndFunction
\Statex
\Function{LEFT}{$i$}
  \State \Return $2i+1$
\EndFunction
\end{algorithmic}

For a given tree node which is represented as the $i$-th element in an
array, since the tree is complete, we can easily find its parent node
as the $\lfloor i/2 \rfloor$-th element in the array; Its left child
with index of $2i$ and right child with index of $2i+1$. If the index
of the child exceeds the length of the array, it simply means this
node don't have such child.

This mapping calculation can be performed fast if bit-wise operation
is used.

\subsection*{Definition of implicit binary heap by array in C++}
In C++ language, array index starts from zero, but not one. The mapping
from array to binary tree should be adjusted accordingly.

\lstset{language=C++}
\begin{lstlisting}
template<class T>
T parent(T i){ return ((i+1)>>1)-1; }

template<class T>
T left(T i){ return (i<<1)+1; }

template<class T>
T right(T i){ return (i+1)<<1; }
\end{lstlisting}

The type T must support bit-wise operation.

\subsection*{Definition of implicit binary heap by array in Python}
Similar as C/C++, the array index in Python starts from 0, so we provide
the mapping functions as below.

\lstset{language=Python}
\begin{lstlisting}
def parent(i):
    return (i+1)//2-1

def left(i):
    return  2*i+1

def right(i):
    return 2*(i+1)
\end{lstlisting}

% ================================================================
%                 Heapify
% ================================================================
\subsection{Heapify}

The most important thing for heap algorithm is to maintain the heap
property, that the top element should be the minimum (maximum) one.

For the implicit binary heap by array, it means for a given node,
which is represented as the $i$-th index, we must develop a algorithm
to check if all its two children conform to this property and in case
there is violation, we need swap the parent and child to fix the
problem.

In ``Introduction to Algorithms'' book\cite{CLRS}, this algorithm
is given in a recursive way, here we show a pure imperative solution.
Let's take min-heap for example.

\begin{algorithmic}[1]
\Function{HEAPIFY}{$A, i$}
  \State $n \gets LENGTH(A)$
  \Loop
    \State $l \gets LEFT(i)$
    \State $r \gets RIGHT(i)$
    \State $smallest \gets i$
    \If{$l < n$ and $A[l] < A[i]$}
      \State $smallest \gets l$
    \EndIf
    \If{$r < n$ and $A[r] < A[smallest]$}
      \State $smallest \gets r$
    \EndIf
    \If{$smallest \neq i$}
      \State $exchange A[i] \leftrightarrow A[smallest]$
      \State $i \gets smallest$
    \Else
      \State \Return
    \EndIf
  \EndLoop
\EndFunction
\end{algorithmic}

This algorithm assume that for a given node, the children all conform
to the heap property, however, we are not sure if the value of this node
is the smallest compare to its tow children.

For array $A$ and a given index $i$, we need check none its left child or right child
is bigger than $A[i]$, in case we find violation, we pick the smallest one, and set
it as the new value for $A[i]$, the previous value of $A[i]$ is then set as the
new value of the child, and we need go along the child tree to repeat this check and fixing
process until we either reach a leaf node or there is no heap property violation.

Note that the $HEAPIFY$ algorithm takes $O(\lg{N})$ time.

\subsection*{Heapify in C++}

For C++ program, we need it cover both traditional C compatible array, and
the modern container abstraction. There are several options to realize this
requirement.

One method is to pass iterators as argument to heap algorithm. C++/STL
implementation (at the time the author wrote this post) uses this approach.

The advantage of using iterator is that some random access iterator is
just implemented as C pointers, so it compatible with C array well.

However, this method need us change the algorithm from a array index style
to pointer operation style. It's hard to reflect the above pseudo code
quite clear in such style. Because of this problem, we won't use this approach
here. Reader can refer to STL source code for detailed information.

Another option is to pass the array or container as well as the
number of elements as arguments. However, we need abstract the
comparison anyway so that the algorithm works for both max-heap
and min-heap.

\lstset{language=C++}
\begin{lstlisting}
template<class T> struct MinHeap: public std::less<T>{};
template<class T> struct MaxHeap: public std::greater<T>{};
\end{lstlisting}

Here we define MinHeap and MaxHeap as a kind of alias of less-than
and greater-than logic comparison functor template.

The heapify algorithm can be implemented as the following.

\begin{lstlisting}
template<class Array, class LessOp>
void heapify(Array& a, unsigned int i, unsigned int n, LessOp lt){
  while(true){
    unsigned int l=left(i);
    unsigned int r=right(i);
    unsigned int smallest=i;
    if(l < n && lt(a[l], a[i]))
      smallest = l;
    if(r < n && lt(a[r], a[smallest]))
      smallest = r;
    if(smallest != i){
      std::swap(a[i], a[smallest]);
      i = smallest;
    }
    else
      break;
  }
}
\end{lstlisting}

The program accepts the reference of the array (both reference
of the container, and reference to pointer are OK), the index
from where we want to adjust so that all children of it confirms
to the heap property; the number of elements in the array, and
a comparison functor. It checks from the node which is indexed
as $i$ down to the leaf until it find a node that both children are
``less than'' the value of the node based on the comparison functor.
Otherwise, it will locate the ``smallest'' one and swap it with
the node value.

It is also possible to create a concept of range and realize the
algorithm with it. Some C++ library, such as boost has already
support range. Here we can develop a light weight range only for
random access container.

\begin{lstlisting}
template<class RIter> // random access iterator
struct Range{
  typedef typename std::iterator_traits<RIter>::value_type value_type;
  typedef typename std::iterator_traits<RIter>::difference_type size_t;
  typedef typename std::iterator_traits<RIter>::reference  reference;
  typedef RIter iterator;

  Range(RIter left, RIter right):first(left), last(right){}

  reference  operator[](size_t i){ return *(first+i); }
  size_t size() const { return last-first; }

  RIter first;
  RIter last;
};
\end{lstlisting}

For a given left index $l$, and right index $r$, a range represents
$[l, r)$, So it is easy to construct a range with iterators as well
as the pointer of the array and its length.

Two overloaded auxiliary function templates are provided to create
range easily.

\begin{lstlisting}
template<class Iter>
Range<Iter> range(Iter left, Iter right){ return Range<Iter>(left, right); }

template<class Iter>
Range<Iter> range(Iter left, typename Range<Iter>::size_t n){
  return Range<Iter>(left, left+n);
}
\end{lstlisting}

The above algorithm can be implemented with range like below.

\begin{lstlisting}
template<class R, class LessOp>
void heapify(R a, typename R::size_t i, LessOp lt){
  typename R::size_t l, r, smallest;
  while(true){
    l = left(i);
    r = right(i);
    smallest = i;
    if( l < a.size() && lt(a[l], a[i]))
      smallest = l;
    if( r < a.size() && lt(a[r], a[smallest]))
      smallest = r;
    if( smallest != i){
      std::swap(a[i], a[smallest]);
      i = smallest;
    }
    else
      break;
  }
}
\end{lstlisting}

Almost everything is as same as the former one except that the number
of elements can be given by the size of the range.

In order to verify the program, a same test case as in figure 6.2 in \cite{CLRS}
is fed to our function.

\begin{lstlisting}
// test c-array
const int a[] = {16, 4, 10, 14, 7, 9, 3, 2, 8, 1};
const unsigned int n = sizeof(a)/sizeof(a[0]);
int x[n];
std::copy(a, a+n, x);
heapify(x, 1, n, MaxHeap<int>());
print_range(x, x+n);

// test random access container
std::vector<short> y(a, a+n);
heapify(y, 1, n, MaxHeap<short>());
print_range(y.begin(), y.end());
\end{lstlisting}

The same test case can also be applied to the ``range'' version of
program.

\begin{lstlisting}
heapify(range(x, n), 1, MaxHeap<int>());

//...

heapify(range(y.begin(), y.end()), 1, MaxHeap<short>());
\end{lstlisting}

Where ``print\_range'' is a helper function to output all elements
in a container, a C array or a range.

\begin{lstlisting}
template<class Iter>
void print_range(Iter first, Iter last){
  for(; first!=last; ++first)
    std::cout<<*first<<", ";
  std::cout<<"\n";
}

template<class R>
void print_range(R a){
  print_range(a.first, a.last);
}
\end{lstlisting}

The above test code can output a result as below:

\begin{verbatim}
16, 14, 10, 8, 7, 9, 3, 2, 4, 1,
16, 14, 10, 8, 7, 9, 3, 2, 4, 1,
\end{verbatim}

Figure \ref{fig:heapify} shows how this algorithm works.

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[scale=0.3]{img/heapify-1.ps}

    a. Step 1, 14 is the biggest element among 4, 14, and 7. Swap 4 with the left child;

    \includegraphics[scale=0.3]{img/heapify-2.ps}

    b. Step 2, 8 is the biggest element among 2, 4, and 8. Swap 4 with the right child;

    \includegraphics[scale=0.3]{img/heapify-3.ps}

    c. 4 is the leaf node. It hasn't any children. Process terminates.
    \caption{Heapify example, a max-heap case.} \label{fig:heapify}
  \end{center}
\end{figure}

\subsection*{Heapify in Python}

In order to cover both min-heap and max-heap, we abstract the comparison
as the following lambda expressions.

\lstset{language = Python}
\begin{lstlisting}
MIN_HEAP = lambda a, b: a < b
MAX_HEAP = lambda a, b: a > b
\end{lstlisting}

By passing the above defined comparison operation as an argument, the ``Heapify''
algorithm is given as below.

\begin{lstlisting}
def heapify(x, i, less_p = MIN_HEAP):
    n = len(x)
    while True:
        l = left(i)
        r = right(i)
        smallest = i
        if l < n and less_p(x[l], x[i]):
            smallest = l
        if r < n and less_p(x[r], x[smallest]):
            smallest = r
        if smallest != i:
            (x[i], x[smallest])=(x[smallest], x[i])
            i  = smallest
        else:
            break
\end{lstlisting}

We can use the same test case as presents in Figure 6.2 of \cite{CLRS}.

\begin{lstlisting}
l = [16, 4, 10, 14, 7, 9, 3, 2, 8, 1]
heapify(l, 1, MAX_HEAP)
print l
\end{lstlisting}

The result is something like this.

\begin{verbatim}
[16, 14, 10, 8, 7, 9, 3, 2, 4, 1]
\end{verbatim}

This result is as same as the one presented in \ref{fig:heapify}.

% ================================================================
%                 Build a heap
% ================================================================
\subsection{Build a heap}

With heapify algorithm, it is easy to build a heap from an arbitrary
array. Observe that the number of nodes in a complete binary tree
for each level is a list like:

$1, 2, 4, 8, ..., 2^i, ...$.

The only exception is the last level. Since the tree may not full
(note that complete binary tree doesn't mean full binary tree), the
last level contains at most $2^{p-1}$ nodes, where $2^p \leq n$ and $n$
is the length of the array.

Heapify algorithm doesn't take any effect on leave node, which means
we can skip applying heapify for all nodes. In other words,
all leaf nodes have already satisfied heap property. We only need
start checking and maintaining heap property from the last branch node.
the Index of the last branch node is no greater than $\lfloor n/2 rfloor$.

Based on this fact, we can build a heap with the following algorithm.
(Assume the heap is min-heap).

\begin{algorithmic}[1]
\Function{BUILD-HEAP}{$A$}
  \State $n \gets LENGTH(A)$
  \For{$i \gets \lfloor n/2 \rfloor$ downto $1$}
    \State $HEAPIFY(A, i)$
  \EndFor
\EndFunction
\end{algorithmic}

Although the complexity of $HEAPIFY$ is $O(\lg{N})$, the running time
of $BUILD\_HEAP$ doesn't bound to $O(N \lg{N})$ but to $O(N)$, so this
is a linear time algorithm. Please refer to \cite{CLRS} for the
detailed proof.

\subsection*{Build a heap in C++}

The only adjustment in C++ program from the above algorithm is
about the starting index from 1 to 0.

\lstset{language=C++}
\begin{lstlisting}
template<class Array, class LessOp>
void build_heap(Array& a, unsigned int n, LessOp lt){
  unsigned int i = (n-1)>>1;
  do {
      heapify(a, i, n, lt);
  } while (i--);
}
\end{lstlisting}

Note that since the unsigned type is used to represent index,
It can't lower than zero. We can't just use a for loop as below.

\begin{lstlisting}
for(unsigned int i = (n-1)>>1; i>=0; --i) //wrong, i always >=0
\end{lstlisting}

This program can be easily adjusted with range concept.

\begin{lstlisting}
template<class RangeType, class LessOp>
void build_heap(RangeType a, LessOp lt){
  typename RangeType::size_t i = (a.size()-1)>>1;
  do {
    heapify(a, i, lt);
  } while (i--);
}
\end{lstlisting}

We can test our program with the same data as in Figure 6.3 in \cite{CLRS}.

\begin{lstlisting}
// test c-array
const int a[] = {4, 1, 3, 2, 16, 9, 10, 14, 8, 7};
const unsigned int n = sizeof(a)/sizeof(a[0]);
int x[n];
std::copy(a, a+n, x);
build_heap(range(x, n), MaxHeap<int>());
print_range(x, x+n);

// test random access container
std::vector<int> y(a, a+n);
build_heap(range(y.begin(), y.end()), MaxHeap<short>());
print_range(y.begin(), y.end());
\end{lstlisting}

Running results are printed in console like the following.

\begin{verbatim}
16, 14, 10, 8, 7, 9, 3, 2, 4, 1,
16, 14, 10, 8, 7, 9, 3, 2, 4, 1,
\end{verbatim}

Figure \ref{fig:build-heap-1} and \ref{fig:build-heap-2}
show the steps when build a heap from
an arbitrary array. The node in black color is the one we will apply
$HEAPIFY$ algorithm, the nodes in gray color are swapped during
$HEAPIFY$.

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[scale=0.3]{img/build-heap-array.ps}

    a. An array in arbitrary order before build-heap process;

    \includegraphics[scale=0.3]{img/build-heap-1.ps}

    b. Step 1, The array is mapped to binary tree. The first branch node, which is
16 is to be examined;

    \includegraphics[scale=0.3]{img/build-heap-2.ps}

    c. Step 2, 16 is the largest element in current sub tree, next is to check node
with value 2;

    \caption{Build a heap from an arbitrary array. Gray nodes are changed in each step,
black node is the one to be processed next step.} \label{fig:build-heap-1}
  \end{center}
\end{figure}

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[scale=0.3]{img/build-heap-3.ps}

    d. Step 3, 14 is the largest value in the sub-tree, swap 14 and 2; next is to check
node with value 3;

    \includegraphics[scale=0.3]{img/build-heap-4.ps}

    e. Step 4, 10 is the largest value in the sub-tree, swap 10 and 3; next is to check
node with value 1;

    \includegraphics[scale=0.3]{img/build-heap-5.ps}

    f. Step 5, 16 is the largest value in current node, swap 16 and 1 first; then
similarly, swap 1 and 7; next is to check the root node with value 4;

    \includegraphics[scale=0.3]{img/build-heap-6.ps}

    g. Step 6, Swap 4 and 16, then swap 4 and 14, and then swap 4 and 8;
And the whole build process finish.

    \caption{Build a heap from an arbitrary array. Gray nodes are changed in each step,
black node is the one to be processed next step.} \label{fig:build-heap-2}
  \end{center}
\end{figure}


\subsection*{Build a heap in Python}

Like the C++ heap building program, we check from the last non-leaf
node and apply $HEAPIFY$ algorithm, and repeat the process back to
the root node. However, we use explicit calculation (divided by 2)
instead of using bit-wise shifting.

\lstset{language=Python}
\begin{lstlisting}
def build_heap(x, less_p = MIN_HEAP):
    n = len(x)
    for i in reversed(range(n//2)):
        heapify(x, i, less_p)
\end{lstlisting}

We can feed the similar test case to this program as below:

\begin{lstlisting}
l = [4, 1, 3, 2, 16, 9, 10, 14, 8, 7]
build_heap(l, MAX_HEAP)
print l
\end{lstlisting}

It will output the same result as the C++ program.

\begin{verbatim}
[16, 14, 10, 8, 7, 9, 3, 2, 4, 1]
\end{verbatim}

% ================================================================
%                 Basic heap operations
% ================================================================
\subsection{Basic heap operations}

From the generic definition of heap (not necessarily binary heap),
It's essential to provides basic operations so that user can access
the data and modify it.

The most important operations included accessing the top element
(find the minimum or maximum element), pop one element (the minimum
one or the maximum one depends on the type of the heap)
from the heap, find the top N elements, decrease a key (note this
is for min-heap, and it will be increase a key for max-heap), and
insertion.

For binary tree, most of this operation is bound to $O(\lg{N})$ worst-case,
some of them, such as top is $O(1)$ time.

\subsubsection{Access the top element (minimum)}
According to the definition of heap, there must be an operation to
return the top element. for implicit binary tree by array, it is the
root node which stores the minimum (maximum) value.

\begin{algorithmic}[1]
\Function{TOP}{$A$}
  \State \Return $A[0]$
\EndFunction
\end{algorithmic}

This operation is trivial. It takes $O(1)$ time.

\subsubsection*{Access the top element in C++}

By translate the above algorithm directly in C++, we get the
following program.

\lstset{language=C++}
\begin{lstlisting}
template<class T>
typename ValueType<T>::Result heap_top(T a){ return a[0]; }
\end{lstlisting}

There is a small trick to get the type of the element store in
array no matter if the array is STL container or plain C like
array.

\begin{lstlisting}
template<class T> struct ValueType{
  typedef typename T::value_type Result;
};

template<class T> struct ValueType<T*>{
  typedef T Result; // c-pointer type
};

template<class T, unsigned int n> struct ValueType<T[n]>{
  typedef T Result; // c-array type
};
\end{lstlisting}

Not that the C++ template meta programming support to
specialize for a certain type.

Here we skip the error handling of empty heap case. If the
heap is empty, one option is just to raise exception.

\subsubsection*{Access the top element in Python}

The python version of this program is also simple, we omit
the error handling for empty heap as well.

\lstset{language=C++}
\begin{lstlisting}
def heap_top(x):
    return x[0] #ignore empty case
\end{lstlisting}

\subsubsection{Heap Pop (delete minimum)}

Different from the top operation, pop operation is a bit
complex, because the heap property has to be maintained
after the top element is removed.

The solution is to apply $HEAPIFY$ algorithm immediately to the
next node to the root node which has been removed.

A quick but slow algorithm based on this idea may look like
the following.

\begin{algorithmic}[1]
\Function{POP-SLOW}{$A$}
  \State $x \gets TOP(A)$
  \State $REMOVE(A, 1)$
  \If{$A$ is not empty}
    \State $HEAPIFY(A, 1)$
  \EndIf
  \State \Return $x$
\EndFunction
\end{algorithmic}

This algorithm first remember the top element in $x$, then
it removes the first element from the array, the size of
this array reduced by one. After that if the array isn't
empty, $HEAPIFY$ will applied to the modified array on
the first element (previous the second element).

Removing an element from array takes $O(N)$ time,
where $N$ is the length of the array. Removing the first
element need shift all the rest values one by one.
Because of this bottle neck, it slows the whole algorithm
to $O(N)$.

In order to solve this program, one alternative way is
to just swap the first element and the last one in the
array, then shrink the array size by one.

\begin{algorithmic}[1]
\Function{POP}{$A$}
  \State $x \gets TOP(A)$
  \State $SWAP(A[1], A[HEAP-SIZE(A)])$
  \State $REMOVE(A, HEAP-SIZE(A))$
  \If{$A$ is not empty}
    \State $HEAPIFY(A, 1)$
  \EndIf
  \State \Return $x$
\EndFunction
\end{algorithmic}

Note that remove the last element from the array takes
only $O(1)$ time, and $HEAPIFY$ is bound to $O(\lg{N})$.
The whole algorithm is bound to $O(\lg{N})$ time.

\subsubsection*{Pop in C++}

In C++ program, we abstract the min-heap and max-heap as heap type
template parameter, and pass it explicitly.

First is the `reference + size' approach.

\lstset{language=C++}
\begin{lstlisting}
template<class T, class LessOp>
typename ValueType<T>::Result heap_pop(T& a, unsigned int& n, LessOp lt){
  typename ValueType<T>::Result top = heap_top(a);
  a[0] = a[n-1];
  heapify(a, 0, --n, lt);
  return top;
}
\end{lstlisting}

And it can be adapted to ``range'' abstraction as well.

\begin{lstlisting}
template<class R, class LessOp>
typename R::value_type heap_pop(R& a, LessOp lt){
  typename R::value_type top = heap_top(a);
  std::swap(a[0], a[a.size()-1]);
  --a.last;
  heapify_(a, 0, lt);
  return top;
}
\end{lstlisting}

\subsubsection*{Pop in Python}

Python provides pop() function to get rid of the last element,
so the program can be developed as below.

\begin{lstlisting}
def heap_pop(x, less_p = MIN_HEAP):
    top = heap_top(x)
    x[0] = x[-1] # this is faster than top = x.pop(0)
    x.pop()
    if x!=[]:
        heapify(x, 0, less_p)
    return top
\end{lstlisting}

\subsubsection{Find the first $K$ biggest (smallest) element}

With pop operation, it is easy to implement algorithm to
find the top $K$ elements. In order to find the biggest $K$
values form an array, we can build a max-heap, then perform
pop operation $K$ times.

\begin{algorithmic}[1]
\Function{TOP-K}{$A, k$}
  \State $BUILD-HEAP(A)$
  \For{$i \gets 1, MIN(k, LENGTH(A))$}
    \State $APPEND(Result, POP(A))$
  \EndFor
  \State \Return $Result$
\EndFunction
\end{algorithmic}

Note that if $K$ is bigger than the length of the array, it means
we need return the whole array as the result. That's why it need
use the $MIN$ function in the algorithm.

\subsubsection*{Find the first $K$ biggest (smallest) element in C++}

In C++ program, we can pass the iterator for output, so the
``reference - size'' version looks like below.

\lstset{language=C++}
\begin{lstlisting}
template<class Iter, class Array, class LessOp>
void heap_top_k(Iter res, unsigned int k,
                Array& a, unsigned int& n, LessOp lt){
  build_heap(a, n, lt);
  unsigned int count = std::min(k, n);
  for(unsigned int i=0; i<count; ++i)
    *res++=heap_pop(a, n, lt);
}
\end{lstlisting}

When we adapt to `range' concept, it is possible to manipulate
the data in place, so that we can put the top $K$ element
in first $K$ positions in the array.

\begin{lstlisting}
template<class R, class LessOp>
void heap_top_k(R a, typename R::size_t k, LessOp lt){
  typename R::size_t count = std::min(k, a.size());
  build_heap(a, lt);
  while(count--){
    ++a.first;
    heapify(a, 0, lt);
  }
}
\end{lstlisting}

The algorithm doesn't utilize `pop' function, instead, after
the heap is built, the first element is the top one, it
adjusts the range one position next, then apply heapify
to the new range. This process is repeated for $K$ times
so the first $K$ elements are the result.

A simple test cases can be fed to the program for verification.

\begin{lstlisting}
const int a[] = {4, 1, 3, 2, 16, 9, 10, 14, 8, 7};
unsigned int n = sizeof(a)/sizeof(a[0]);
std::vector<int> x(a, a+n);
heap_top_k(range(x.begin(), x.end()), 3, MaxHeap<int>());
print_range(range(x.begin(), 3));
\end{lstlisting}

The result is printed in console like below.

\begin{verbatim}
16, 14, 10,
\end{verbatim}

\subsubsection*{Find the first $K$ biggest (smallest) element in Python}

In Python we can put `pop' function to list comprehension to
get the top $K$ elements like the following.

\lstset{language=Python}
\begin{lstlisting}
def top_k(x, k, less_p = MIN_HEAP):
    build_heap(x, less_p)
    return [heap_pop(x, less_p) for i in range(min(k, len(x)))]
\end{lstlisting}

The testing and result are shown as the following.

\begin{lstlisting}
l = [4, 1, 3, 2, 16, 9, 10, 14, 8, 7]
res = top_k(l, 3, MAX_HEAP)
print res
\end{lstlisting}

Evaluate the code led to below line.

\begin{lstlisting}
[16, 14, 10]
\end{lstlisting}

\subsubsection{Modification: Decrease key}

Heap can be used to implement priority queue, because of this, it
is important to modify the key stored in heap. One typical operation
is to increase the priority of a tasks so that it can be performed
earlier.

Here we present the decrease key operation for a min-heap. The
corresponding operation is increase key for max-heap.

Once we modified a key by decreasing it in a min-heap, it can make
the node conflict with the heap property, that the key may be less
than some values in its ancestors. In order to maintain the
invariant, an auxiliary algorithm is provided to fix the heap
property.

\begin{algorithmic}[1]
\Function{HEAP-FIX}{$A, i$}
  \While{$i>1$ and $A[i] < A[PARENT[i]]$}
    \State Exchange $A[i] \leftrightarrow A[PARENT[i]]$
    \State $i \gets PARENT[i]$
  \EndWhile
\EndFunction
\end{algorithmic}

This algorithm repeatedly examine the key of parent node and
the key in current node. It will swap nodes in case the
parent contains the smaller key. This process is performed
from current node towards the root node till it find that
the parent node holds the smaller key.

With this auxiliary algorithm, decrease key can be realized
easily.

\begin{algorithmic}[1]
\Function{DECREASE-KEY}{$A, i, k$}
  \If{$k < A[i]$}
    \State $A[i] \gets k$
    \State $HEAP-FIX(A, i)$
  \EndIf
\EndFunction
\end{algorithmic}

Note that the algorithm only takes effect when the new key
is less than the original key.

\subsubsection*{Decrease key in C++}
In order to support both min-heap and max-heap, the comparison
function object is passed as an argument in C++ implementation.

\lstset{language=C++}
\begin{lstlisting}
template<class Array, class LessOp>
void heap_fix(Array& a, unsigned int i, LessOp lt){
  while(i>0 && lt(a[i], a[parent(i)])){
    std::swap(a[i], a[parent(i)]);
    i = parent(i);
  }
}

template<class Array, class LessOp>
void heap_decrease_key(Array& a,
                       unsigned int i,
                       typename ValueType<Array>::Result key,
                       LessOp lt){
  if(lt(key, a[i])){
    a[i] = key;
    heap_fix(a, i, lt);
  }
}
\end{lstlisting}

Some very simple verification case can be fed to the program.
Here we use the example presented in \cite{CLRS} Figure 6.5.

\begin{lstlisting}
const int a[] = {16, 14, 10, 8, 7, 9, 3, 2, 4, 1};
const unsigned int n = sizeof(a)/sizeof(a[0]);
int x[n];
std::copy(a, a+n, x);
heap_decrease_key(x, 8, 15, MaxHeap<int>());
print_range(x, x+n);
\end{lstlisting}

Run the above lines will generate the following output.
\begin{verbatim}
16, 15, 10, 14, 7, 9, 3, 2, 8, 1,
\end{verbatim}

In this max-heap example, we try to increase the key of the 9-th node from
4 to 15. As shown in figure \ref{fig:decrease-key}

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[scale=0.3]{img/decrease-key-a.ps}

    a. The 9-th node with key 4 will be modified;

    \includegraphics[scale=0.3]{img/decrease-key-b.ps}

    b. The key is modified to 15, which is greater than its parent;

    \includegraphics[scale=0.3]{img/decrease-key-c.ps}

    c. According the max-heap property, 8 and 15 are swapped.

    \includegraphics[scale=0.3]{img/decrease-key-d.ps}

    d. Since 15 is greater than 14, which is the key of its parent node, 15 and 14 are swapped. Because 15 is less than 16, the algorithm terminates.

    \caption{Example process when increase a key in a max-heap.} \label{fig:decrease-key}
  \end{center}
\end{figure}

\subsubsection*{Decrease key in Python}
The Python version decrease key program is similar as well. It first
checks if the new key is ``less than'' the original one, if yes it
modifies the value, and performs the fixing process.

\lstset{language=Python}
\begin{lstlisting}
def heap_decrease_key(x, i, key, less_p = MIN_HEAP):
    if less_p(key, x[i]):
        x[i] = key
        heap_fix(x, i, less_p)

def heap_fix(x, i, less_p = MIN_HEAP):
    while i>0 and less_p(x[i],x[parent(i)]):
        (x[parent(i)], x[i]) = (x[i], x[parent(i)])
        i = parent(i)
\end{lstlisting}

We can use the same test case to verify the program.

\begin{lstlisting}
l = [16, 14, 10, 8, 7, 9, 3, 2, 4, 1]
heap_decrease_key(l, 8, 15, MAX_HEAP)
print l
\end{lstlisting}

It will output the same result as the C++ program.

\begin{verbatim}
[16, 15, 10, 14, 7, 9, 3, 2, 8, 1]
\end{verbatim}

\subsubsection{Insertion}

In \cite{CLRS}, insertion is implemented by using $DECREASE-KEY$.
The approach is to first insert a node with infinity key. According
to the min-heap property, the node should be the last element
in the under ground array. After that, the key is decreased to
the value to be inserted, so that we can call decrease-key to
finish the process.

Instead of reuse $DECREASE-KEY$, we can reuse $HEAP-FIX$ to implement
insertion. The new key is directly appended at the end of the array,
and the $HEAP-FIX$ is applied on this new node.

\begin{algorithmic}[1]
\Function{HEAP-PUSH}{$A, k$}
  \State $APPEND(A, k)$
  \State $HEAP-FIX(A, SIZE(A))$
\EndFunction
\end{algorithmic}

\subsubsection*{Insertion by decreasing key method in C++}
In C++, traditional C array is static sized, so appending a new element
to the array has to be managed properly. In order to simplify the
problem, we assume the necessary has already been allocated (by
client program).

First is the ``reference + size'' version of program.

\lstset{language=C++}
\begin{lstlisting}
template<class Array, class LessOp>
void heap_push(Array& a,
               unsigned int& n,
               typename ValueType<Array>::Result key,
               LessOp lt){
  a[n] = key;
  heap_fix(a, n, lt);
  ++n;
}
\end{lstlisting}

Note that the size is explicitly increased. so after calling this
function, the count of the element is increased by one.

It is also possible to provide equivalent program by using `range'.

\begin{lstlisting}
template<class R, class LessOp>
void heap_push(R a, typename R::value_type key, LessOp lt){
  *a.last++ = key;
  heap_fix(a, a.size()-1, lt);
}
\end{lstlisting}

We can test the insert program with a very simple case.

\begin{lstlisting}
const int a[] = {16, 14, 10, 8, 7, 9, 3, 2, 4, 1};
unsigned int n = sizeof(a)/sizeof(a[0]);
std::vector<int> x(a, a+n);
x.push_back(0);
heap_push(range(x.begin(), n), 17, MaxHeap<int>());
print_range(x.begin(), x.end());
\end{lstlisting}

Note that, in client program (this test program), we
reserved the memory in advance, or it will cause
access violation problem.
Running these lines will output the following result.

\begin{verbatim}
17, 16, 10, 8, 14, 9, 3, 2, 4, 1, 7,
\end{verbatim}

We can found the new element 17 is inserted at the proper
position of the heap.

\subsubsection*{Insertion directly in Python}

In python program, append a new element to a list is build-in
supported. So client program doesn't need to take care of the
similar problem as described in C++ implementation.

\lstset{language=Python}
\begin{lstlisting}
def heap_insert(x, key, less_p = MIN_HEAP):
    i = len(x)
    x.append(key)
    heap_fix(x, i, less_p)
\end{lstlisting}

If the same test case is fed to the above function, we can
get the output like the following.

\begin{lstlisting}
l = [16, 14, 10, 8, 7, 9, 3, 2, 4, 1]
heap_insert(l, 17, MAX_HEAP)
print l
\end{lstlisting}

\begin{verbatim}
[17, 16, 10, 8, 14, 9, 3, 2, 4, 1, 7
\end{verbatim}

% ================================================================
%                 Heap sort
% ================================================================
\subsection{Heap sort}
\label{heap-sort}

Heap sort algorithm is an interesting application of heap. According
to the heap property, the min(max) element can be easily accessed
by from the top of the heap. So a straightforward way to sort an
arbitrary of values is to first build a heap from them, then continuously
pops the smallest element from the heap till the heap is empty.

The algorithm based on this strategy is something like below.

\begin{algorithmic}[1]
\Function{HEAP-SORT}{$A$}
  \State $R \gets NIL$
  \State $BUILD-HEAP(A)$
  \While{$A \neq NIL$}
    \State $APPEND(R, HEAP-POP(A))$
  \EndWhile
  \State \Return $R$
\EndFunction
\end{algorithmic}

Robert. W. Floyd found a very fast implementation of heap sort.
The idea is to build a max-heap instead of min-heap, so the first
element is the biggest one. Then this biggest element is swapped
with the last element in the array, so that it is in the right
position after sorting. Now the last element becomes the top
of the heap, it may violate the heap property. We can perform
$HEAPIFY$ on it with the heap size shrink by one. This process
is repeated till there is only one element left in the heap.

\begin{algorithmic}[1]
\Function{HEAP-SORT-FAST}{$A$}
  \State $BUILD-MAX-HEAP(A)$
  \While{$SIZE(A) > 1$}
    \State Exchange $A[1] \leftrightarrow A[SIZE(A)]$
    \State $SIZE(A) \gets SIZE(A) - 1$
    \State $HEAPIFY(A, 1)$
  \EndWhile
\EndFunction
\end{algorithmic}

Note that this algorithm is in-place algorithm. It's the
fastest heap sort algorithm by far.

In terms of complexity, $BUILD-HEAP$ is bound to $O(N)$.
Since $HEAPIFY$ is $O(\lg{N})$, and it
is called $O(N)$ times, so both above algorithms take $O(N \lg{N})$
time to run.

\subsection*{Floyd's heap sort algorithm in C++}

Only Floyd's algorithm is given in C++ in this post.

\lstset{language=C++}
\begin{lstlisting}
template<class Array, class GreaterOp>
void heap_sort(Array& a, unsigned int n, GreaterOp gt){
  for(build_heap(a, n, gt); n>1; --n){
    std::swap(a[0], a[n-1]);
    heapify(a, 0, n-1, gt);
  }
}
\end{lstlisting}

A very simple test case is used for verification.

\begin{lstlisting}
const int a[] = {16, 14, 10, 8, 7, 9, 3, 2, 4, 1};
std::vector<int> y(a, a+n);
heap_sort(range(y.begin(), y.end()), MaxHeap<int>());
print_range(y.begin(), y.end());
\end{lstlisting}

The result is output as we expect.

\begin{verbatim}
1, 2, 3, 4, 7, 8, 9, 10, 14, 16,
\end{verbatim}

\subsection*{General heap sort algorithm in Python}

Instead of Floyd method, we'll show the straightforward
`popping $N$ times' algorithm in Python. Please refer
to \cite{rosetta-heapsort} for Floyd algorithm in Python.

\lstset{language=Python}
\begin{lstlisting}
def heap_sort(x, less_p = MIN_HEAP):
    res = []
    build_heap(x, less_p)
    while x!=[]:
        res.append(heap_pop(x, less_p))
    return res
\end{lstlisting}

And the test case is as same as the one we used in C++ program.

\begin{lstlisting}
l = [16, 14, 10, 8, 7, 9, 3, 2, 4, 1]
res = heap_sort(l)
print res
\end{lstlisting}

The result is output as the following.

\begin{verbatim}
[1, 2, 3, 4, 7, 8, 9, 10, 14, 16]
\end{verbatim}

% ================================================================
%                 Explicit binary heap
% ================================================================
\section{Leftist heap and Skew heap, explicit binary heaps}
\label{ebheap}

Instead of using implicit binary tree by array, it is natural to
consider why we can't use explicit binary tree to realize heap?

There are some problems must be solved if we turn into explicit
binary tree as the under ground data strucutre for heap.

The first problem is about the $HEAP-POP$ or $DELETE-MIN$ operation.
If the explicit binary tree is represent as the form of
(left value right), which is shown in figure \ref{fig:lvr}

\begin{figure}[htbp]
       \begin{center}
       	  \includegraphics[scale=1]{img/lvr.ps}
        \caption{A binary tree, all values in left child and right child are smaller than $k$.} \label{fig:lvr}
       \end{center}
\end{figure}

If $k$ is the top element, all values in left and right children are less
than $k$. After $k$ is popped, only left and right children are left.
They have to be merged to a new tree. Since heap property should be maintained
after merge, so the new root element is the smallest one.

Since both left child and right child are heaps in binary tree, the two trivial
cases can be found immediately.

\begin{algorithmic}[1]
\Function{MERGE}{$L, R$}
  \If{$L = NIL$}
    \State \Return $R$
  \ElsIf{$R = NIL$}
    \State \Return $L$
  \Else
    \State $...$
  \EndIf
\EndFunction
\end{algorithmic}

If neither left child nor right child is empty tree, because they all fit
heap property, the top element of them are all minimum value respectively.
One solution is to compare the root value of the left and right children,
select the smaller one as the new root of the merged heap, and recursively
merget the other child to one of the children of the smaller one.
For instance if $L = (A x B)$ and $R = (A' y B')$, where $A, A', B, B'$
are all sub trees, and $x < y$. There are two candidate results according
to this strategy.

\begin{itemize}
\item $(MERGE(A, R) x B)$
\item $(A x MERGE(B, R))$
\end{itemize}

Both are correct result. One simplified solution is only merge on right
sub tree. Leftist tree provides a systematically approach based on this
idea.

% ================================================================
%                 Definition
% ================================================================
\subsection{Definition}

The heap implemented by Leftist tree is called Leftist heap. Leftist
tree is first introduced by C. A. Crane in 1972\cite{wiki-leftist-tree}.

\subsubsection{Rank (S-value)}

In Leftist tree, a rank value (or $S$ value) is defined to each node.
Rank is the distance to the nearest external node. Where external node
is a NIL concept extended from leaf node.

For example, in figure \ref{fig:rank}, the rank of NIL
is defined 0, consider the root node 4, The nearest leaf node is
the child of node 8. So the rank of root node 4 is 2. Because node
6 and node 8 are all only contain NIL, so the rank values are 1.
Although node 5 has non-NIL left child, However, since the right
child is NIL, so the rank value, which is the minimum distance
to leaf is still 1.

\begin{figure}[htbp]
   \begin{center}
     \includegraphics[scale=0.5]{img/rank.ps}
     \caption{rank(4) = 2, rank(6) = rank(8) = rank(5) = 1.} \label{fig:rank}
   \end{center}
\end{figure}

\subsubsection{Leftist property}

With rank defined, we can create a strategy when merging.

\begin{itemize}
\item Every time when merging, we always merge to right child; Denote the rank
of the new right sub tree as $r_r$;
\item Compare the ranks of the left and right children, if the rank of
left sub tree is $r_l$ and $r_l < r_r$, we swap the left and right children.
\end{itemize}

We call this `Leftist property'. Generally speaking, a Leftist tree always
has the shortest path to an external node on the right.

A Leftist tree tends to be very unbalanced, However, it ensures an important
property as specified in the following theorem.

If a Leftist tree $T$ contains $N$ internal nodes, the path from root to the
rightmost external node contains at most $\lfloor \log{(N+1)} \rfloor$ nodes.

For the proof of these theorem, please refer to \cite{brono-book} and \cite{TAOCP}.

With this theorem, algorithms operate along this path are all bound to $O(\lg N)$.

\subsection*{Definition of Leftist heap in Haskell}

In Haskell the definition of Leftist tree is almost as same as the
binary search tree except there is a rank field added.

\lstset{language=Haskell}
\begin{lstlisting}
data LHeap a = E -- Empty
             | Node Int a (LHeap a) (LHeap a) -- rank, element, left, right
               deriving (Eq, Show)
\end{lstlisting}

In order to access the rank field, a helper function is provided.

\begin{lstlisting}
rank::LHeap a -> Int
rank E = 0
rank (Node r _ _ _) = r
\end{lstlisting}

\subsection*{Definition of Leftist heap in Scheme/Lisp}

In Scheme/Lisp, list is used to represent the Leftist tree. In order
to output the tree easily in in-order format, a node is arranged as
(left rank element right). Some auxiliary functions are defined
to access these fields of a node.

\lstset{language=lisp}
\begin{lstlisting}
(define (left t)
  (if (null? t) '() (car t)))

(define (rank t)
  (if (null? t) 0 (cadr t)))

(define (elem t)
  (if (null? t) '() (caddr t)))

(define (right t)
  (if (null? t) '() (cadddr t)))
\end{lstlisting}

And a construction function is provided so that a node can be
built explicitly.

\begin{lstlisting}
(define (make-tree l s x r) ;; l: left, s: rank, x: elem, r: right
  (list l s x r))
\end{lstlisting}

% ================================================================
%                 Merge
% ================================================================
\subsection{Merge}

In order to realize `merge', an auxiliary algorithm is given to
compare the ranks and do swapping if necessary.

\begin{algorithmic}[1]
\Function{LEFTIFY}{$T$}
  \State $l \gets LEFT(T), r \gets RIGHT(T)$
  \State $k \gets KEY(T)$
  \If{$RANK(l) < RANK(r)$}
    \State $RANK(T) \gets RANK(L) + 1$
  \Else
    \State $RANK(T) \gets RANK(R) + 1$
    \State Exchange $l \leftrightarrow r$
  \EndIf
\EndFunction
\end{algorithmic}

The algorithm compares the rank of the left and
right sub trees, pick the less one and add it by one as the
rank of the modified node. If the rank of left side is greater,
it will also swap the left and right children.

The reason why rank need to be increased by one is because there
is a new key added on top of the tree, which causes the rank
increase.

With $LEFTIFY$ defined, merge algorithm can be provided as the
following.

\begin{algorithmic}[1]
\Function{MERGE}{$L, R$}
  \If{$L = NIL$}
    \State \Return $R$
  \ElsIf{$R = NIL$}
    \State \Return $L$
  \Else
    \State $T \gets CREATE-NEW-NODE()$
    \If{$KEY(L) < KEY(R)$}
      \State $KEY(T) \gets KEY(L)$
      \State $LEFT(T) \gets LEFT(L)$
      \State $RIGHT(T) \gets MERGE(RIGHT(L), R)$
    \Else
      \State $KEY(T) \gets KEY(R)$
      \State $LEFT(T) \gets LEFT(R)$
      \State $RIGHT(T) \gets MERGE(L, RIGHT(R))$
    \EndIf
    \State $LEFTIFY(T)$
  \EndIf
  \State \Return $T$
\EndFunction
\end{algorithmic}

Note that $MERGE$ algorithm always operate on right side, and call
$LEFITFY$ to ensure the `Leftist property, so that this algorithm
is bound to $O(\lg N)$.

\subsection*{Merge in Haskell}

Translate the algorithm to Haskell lead to the following program.
Here we modified the pseudo code to a pure functional style.

\lstset{language=Haskell}
\begin{lstlisting}
merge::(Ord a)=>LHeap a -> LHeap a -> LHeap a
merge E h = h
merge h E = h
merge h1@(Node _ x l r) h2@(Node _ y l' r') =
    if x < y then makeNode x l (merge r h2)
    else makeNode y l' (merge h1 r')

makeNode::a -> LHeap a -> LHeap a -> LHeap a
makeNode x a b = if rank a < rank b then Node (rank a + 1) x b a
                 else Node (rank b + 1) x a b
\end{lstlisting}

\subsection*{Merge in Scheme/Lisp}

In Scheme/Lisp, the $LEFTIFY$ algorithm can be defined as an
inner function inside $MERGE$.

\lstset{language=lisp}
\begin{lstlisting}
(define (merge t1 t2)
  (define (make-node x a b)
    (if (< (rank a) (rank b))
	(make-tree b (+ (rank a) 1) x a)
	(make-tree a (+ (rank b) 1) x b)))
  (cond ((null? t1) t2)
	((null? t2) t1)
	((< (elem t1) (elem t2)) (make-node (elem t1) (left t1) (merge (right t1) t2)))
	(else (make-node (elem t2) (left t2) (merge t1 (right t2))))))
\end{lstlisting}

\subsubsection{Merge operation in implicit binary heap by array}

In most cases implicit binary heap by array performs very fast, and
it fits modern computer with cache technology well. However, merge
is the algorithm bounds to $O(N)$ time. The best you can do is
concatenate two arrays together and make a heap of the result \cite{NIST}.

\begin{algorithmic}[1]
\Function{MERGE-HEAP}{$A, B$}
  $C \gets CONCAT(A, B)$
  $BUILD-HEAP(C)$
\EndFunction
\end{algorithmic}

We omit the implementation of this algorithm in C++ and Python because
they are trivial.

% ================================================================
%                 Basic heap operations
% ================================================================
\subsection{Basic heap operations}

Most of the basic heap operations can be implemented easily with $MERGE$
algorithm define above.

\subsubsection{Find minimum (top) and delete minimum (pop)}
Since we keep the smallest element in root node, finding the minimum
value (top element) is trivial. It's a $O(1)$ operation.

\begin{algorithmic}[1]
\Function{TOP}{$T$}
  \State \Return $KEY(T)$
\EndFunction
\end{algorithmic}

While if the top element popped, left and right children are merged
so the heap updated.

\begin{algorithmic}[1]
\Function{POP}{$T$}
  \State \Return $MERGE(LEFT(T), RIGHT(T))$
\EndFunction
\end{algorithmic}

Note that pop operation on Leftist heap takes $O(\lg N)$ time.

\subsubsection*{Find minimum (top) and delete minimum in Haskell}

We skip the error handling of operation on an empty
Leftist heap.

\lstset{language=Haskell}
\begin{lstlisting}
findMin :: LHeap a -> a
findMin (Node _ x _ _) = x
\end{lstlisting}

\begin{lstlisting}
deleteMin :: (Ord a) => LHeap a -> LHeap a
deleteMin (Node _ _ l r) = merge l r
\end{lstlisting}

\subsubsection*{Find minimum (top) and delete minimum in Scheme/Lisp}

With merge function defined, these operations are trivial to implement
in Scheme/Lisp.

\lstset{language=lisp}
\begin{lstlisting}
(define (find-min t)
  (elem t))

(define (delete-min t)
  (merge (left t) (right t)))
\end{lstlisting}

\subsubsection{Insertion}

To insert a new key to the heap, one solution is to create a single
leaf node from the key, and perform merge with this leaf node and
the Leftist tree.

\begin{algorithmic}[1]
\Function{INSERT}{$T, k$}
  \State $x \gets CREATE-NEW-NODE()$
  \State $KEY(x) \gets k$
  \State $RAKN(x) \gets 1$
  \State $LEFT(x), RIGHT(x) \gets NIL$
  \State \Return $MERGE(x, T)$
\EndFunction
\end{algorithmic}

Since insert still call merge inside, the algorithm is bound to $O(\lg N)$
time.

\subsubsection*{Insertion in Haskell}

Translating the above algorithm to Haskell is trivial.

\lstset{language=Haskell}
\begin{lstlisting}
insert::(Ord a)=> LHeap a -> a -> LHeap a
insert h x = merge (Node 1 x E E) h
\end{lstlisting}

In order to provide a convenient way to build a Leftist heap from
a list, an auxiliary function is given as the following.

\begin{lstlisting}
fromList :: (Ord a) => [a] -> LHeap a
fromList = foldl insert E
\end{lstlisting}

This function can be used like this.

\begin{lstlisting}
fromList [9, 4, 16, 7, 10, 2, 14, 3, 8, 1]
\end{lstlisting}

It will create a Leftist heap as below.

\begin{verbatim}
Node 1 1 (Node 3 2 (Node 2 4 (Node 2 7 (Node 1 16 E E)
(Node 1 10 E E)) (Node 1 9 E E)) (Node 2 3 (Node 1 14 E E)
(Node 1 8 E E))) E
\end{verbatim}

Figure \ref{fig:leftist-tree} shows the result respectively.

\begin{figure}[htbp]
   \begin{center}
   	  \includegraphics[scale=0.5]{img/leftist-tree.ps}
    \caption{A Leftist tree.} \label{fig:leftist-tree}
   \end{center}
\end{figure}

\subsubsection*{Insertion in Scheme/Lisp}

Based on the algorithm, once inserting a new element, a leaf node
is created and merge to the heap.

\lstset{language=lisp}
\begin{lstlisting}
(define (insert t x)
  (merge (make-tree '() 1 x '()) t))
\end{lstlisting}

This function can be verified by continuously insert all elements
from a list so that a Leftist heap can be built as a result.

\begin{lstlisting}
(define (from-list lst)
  (fold-left insert '() lst))
\end{lstlisting}

One example test case which is equivalent to the Haskell program
is shown like the following.

\begin{lstlisting}
(define (test-from-list)
  (from-list '(16 14 10 8 7 9 3 2 4 1)))
\end{lstlisting}

Evaluate this function yields a Leftist tree, which can be output
in in-order as below.

\begin{lstlisting}
((((((((() 1 16 ()) 1 14 ()) 1 10 ()) 1 8 ()) 2 7 (() 1 9 ())) 1 3
()) 2 2 (() 1 4 ())) 1 1 ())
\end{lstlisting}

% ================================================================
%                 Heap sort
% ================================================================
\subsection{Heap sort by Leftist Heap}

With all the basic operations defined, it's straightforward to
implement heap sort in a $N$-popping way. Once we need sort a
list of elements, we first build a Leftist heap from the list.
Then repeatedly pop the minimum element from the heap until heap
is empty.

First is the algorithm to build the Leftist heap by insert all
elements to an empty heap.

\begin{algorithmic}[1]
\Function{BUILD-HEAP}{$A$}
  \State $H \gets NIL$
  \For{each $x$ in $A$}
    \State $T \gets INSERT(T, x)$
  \EndFor
  \State \Return $T$
\EndFunction
\end{algorithmic}

And the heap sort algorithm is as same as the generic one presented
in \ref{heap-sort}.

Note this algorithm is bound to $O(N \lg N)$ time.

\subsection*{Heap sort in Haskell}

In Haskell program, since we have already defined the `fromList'
auxiliary function to build Leftist heap from a list, the heap sort
algorithm can utilize it.

\lstset{language=Haskell}
\begin{lstlisting}
heapSort :: (Ord a) => [a] -> [a]
heapSort = hsort . fromList where
    hsort E = []
    hsort h = (findMin h):(hsort $ deleteMin h)
\end{lstlisting} %$

Here is an example case which is used in previous C++ and Python
programs.

\begin{lstlisting}
heapSort [16, 14, 10, 8, 7, 9, 3, 2, 4, 1]
\end{lstlisting}

It will output the same result as the following.

\begin{verbatim}
[1,2,3,4,7,8,9,10,14,16]
\end{verbatim}

\subsection*{Heap sort in Scheme/Lisp}

In Scheme/Lisp program, we can first use `from-list' function
to turn a list of element into a Leftist heap, then repeatedly
pop the smallest one to a result list.

\lstset{language=lisp}
\begin{lstlisting}
(define (heap-sort lst)
  (define (hsort t)
    (if (null? t) '() (cons (find-min t) (hsort (delete-min t)))))
  (hsort (from-list lst)))
\end{lstlisting}

Here is a simple test case.

\begin{lstlisting}
heap-sort '(16 14 10 8 7 9 3 2 4 1))
\end{lstlisting}

Evaluate the case will out put an ordered list.

\begin{lstlisting}
(1 2 3 4 7 8 9 10 14 16)
\end{lstlisting}

% ================================================================
%                 Skew Heap
% ================================================================


\subsection{Skew heaps}
\label{skew-heap}

The problem with Leftist heap is that, it performs bad in some cases.
For example, if we examine the Leftist heap behind the above heap
sort test case, it's a very unbalanced binary tree as shown in figure
\ref{fig:unbalanced-leftist-tree}\footnote{run Haskell Leftist tree
function: fromList [16, 14, 10, 8, 7, 9, 3, 2, 4, 1] will generates
the result: Node 1 1 (Node 2 2 (Node 1 3 (Node 2 7 (Node 1 8 (Node 1 10 (Node 1 14 (Node 1 16 E E) E) E) E) (Node 1 9 E E)) E) (Node 1 4 E E)) E}.

\begin{figure}[htbp]
   \begin{center}
   	  \includegraphics[scale=0.3]{img/unbalanced-leftist-tree.ps}
    \caption{A very unbalanced Leftist tree build from list [16, 14, 10, 8, 7, 9, 3, 2, 4, 1].} \label{fig:unbalanced-leftist-tree}
   \end{center}
\end{figure}

The binary tree is almost turned to be a linked-list. The worst case
is when feed a ordered list for building Leftist tree, since the
tree changed to linked-list, the time bound degrade from $O(\lg N)$
to $O(N)$.

Skew heap (or {\em self-adjusting heap}) is one step ahead simplified Leftist heap \cite{wiki-skew-heap} \cite{self-adjusting-heaps}.

Remind the Leftist heap, we swap the left and right children during merge
when the rank on left side is less than right side. This comparison strategy
doesn't work when one of the sub tree has only one child. Because
in such case, the rank of the sub tree is always 1 no matter how
big it is. A Brute-force approach is to swap the left and right children
every time when merge. This idea leads to Skew heap.

\subsubsection{Definition of Skew heap}

A Skew heap is a heap implemented with Skew tree. A Skew tree is a special
binary tree. The minimum element is stored in root node. Every sub tree is
also a skew tree.

Based on above discussion, there is no use to keep the rank (or $S$-value)
field, so the Skew heap definition is as same as the binary tree from the
programming language point of view.

\subsubsection*{Definition of Skew heap in Haskell}

After removing rank from Leftist heap definition, we can get the Skew
heap one.

\lstset{language=Haskell}
\begin{lstlisting}
data SHeap a = E -- Empty
             | Node a (SHeap a) (SHeap a) -- element, left, right
               deriving (Eq, Show)
\end{lstlisting}

\subsubsection*{Definition of Skew heap in Scheme/Lisp}
\label{skew-heap-def-lisp}

Since Skew heap is just a special kind of binary tree, the definition is
as same as binary tree's. In Scheme/Lisp, the inner data structure is
list. We organize it in in-order for easy output purpose.

Some auxiliary access functions and a simple constructor is provided.

\lstset{language = lisp}
\begin{lstlisting}
(define (left t)
  (if (null? t) '() (car t)))

(define (elem t)
  (if (null? t) '() (cadr t)))

(define (right t)
  (if (null? t) '() (caddr t)))

;; constructor
(define (make-tree l x r) ;; l: left, x: element, r: right
  (list l x r))
\end{lstlisting}

\subsubsection{Merge}

The merge algorithm tends to be very simple. When we merge two Skew
trees, we compare the root element of each tree, and pick the smaller
one as the new root, we then merge the other tree contains bigger
element onto the right sub tree and swap the left and right children.

\begin{algorithmic}[1]
\Function{MERGE}{$L, R$}
  \If{$L = NIL$}
    \State \Return $R$
  \ElsIf{$R = NIL$}
    \State \Return $L$
  \Else
    \State $T \gets CREATE-EMPTY-NODE()$
    \If{$KEY(L) < KEY(R)$}
      \State $KEY(T) \gets KEY(L)$
      \State $LEFT(T) \gets MERGE(R, RIGHT(L))$
      \State $RIGHT(T) \gets LEFT(L)$
    \Else
      \State $KEY(T) \gets KEY(R)$
      \State $LEFT(T) \gets MERGE(L, RIGHT(R))$
      \State $RIGHT(T) \gets LEFT(R)$
    \EndIf
    \State \Return $T$
  \EndIf
\EndFunction
\end{algorithmic}

\subsubsection*{Skew heap in Haskell}

Translating the above algorithm into Haskell gets a simple merge program.

\lstset{language=Haskell}
\begin{lstlisting}
merge::(Ord a)=>SHeap a -> SHeap a -> SHeap a
merge E h = h
merge h E = h
merge h1@(Node x l r) h2@(Node y l' r') =
    if x < y then Node x (merge r h2) l
    else Node y (merge h1 r') l'
\end{lstlisting}

All the rest programs are as same as the Leftist heap except we needn't
provide rank value when construct a node.

\begin{lstlisting}
insert::(Ord a)=> SHeap a -> a -> SHeap a
insert h x = merge (Node x E E) h

findMin :: SHeap a -> a
findMin (Node x _ _) = x

deleteMin :: (Ord a) => SHeap a -> SHeap a
deleteMin (Node _ l r) = merge l r
\end{lstlisting}

If we feed a completely ordered list to Skew heap, it will results a
fairly balanced binary trees as shown in figure \ref{fig:skew-tree}.

\begin{lstlisting}
*SkewHeap>fromList [1..10]
Node 1 (Node 2 (Node 6 (Node 10 E E) E) (Node 4 (Node 8 E E) E))
(Node 3 (Node 5 (Node 9 E E) E) (Node 7 E E))
\end{lstlisting}

\begin{figure}[htbp]
   \begin{center}
   	  \includegraphics[scale=0.5]{img/skew-tree.ps}
    \caption{Skew tree is still balanced even the input is an ordered list.} \label{fig:skew-tree}
   \end{center}
\end{figure}

\subsubsection*{Skew heap in Scheme/Lisp}

In merge program, if any one of the tree to be merged, it just returns the
other one. For non-trivial case, the program select the smaller one
as the new root element, merge the tree contains the bigger element
to the right child, then swap the two children.

\lstset{language = lisp}
\begin{lstlisting}
(define (merge t1 t2)
  (cond ((null? t1) t2)
	((null? t2) t1)
	((< (elem t1) (elem t2))
	 (make-tree (merge (right t1) t2)
		    (elem t1)
		    (left t1)))
	(else
	 (make-tree (merge t1 (right t2))
		    (elem t2)
		    (left t2)))))
\end{lstlisting}

With merge function defined, insert can be treated as just a special
merge case, that one tree is a leaf which contains the value to be
inserted.

\begin{lstlisting}
(define (insert t x)
  (merge (make-tree '() x '()) t))
\end{lstlisting}

The find minimum, delete minimum and heap sort functions are all
as same as the leftist heap programs, that they can be put to
a generic module.

We only show the testing result in this section.

\begin{lstlisting}
(load "skewheap.scm")
;Loading "skewheap.scm"... done
;Value: insert

(test-from-list)
;Value 13: (((() 4 ()) 2 (((() 9 ()) 7 ((((() 16 ()) 14 ()) 10 ())
8 ())) 3 ())) 1 ())

(test-sort)
;Value 14: (1 2 3 4 7 8 9 10 14 16)
\end{lstlisting}

% ================================================================
%                 Splay Heap
% ================================================================

\section{Splay heap, another explicit binary heap}
\label{splayheap}

Leftist heap presents that it's quite possible to implement
heap data structure with explicit binary tree. Skew heap
shows one method to solve the balance problem. Splay heap
on the other hand, shows another balance approach.

Although Leftist heap and Skew heap use binary trees, they
are not Binary Search tree (BST). If we turn the underground
data structure to binary search tree, the minimum(maximum)
element isn't located in root node. It takes $O(\lg N)$ time
to find the minimum(maximum) element.

Binary search tree becomes inefficient if it isn't well
balanced, operations degrades to $O(N)$ in the worst case.
Although it's quite OK to use red-black tree to implement
binary heap, Splay tree provides a light weight implementation
with acceptable dynamic balancing result.

% ================================================================
%                 Definition
% ================================================================
\subsection{Definition}

Splay tree uses a cache-like approach that it keeps rotating the current
access node close to the top, so that the node can be accessed fast
next time. It defines such kinds of operation as ``Splay''. For an
unbalanced binary search tree, after several times of splay operation, the
tree tends to be more and more balanced. Most basic operation of
Splay tree have amortized $O(\lg N)$ time. Splay tree was invented
by Daniel Dominic Sleator and Robert Endre Tarjan in 1985\cite{wiki-splay-tree}
\cite{self-adjusting-trees}.

\subsubsection{Splaying}

There are two kinds of splaying approach method. The first one is
a bit complex, However, it can be implemented fairly simple with
pattern matching. The second one is simple, but the implementation
is a bit complex.

Denote the node is currently accessed as $X$, it's parent node as $P$,
and it's grand parent node (if has) as $G$. There are 3 steps for
splaying. Each step contains 2 symmetric cases. For illustration
purpose, only one case is shown for each step.

\begin{itemize}
\item {\em Zig-zig step.} As shown in figure \ref{fig:zig-zig}, in this case,
both $X$, and its parent $P$ are either left children or right children. By
rotating 2 times, X becomes the new root.

\begin{figure}[htbp]
   \begin{center}
   	  \includegraphics[scale=0.5]{img/zig-zig-a.ps}
          \includegraphics[scale=0.5]{img/zig-zig-b.ps}
          \caption{Zig-zig case.} \label{fig:zig-zig}
   \end{center}
\end{figure}

\item {\em Zig-zag step.} As shown in figure \ref{fig:zig-zag}, in this
case, $X$ is the right child of its parent while $P$ is the left child.
Or $X$ is the left child of $P$, and $P$ is the right child of $G$.
After rotation, $X$ becomes the new root, $P$ and $G$ become siblings.

\begin{figure}[htbp]
   \begin{center}
   	  \includegraphics[scale=0.5]{img/zig-zag-a.ps}
          \includegraphics[scale=0.5]{img/zig-zag-b.ps}
          \caption{Zig-zag case.} \label{fig:zig-zag}
   \end{center}
\end{figure}

\item {\em Zig step.} As shown in figure \ref{fig:zig}, in this case,
$P$ is the root, we perform one rotate, so that $X$ becomes new root.
Note this is the last step in splay operation.

\begin{figure}[htbp]
   \begin{center}
   	  \includegraphics[scale=0.5]{img/zig-a.ps}
          \includegraphics[scale=0.5]{img/zig-b.ps}
          \caption{Zig case.} \label{fig:zig}
   \end{center}
\end{figure}

\end{itemize}

Okasaki found a simple rule for Splaying \cite{okasaki-book},
that every time we follow
two left branches in a row, or two right branches in a row, we rotate
those two nodes.

Based on this rule, the Splaying can be realized in such a way.
When we access node for a key $x$ (can be during the process of
inserting a node, or looking up a node, or deleting a node), if
we found we traverse two left branches (or two right branches), we
partition the tree in two part $L$ and $R$, where $L$ contains all
nodes smaller than $x$, and $R$ contains all nodes bigger than $x$.
We can then create a new tree (for instance in insertion),
with $x$ as the root, $L$ as the left child and $R$ is the right child.
Note the partition process is recursive, because it will do splaying
inside.

\begin{algorithmic}[1]
\Function{PARTITION}{$T, pivot$}
  \If{$T = NIL$}
    \State \Return $(NIL, NIL)$
  \EndIf
  \State $x \gets KEY(T)$
  \State $l \gets LEFT(T)$
  \State $r \gets RIGHT(T)$
  \State $L \gets NIL$
  \State $R \gets NIL$
  \If{$x < pivot$}
    \If{$r = NIL$}
      \State $L \gets T$
    \Else
      \State $x' \gets KEY(r)$
      \State $l' \gets LEFT(r)$
      \State $r' \gets RIGHT(r)$
      \If{$x' < pivot$}
        \State $(small, big) \gets PARTITION(r', pivot)$
        \State $L \gets CREATE-NODE(CREATE-NODE(l, x, r), x', small)$
        \State $R \gets big$
      \Else
        \State $(small, big) \gets PARTITION(l', pivot)$
        \State $L \gets CREATE-NODE(l, x, small)$
        \State $R \gets CREATE-NODE(big, x', r')$
      \EndIf
    \EndIf
  \Else
    \If{$l = NIL$}
      \State $L \gets NIL$
    \Else
      \State $x' \gets KEY(L)$
      \State $l' \gets LEFT(L)$
      \State $r' \gets RIGHT(L)$
      \If{$x' > pivot$}
        \State $(small, big) \gets PARTITION(l', pivot)$
        \State $L \gets small$
        \State $R \gets CREATE-NODE(l', x', CREATE-NODE(r', x, r))$
      \Else
        \State $(small, big) \gets PARTITION(r', pivot)$
        \State $L \gets CREATE-NODE(l', x, small)$
        \State $R \gets CREATE-NODE(big, x, r)$
      \EndIf
    \EndIf
  \EndIf
  \State \Return $(L, R)$
\EndFunction

\Function{CREATE-NODE}{$l, x, r$}
  \State $T \gets CREATE-NEW-NODE()$
  \State $KEY(T) \gets x$
  \State $LEFT(T) \gets l$
  \State $RIGHT(T) \gets r$
  \State \Return $T$
\EndFunction
\end{algorithmic}

\subsection*{Definition of Splay heap in Haskell}

Since Splay tree is a special binary search tree, the definition of
them are same.

\lstset{language=Haskell}
\begin{lstlisting}
data STree a = E -- Empty
             | Node (STree a) a (STree a) -- left, element, right
               deriving (Eq, Show)
\end{lstlisting}

Translate the above algorithm into Haskell gets the following partition
program.

\begin{lstlisting}
partition :: (Ord a) => STree a -> a -> (STree a, STree a)
partition E _ = (E, E)
partition t@(Node l x r) y
    | x < y =
        case r of
          E -> (t, E)
          Node l' x' r' ->
              if x' < y then
                  let (small, big) = partition r' y in
                  (Node (Node l x l') x' small, big)
              else
                  let (small, big) = partition l' y in
                  (Node l x small, Node big x' r')
    | otherwise =
        case l of
          E -> (E, t)
          Node l' x' r' ->
              if y < x' then
                  let (small, big) = partition l' y in
                  (small, Node l' x' (Node r' x r))
              else
                  let (small, big) = partition r' y in
                  (Node l' x' small, Node big x r)
\end{lstlisting}

In a language which supports `pattern matching' such as Haskell,
Splay can be implemented in a very simple and straightforward style
by translating the figure \ref{fig:zig-zig}, \ref{fig:zig-zag} and
\ref{fig:zig} directly into patterns. Note that for each step,
there are two left-right symmetric cases.

\lstset{language=Haskell}
\begin{lstlisting}
-- splay by pattern matching
splay :: (Eq a) => STree a -> a ->STree a
-- zig-zig
splay t@(Node (Node (Node a x b) p c) g d) y =
    if x == y then Node a x (Node b p (Node c g d)) else t
splay t@(Node a g (Node b p (Node c x d))) y =
    if x == y then Node (Node (Node a g b) p c) x d else t
-- zig-zag
splay t@(Node (Node a p (Node b x c)) g d) y =
    if x == y then Node (Node a p b) x (Node c g d) else t
splay t@(Node a g (Node (Node b x c) p d)) y =
    if x == y then Node (Node a g b) x (Node c p d) else t
-- zig
splay t@(Node (Node a x b) p c) y = if x == y then Node a x (Node b p c) else t
splay t@(Node a p (Node b x c)) y = if x == y then Node (Node a p b) x c else t
-- otherwise
splay t _ = t
\end{lstlisting}

\subsection*{Definition of Splay heap in Scheme/Lisp}

Since Splay heap is essentially binary search tree. The definition
is same. In order to output the tree in in-order. we arrange it
as (left element right) format.

Some auxiliary functions to access left child, element, right
child and constructor are defined as same as in ref{skew-heap-def-lisp}.

The function which can partition the tree into 2 parts according to
a pivot value based on algorithm $PARTITION$ is defined as the following.
It's a bit complex, but not hard. We compared the pivot and the element
and traverse the tree based on binary search tree property. In case
there are two left (or right) children traversed, the tree is rotated
by splaying. It makes the tree more and more balanced.

\lstset{language=lisp}
\begin{lstlisting}
(define (partition t pivot)
  (if (null? t)
      (cons '() '())
      (let ((l (left t))
	    (x (elem t))
	    (r (right t)))
	(if (< x pivot)
	    (if (null? r)
		(cons t '())
		(let ((l1 (left r))
		      (x1 (elem r))
		      (r1 (right r)))
		  (if (< x1 pivot)
		      (let* ((p (partition r1 pivot))
			     (small (car p))
			     (big (cdr p)))
			(cons (make-tree (make-tree l x l1) x1 small) big))
		      (let* ((p (partition l1 pivot))
			     (small (car p))
			     (big (cdr p)))
			(cons (make-tree l x small) (make-tree big x1 r1))))))
	    (if (null? l)
		(cons '() t)
		(let ((l1 (left l))
		      (x1 (elem l))
		      (r1 (right l)))
		  (if (> x1 pivot)
		      (let* ((p (partition l1 pivot))
			     (small (car p))
			     (big (cdr p)))
			(cons small (make-tree l1 x1 (make-tree r1 x r))))
		      (let* ((p (partition r1 pivot))
			     (small (car p))
			     (big (cdr p)))
			(cons (make-tree l1 x1 small) (make-tree big x r))))))))))
\end{lstlisting}

Although there is no direct pattern matching language feature supporting in
Scheme/Lisp, it possible to provide a splay function by guard clause.
Compare to the Haskell one, it is a bit more complex.

\begin{lstlisting}
(define (splay l x r y)
  (cond ((eq? y (elem (left l))) ;; zig-zig
	 (make-tree (left (left l))
		    (elem (left l))
		    (make-tree (right (left l))
			       (elem l)
			       (make-tree (right l) x r))))
	((eq? y (elem (right r))) ;; zig-zig
	 (make-tree (make-tree (make-tree l x (left r))
			       (elem r)
			       (left (right r)))
		    (elem (right r))
		    (right (right r))))
	((eq? y (elem (right l))) ;; zig-zag
	 (make-tree (make-tree (left l) (elem l) (left (right l)))
		    (elem (right l))
		    (make-tree (right (right l)) x r)))
	((eq? y (elem (left r))) ;; zig-zag
	 (make-tree (make-tree l x (left (left r)))
		    (elem (left r))
		    (make-tree (right (left r)) (elem r) (right r))))
	((eq? y (elem l)) ;; zig
	 (make-tree (left l) (elem l) (make-tree (right l) x r)))
	((eq? y (elem r)) ;; zig
	 (make-tree (make-tree l x (left r)) (elem r) (right r)))
	(else (make-tree l x r))))
\end{lstlisting}

% ================================================================
%                 Basic heap operations
% ================================================================
\subsection{Basic heap operations}

There are two methods to implement basic heap operations for Splay
heap. One is by using $PARTITION$ algorithm we defined, the other
is to utilize a $SPLAY$ process, which can be realized in pattern matching
way in languages equipped with this feature.

\subsubsection{Insertion}

If using $PARTITION$ algorithm, once we want to insert a new $x$ into
a heap $T$, we can first partition it into two trees, $L$ and $R$. Where
L contains all nodes smaller than $x$, and $R$ contains all bigger ones.
We then construct a new node, with $x$ as the root and $L$, $R$ as children.

\begin{algorithmic}[1]
\Function{INSERT}{$T, x$}
  \State $(L, R) \gets PARTITION(T, x)$
  \State \Return $CREATE-NODE(L, x, R)$
\EndFunction
\end{algorithmic}

If there is a $SPLAY$ algorithm defined, the insert can be done in a
recursive way as the following.

\begin{algorithmic}[1]
\Function{INSERT'}{$T, x$}
  \If{$T = NIL$}
    \State \Return $CREATE-NODE(NIL, x, NIL)$
  \EndIf
  \If{$KEY(T) < x$}
    \State $LEFT(T) \gets INSERT'(LEFT(T), x)$
  \Else
    \State $RIGHT(T) \gets INSERT'(RIGHT(T), x)$
  \EndIf
  \State \Return $SPLAY(T, x)$
\EndFunction
\end{algorithmic}

\subsubsection*{Insertion in Haskell}

It's easy to translate the above algorithms into Haskell.

\lstset{language=Haskell}
\begin{lstlisting}
insert :: (Ord a) => STree a -> a -> STree a
insert t x = Node small x big where (small, big) = partition t x
\end{lstlisting}

And the pattern matching one is in recursive manner as below.

\begin{lstlisting}
insert' :: (Ord a) => STree a -> a -> STree a
insert' E y = Node E y E
insert' (Node l x r) y
    | x > y     = splay (Node (insert' l y) x r) y
    | otherwise = splay (Node l x (insert' r y)) y
\end{lstlisting}

\subsubsection*{Insertion in Scheme/Lisp}

By using partition method, when insert a new element to the Splay
heap, we use this element as the pivot to partition the tree.
After that we set this new element as the new root, the sub tree
contains all elements smaller than root as the left child, and
the others as the right child. Note that we intend not handle the
duplicated elements case, because it quite possible to contains
them in Splay heap.

\lstset{language=lisp}
\begin{lstlisting}
(define (insert t x)
  (let* ((p (partition t x))
	 (small (car p))
	 (big (cdr p)))
    (make-tree small x big)))
\end{lstlisting}

And if use splay function, the insert can be implemented straightforward
like binary search tree, except that we need apply splaying recursively
after that.

\begin{lstlisting}
(define (insert-splay t x)
  (cond ((null? t) (make-tree '() x '()))
	((> (elem t) x)
	 (splay (insert-splay (left t) x) (elem t) (right t) x))
	(else
	 (splay (left t) (elem t) (insert-splay (right t) x) x))))
\end{lstlisting}

\subsubsection{Verify how Splay improve the balance}
In order to show how Splaying improves the balance of binary search
tree, we first insert a list of ordered element to the tree and then
performs a large number of arbitrary node access.

A look-up algorithm is provided with Splaying operation inside.

\begin{algorithmic}[1]
\Function{LOOKUP}{$T, x$}
  \If{$KEY(T) = x$}
    \State \Return $T$
  \ElsIf{$KEY(T) > x$}
    \State $LEFT(T) \gets LOOKUP(LEFT(T), x)$
  \Else
    \State $RIGHT(T) \gets LOOKUP(RIGHT(T), x)$
  \EndIf
  \State \Return $SPLAY(T, x)$
\EndFunction
\end{algorithmic}

Translate this algorithm into Haskell yields the following program\footnote{$LOOKUP$ algorithm in other language is skipped.}.

\lstset{language=Haskell}
\begin{lstlisting}
lookup' :: (Ord a) => STree a -> a -> STree a
lookup' E _ = E
lookup' t@(Node l x r) y
    | x == y    = t
    | x > y     = splay (Node (lookup' l y) x r) y
    | otherwise = splay (Node l x (lookup' r y)) y
\end{lstlisting}

Next we can create a Splay heap by inserting sequence number from
1 to 10. After that, We random select from a number from this range,
and perform looking up 1000 times as below.

\begin{lstlisting}
testSplay = do
  xs <- sequence (replicate 1000 (randomRIO(1, 10)))
  putStrLn $ show (foldl lookup' t xs)
      where
        t = foldl insert' (E::STree Int) [1..10]
\end{lstlisting} %$

Run these test will make the tree quite balance. Below is an example
result. Figure \ref{fig:splay-result} shows the tree after splaying.

\begin{verbatim}
Node (Node (Node (Node E 1 E) 2 E) 3 E) 4 (Node (Node E 5 E) 6 (Node
(Node (Node E 7 E) 8 E) 9 (Node E 10 E)))
\end{verbatim}

\begin{figure}[htbp]
   \begin{center}
   	  \includegraphics[scale=0.5]{img/splay-tree.ps}
          \caption{Splaying helps improving the balance.} \label{fig:splay-result}
   \end{center}
\end{figure}


\subsubsection{Find minimum (top) and delete minimum (pop)}
Since Splay tree is just a special binary search tree, so the minimum
element is stored in the left most node. We need keep traversing
the left child.

\begin{algorithmic}[1]
\Function{FIND-MIN}{$T$}
  \If{$LEFT(T) = NIL$}
    \State \Return $KEY(T)$
  \Else
    \State \Return $FIND-MIN(LEFT(T))$
  \EndIf
\EndFunction
\end{algorithmic}

And for pop operation, the algorithm need keep traversing in left
and also remove the minimum element from the tree. In case there
are two left nodes traversed, a splaying operation should be performed.

\begin{algorithmic}[1]
\Function{DEL-MIN}{$T$}
  \If{$LEFT(T) = NIL$}
    \State \Return $RIGHT(T)$
  \ElsIf{$LEFT(LEFT(T)) = NIL$}
    \State $LEFT(T) \gets RIGHT(LEFT(T))$
    \State \Return $T$
  \Else
    \Comment{Splaying}
    \State $l \gets LEFT(T)$
    \State $r \gets CREATE-NEW-NODE()$
    \State $LEFT(r) \gets RIGHT(l)$
    \State $KEY(r) \gets KEY(T)$
    \State $RIGHT(r) \gets RIGHT(T)$
    \State $T' \gets CREATE-NEW-NODE()$
    \State $LEFT(T') \gets DEL-MIN(LEFT(l))$
    \State $KEY(T') \gets KEY(l)$
    \State $RIGHT(T') \gets r$
    \State \Return $T'$
  \EndIf
\EndFunction
\end{algorithmic}

Note that the find minimum and delete minimum algorithms both are
bound to $O(\lg N)$.

\subsubsection*{Find minimum (top) and delete minimum in Haskell}

When translate the above algorithms into Haskell, one option is to
use pattern matching.

\lstset{language=Haskell}
\begin{lstlisting}
findMin :: STree a -> a
findMin (Node E x _) = x
findMin (Node l x _) = findMin l

deleteMin :: STree a -> STree a
deleteMin (Node E x r) = r
deleteMin (Node (Node E x' r') x r) = Node r' x r
deleteMin (Node (Node l' x' r') x r) = Node (deleteMin l') x' (Node r' x r)
\end{lstlisting}

\subsubsection*{Find minimum (top) and delete minimum in Scheme/Lisp}

In Scheme/Lisp, finding minimum element means keep traversing to left;
while for max-heap, we need change the program to keep traversing to
right.

\lstset{language=lisp}
\begin{lstlisting}
(define (find-min t)
  (if (null? (left t))
      (elem t)
      (find-min (left t))))
\end{lstlisting}

And for delete minimum program, except for trivial case, splaying also
need be performed if we traverse in left twice.

\begin{lstlisting}
(define (delete-min t)
  (cond ((null? (left t)) (right t))
	((null? (left (left t))) (make-tree (right (left t)) (elem t) (right t)))
	(else (make-tree (delete-min (left (left t)))
			 (elem (left t))
			 (make-tree (right (left t)) (elem t) (right t)))))
\end{lstlisting}

\subsubsection{Merge}
Merge is another basic operation for heaps as it is widely used in Graph algorithms. By using $PARTITION$ algorithm, merge can be realized in $O(\lg N)$ time.

When merging two Splay trees, for non-trivial case, we can take the root element of the first tree as the new root element, then partition the second tree with the new root as the pivot value. After that we recursively merge
the children of the first tree with the partition result. This algorithm is shown as the following.

\begin{algorithmic}[1]
\Function{MERGE}{$T1, T2$}
  \If{$T1 = NIL$}
    \State \Return $T2$
  \Else
    \State $L \gets LEFT(T1)$
    \State $R \gets RIGHT(T1)$
    \State $k \gets KEY(T1)$
    \State $(L', R') \gets PARTITION(T2, k)$
    \State \Return $CREATE-NODE(MERGE(L, L'), k MERGE(R, R'))$
  \EndIf
\EndFunction
\end{algorithmic}

\subsubsection*{Merge two Splay heaps in Haskell}
In Haskell, we can handle the trivial and non-trivial case by pattern matching.

\lstset{language=Haskell}
\begin{lstlisting}
merge :: (Ord a) => STree a -> STree a -> STree a
merge E t = t
merge (Node l x r) t = Node (merge l l') x (merge r r')
    where (l', r') = partition t x
\end{lstlisting}

\subsubsection*{Merge two Splay heaps in Scheme/Lisp}

In Scheme/Lisp, we translate the above algorithm strictly as the following.

\lstset{language=lisp}
\begin{lstlisting}
(define (merge t1 t2)
  (if (null? t1)
      t2
      (let* ((p (partition t2 (elem t1)))
	     (small (car p))
	     (big (cdr p)))
	(make-tree (merge (left t1) small) (elem t1) (merge (right t1) big)))))
\end{lstlisting}

% ================================================================
%                 Heap sort
% ================================================================
\subsection{Heap sort}

Since the internal implementation of the Splay heap is completely
transparent to the heap interface, the heap sort algorithm can
be reused. It means that the heap sort algorithm is generic no
matter what the underground data structure is.

% ================================================================
%                 Short summary
% ================================================================
\section{Notes and short summary}

In this post, we reviewed the definition of binary heap, and adjust it
a bit so that as long as the heap property is maintained, all binary
representation of data structures can be used to implement binary heap.

This enable us not only limit to the popular implicit binary heap
by array, but also extend to explicit binary heaps including Leftist
heap, Skew heap and Splay heap. Note that, the implicit binary heap
by array is particularly convenient for imperative implementation
because it intense uses random index access which can be mapped to
a completely binary tree. It's hard to directly find functional
counterpart in this way.

However, by using explicit binary tree, functional implementation
can be easily achieved, most of them have $O(\lg N)$ worst case
performance, and some of them even reach $O(1)$ amortize time.
Okasaki in \cite{okasaki-book} shows good analysis of these data
structures.

In this post, Only pure functional realization for Leftist heap,
Skew heap, and Splay heap are explained, they are all possible to
be implemented in imperative way. I skipped them only for the
purpose of presenting comparable functional algorithm to the
implicit binary heap by array.

It's very natural to extend the concept from binary tree to
K-ary (K-way) tree, which leads to other useful heap concept such as
Binomial heaps, Fibonacci heaps and pairing heaps. I'll introduce
them in a separate post later.

% ================================================================
%                 Appendix
% ================================================================
\section{Appendix} \label{appendix}
%\appendix
All programs provided along with this article are free for
downloading.

\subsection{Prerequisite software}
GNU Make is used for easy build some of the program. For C++ and ANSI C programs,
GNU GCC and G++ 3.4.4 are used.
For Haskell programs GHC 6.10.4 is used
for building. For Python programs, Python 2.5 is used for testing, for
Scheme/Lisp program, MIT Scheme 14.9 is used.

all source files are put in one folder. Invoke 'make' or 'make all'
will build C++ Program.

There is no separate Haskell main program module, however, it is possible to run the program in GHCi.

\begin{itemize}
\item bheap.hpp. This is the C++ source file contains binary heap defintion and functions, There are two types of approach, one is the ``reference + size'' way, the other is ``range'' representation.

\item test.cpp. This is the main C++ program to test bheap.hpp module.

\item bheap.py. This is the Python source file for binary heap implementation. It's a self-contained program with test cases embedded. run it directly will performs all test cases. It is also possible to import it as a module.

\item LeftistHeap.hs. This is the Haskell program for Leftist Heap with some simple test cases as well. It can be loaded to GHCi directly.

\item SkewHeap.hs. This is the Haskell program for Skew heap defintion. Some very simple test cases are provided.

\item SplayHeap.hs. This is the Haskell proram for Splay heap definition.

\item lefitst.scm. This is the Scheme/Lisp program for Leftist heap.

\item skewheap.scm. This is the Scheme/Lisp program for Skew heap. Same as leftist.scm, some gerneric functions are reused.

\item genheap.scm. This is the Scheme/Lisp general heap function utilities which are all same for varies of heaps. It can be overwritten afterwards.

\item splayheap.scm. This is the Scheme/Lisp program for Splay heap definition.
\end{itemize}

download position: http://sites.google.com/site/algoxy/btree/bheap.zip

\begin{thebibliography}{99}

\bibitem{CLRS}
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein. ``Introduction to Algorithms, Second Edition''. The MIT Press, 2001. ISBN: 0262032937.

\bibitem{wiki-heap}
Heap (data structure), Wikipedia. http://en.wikipedia.org/wiki/Heap\_(data\_structure)

\bibitem{wiki-heapsort}
Heapsort, Wikipedia. http://en.wikipedia.org/wiki/Heapsort

\bibitem{okasaki-book}
Chris Okasaki. ``Purely Functional Data Structures''. Cambridge university press, (July 1, 1999), ISBN-13: 978-0521663502

\bibitem{rosetta-heapsort}
Sorting algorithms/Heapsort. Rosetta Code. http://rosettacode.org/wiki/Sorting\_algorithms/Heapsort

\bibitem{wiki-leftist-tree}
Leftist Tree, Wikipedia. http://en.wikipedia.org/wiki/Leftist\_tree

\bibitem{brono-book}
Bruno R. Preiss. Data Structures and Algorithms with Object-Oriented Design Patterns in Java. http://www.brpreiss.com/books/opus5/index.html

\bibitem{TAOCP}
Donald E. Knuth. ``The Art of Computer Programming. Volume 3: Sorting and Searching.''. Addison-Wesley Professional;
2nd Edition (October 15, 1998). ISBN-13: 978-0201485417. Section 5.2.3 and 6.2.3

\bibitem{wiki-skew-heap}
Skew heap, Wikipedia. http://en.wikipedia.org/wiki/Skew\_heap

\bibitem{self-adjusting-heaps}
Sleator, Daniel Dominic; Jarjan, Robert Endre. ``Self-adjusting heaps'' SIAM Journal on Computing 15(1):52-69. doi:10.1137/0215004 ISSN 00975397 (1986)

\bibitem{wiki-splay-tree}
Splay tree, Wikipedia. http://en.wikipedia.org/wiki/Splay\_tree

\bibitem{self-adjusting-trees}
Sleator, Daniel D.; Tarjan, Robert E. (1985), ``Self-Adjusting Binary Search Trees'', Journal of the ACM 32(3):652 - 686, doi: 10.1145/3828.3835

\bibitem{NIST}
NIST, ``binary heap''. http://xw2k.nist.gov/dads//HTML/binaryheap.html

\end{thebibliography}

\ifx\wholebook\relax \else
\end{document}
\fi
