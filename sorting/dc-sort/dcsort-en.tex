\ifx\wholebook\relax \else
% ------------------------

\documentclass{article}
%------------------- Other types of document example ------------------------
%
%\documentclass[twocolumn]{IEEEtran-new}
%\documentclass[12pt,twoside,draft]{IEEEtran}
%\documentstyle[9pt,twocolumn,technote,twoside]{IEEEtran}
%
%-----------------------------------------------------------------------------
%\input{../../../common.tex}
\input{../../common-en.tex}

\setcounter{page}{1}

\begin{document}

%--------------------------

% ================================================================
%                 COVER PAGE
% ================================================================

\title{Divide and conquer, Quick sort vs. Merge sort}

\author{Larry~LIU~Xinyu
\thanks{{\bfseries Larry LIU Xinyu } \newline
  Email: liuxinyu95@gmail.com \newline}
  }

\maketitle
\fi

\markboth{Quick sort vs. Merge sort}{Elementary Algorithms}

\ifx\wholebook\relax
\chapter{Divide and conquer, Quick sort vs. Merge sort}
\numberwithin{Exercise}{chapter}
\fi

% ================================================================
%                 Introduction
% ================================================================
\section{Introduction}
\label{introduction}

It's proved that the best approximate performance of comparison based sorting is $O(n \lg n)$ \cite{TAOCP}.
In this chapter, two divide and conquer sorting algorithms are introduced. Both of them
perform in $O(n \lg n)$ time. One is quick sort. It is
the most popular sorting algorithm. Quick sort has been well studied, many programming libraries provide
sorting tools based on quick sort.

In this chapter, we'll first introduce the idea of quick sort, which demonstrates the power of divide
and conquer strategy well. Several variants will be explained, and we'll see when quick sort performs poor
in some special cases. That the algorithm is not able to partition the sequence in balance.

In order to solve the unbalanced partition problem, we'll next introduce about merge sort, which ensure
the sequence to be well partitioned in all the cases. Some variants of merge sort, including nature merge
sort, bottom-up merge sort are shown as well.

Same as other chapters, all the algorithm will be realized in both imperative and functional approaches.

% ================================================================
% Quick sort
% ================================================================
\section{Quick sort}
\index{Quick sort}

Consider a teacher arranges a group of kids in kindergarten to stand in a line for some game.
The kids need stand in order of their heights, that the shortest one stands on the left most,
while the tallest stands on the right most. How can the teacher instruct these kids, so that
they can stand in a line by themselves?

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.8]{img/kids-inline.eps}
 \caption{Instruct kids to stand in a line}
 \label{fig:knuth-ssort}
\end{figure}

There are many strategies, and the quick sort approach can be applied here:

\begin{enumerate}
  \item The first kid raises his/her hand. The kids who are shorter than him/her stands to the left to this child; the kids who are taller than him/her stands to the right of this child;
  \item All the kids move to the left, if there are, repeat the above step; all the kids move to the right repeat the same step as well.
\end{enumerate}

Suppose a group of kids with their heights as $\{102, 100, 98, 95, 96, 99, 101, 97\}$ with [cm] as the unit.
The following table illustrate how they stand in order of height by following this method.

\begin{tabular}{ | c c c c c c c c |}
\hline
{\bf 102} & 100 & 98 & 95 & 96 & 99 & 101 & 97 \\
{\bf 100} & 98 & 95 & 96 & 99 & 101 & 97 & {\em 102} \\
{\bf 98} & 95 & 96 & 99 & 97 & {\em 100} & 101 & {\em 102} \\
{\bf 95} & 96 & 97 & {\em 98} & 99 & {\em 100} & {\em 101} & {\em 102} \\
{\em 95} & {\bf 96} & 97 & {\em 98} & {\em 99} & {\em 100} & {\em 101} & {\em 102} \\
{\em 95} & {\em 96} & 97 & {\em 98} & {\em 99} & {\em 100} & {\em 101} & {\em 102} \\
{\em 95} & {\em 96} & {\em 97} & {\em 98} & {\em 99} & {\em 100} & {\em 101} & {\em 102} \\
\hline
\end{tabular}

At the beginning, the first child with height 102 cm raises his/her hand. We call this kid the pivot and mark
this height in bold.
It happens that this is the tallest kid.
So all others stands to the left side, which is represented in the second row in the above table. Note that
the child with height 102 cm is in the final ordered position, thus we mark it italic. Next the kid with
height 100 cm raise hand, so the children of heights 98, 95, 96 and 99 cm stand to his/her left, and there
is only 1 child of height 101 cm who is taller than this pivot kid. So he stands to the right hand.
The 3rd row in the table shows this stage accordingly. After that, the child of 98 cm high is selected
as pivot on left hand; while the child of 101 cm high on the right is selected as pivot. Since there
are no other kids in the unsorted group with 101 cm as pivot, this small group is ordered already and
the kid of height 101 cm is in the final proper position. The same method is applied to the group of kids
which haven't been in correct order until all of them are stands in the final position.

\subsection{Basic version}
\index{Quick Sort!Basic version}
Summarize the above instruction leads to the recursive description of quick sort. In order to sort a sequence
of elements $L$.

\begin{itemize}
\item If $L$ is empty, the result is obviously empty; This is the trivial edge case;
\item Otherwise, select an arbitrary element in $L$ as a pivot, recursively sort all elements not greater than the pivot, put
the result on the left hand of the pivot, {\em and} recursively sort all elements which are greater than the pivot, put
the result on the right hand of the pivot.
\end{itemize}

Note that the emphasized word {\em and}, we don't use `then' here, which indicates it's quite OK that
the recursive sort on the left and right can be done in parallel. We'll return this parallelism topic soon.

Quick sort was first developed by C. A. R. Hoare in 1960 \cite{TAOCP}, \cite{wiki-qs}. What we describe here
is a basic version. Note that it doesn't state how to select the pivot. We'll see soon that the pivot selection
affects the performance of quick sort dramatically.

The most simple method to select the pivot is always choose the first one so that quick sort can be formalized
as the following.

\be
sort(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & L = \phi \\
  sort(\{ x | x \in L', x \leq l_1 \}) \cup \{ l_1 \} \cup sort(\{ x | x \in L', l_1 < x \}) & otherwise \\
  \end{array}
\right.
\ee

Where $l_1$ is the first element of the non-empty list $L$, and $L'$ contains the rest elements $\{l_2, l_3, ...\}$.
Note that we use Zermelo Frankel expression (ZF expression for short)\footnote{Name for the two mathematics who found the modern set theory.}, which is also known as list
comprehension. A ZF expression $\{ a | a \in S, p_1(a), p_2(a), ... \}$ means taking all element in set $S$,
if it satisfies all the predication $p_1, p_2, ...$. ZF expression is originally used for representing
{\em set}, we extend it to express list for the sake of brevity. There can be duplicated elements, and
different permutations represent for different list. Please refer to the appendix
about list in this book for detail.

It's quite straightforward to translate this equation to real code if list comprehension is supported.
The following Haskell code is given for example:

\lstset{language=Haskell}
\begin{lstlisting}
sort [] = []
sort (x:xs) = sort [y | y<-xs, y <= x] ++ [x] ++ sort [y | y<-xs, x < y]
\end{lstlisting}

This might be the shortest quick sort program in the world at the time when this book is written. Even
a verbose version is still very expressive:

\lstset{language=Haskell}
\begin{lstlisting}
sort [] = []
sort (x:xs) = as ++ [x] ++ bs where
    as = sort [ a | a <- xs, a <= x]
    bs = sort [ b | b <- xs, x < b]
\end{lstlisting}

There are some variants of this basic quick sort program, such as using explicit filtering instead of
list comprehension. The following Python program demonstrates this for example:

\lstset{language=Python}
\begin{lstlisting}
def sort(xs):
    if xs == []:
        return []
    pivot = xs[0]
    as = sort(filter(lambda x : x <= pivot, xs[1:]))
    bs = sort(filter(lambda x : pivot < x, xs[1:]))
    return as + [pivot] + bs
\end{lstlisting}

\subsection{Strict weak ordering}
\index{Quick Sort!Strict weak ordering}
We assume the elements are sorted in monotonic none decreasing order so far. It's quite possible to customize
the algorithm, so that it can sort the elements in other ordering criteria. This is necessary in practice because
users may sort numbers, strings, or other complex objects (even list of lists for example).

The typical generic solution is to abstract the comparison as a parameter as we mentioned in chapters about
insertion sort and selection sort. Although it needn't the total ordering, the comparison must satisfy
{\em strict weak ordering} at least \cite{wiki-total-order} \cite{wiki-sweak-order}.

For the sake of brevity, we only considering sort the elements by using less than or equal
(equivalent to not greater than) in the rest of the chapter.

\subsection{Partition}
\index{Quick sort!partition}
Observing that the basic version actually takes two passes to find all elements which are greater than the pivot
as well as to find the others which are not respectively. Such partition can be accomplished by only one pass. We explicitly define the partition as below.

\be
partition(p, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\phi, \phi) & L = \phi \\
  (\{ l_1 \} \cup A, B) & p(l_1), (A, B) = partition(p, L') \\
  (A, \{ l_1 \} \cup B) & \lnot p(l_1)
  \end{array}
\right.
\ee

Note that the operation $\{x\} \cup L$ is just a `cons' operation, which only takes constant time.
The quick sort can be modified accordingly.

\be
sort(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & L = \phi \\
  sort(A) \cup \{l_1\} \cup sort(B) & otherwise, (A, B) = partition(\lambda_x x \leq l_1, L')
  \end{array}
\right.
\ee

Translating this new algorithm into Haskell yields the below code.

\lstset{language=Haskell}
\begin{lstlisting}
sort [] = []
sort (x:xs) = sort as ++ [x] ++ sort bs where
    (as, bs) = partition (<= x) xs

partition _ [] = ([], [])
partition p (x:xs) = let (as, bs) = partition p xs in
    if p x then (x:as, bs) else (as, x:bs)
\end{lstlisting}

The concept of partition is very critical to quick sort. Partition is also very important to many
other sort algorithms. We'll explain how it generally affects the sorting methodology by the end
of this chapter. Before further discussion about fine tuning of quick sort specific partition, let's
see how to realize it in-place imperatively.

There are many partition methods. The one given by Nico Lomuto \cite{pearls} \cite{CLRS} will be used here as it's
easy to understand. We'll show other partition algorithms soon and see how partitioning affects the performance.

\begin{figure}[htbp]
   \centering
   \subfloat[Partition invariant]{\includegraphics[scale=0.5]{img/partition-1-way.ps}} \\
   \subfloat[Start]{\includegraphics[scale=0.5]{img/partition-1-way-start.ps}} \\
   \subfloat[Finish]{\includegraphics[scale=0.5]{img/partition-1-way-finish.ps}}
   \caption{Partition a range of array by using the left most element as pivot.}
   \label{fig:partition-1-way}
\end{figure}

Figure \ref{fig:partition-1-way} shows the idea of this one-pass partition method. The array is processed from
left to right. At any time, the array consists of the following parts as shown in figure \ref{fig:partition-1-way} (a):

\begin{itemize}
\item The left most cell contains the pivot; By the end of the partition process, the pivot will be moved to the
final proper position;
\item A segment contains all elements which are not greater than the pivot. The right boundary of this segment is
marked as `left';
\item A segment contains all elements which are greater than the pivot. The right boundary of this segment is marked
as `right'; It means that elements between `left' and `right' marks are greater than the pivot;
\item The rest of elements after `right' mark haven't been processed yet. They may be greater than the
pivot or not.
\end{itemize}

At the beginning of partition, the `left' mark points to the pivot and the `right' mark points to the
the second element next to the pivot in the array as in Figure
\ref{fig:partition-1-way} (b); Then the algorithm repeatedly advances the right mark one element after the other
till passes the end of the array.

In every iteration, the element pointed by the `right' mark is compared with the
pivot. If it is greater than the pivot, it should be among the segment between the `left' and `right' marks, so that
the algorithm goes on to advance the `right' mark and examine the next element; Otherwise, since the element pointed
by `right' mark is less than or equal to the pivot (not greater than), it should be put before the `left' mark.
In order to achieve this, the `left' mark needs be advanced by one, then exchange the elements pointed by the `left'
and `right' marks.

Once the `right' mark passes the last element, it means that all the elements have been processed. The elements
which are greater than the pivot have been moved to the right hand of `left' mark while the others are to the
left hand of this mark. Note that the pivot should move between the two segments. An extra exchanging between the pivot and
the element pointed by `left' mark makes this final one to the correct location. This is shown by the swap
bi-directional arrow in figure \ref{fig:partition-1-way} (c).

The `left' mark (which points the pivot finally) partitions the whole array into two parts, it is
returned as the result. We typically increase the `left' mark by one, so that it points to the
first element greater than the pivot for convenient. Note that the array is modified in-place.

The partition algorithm can be described as the following. It takes three arguments, the array $A$, the lower
and the upper bound to be partitioned \footnote{The partition algorithm used here is slightly different from
the one in \cite{CLRS}. The latter uses the last element in the slice as the pivot.}.

\begin{algorithmic}[1]
\Function{Partition}{A, l, u}
  \State $p \gets A[l]$  \Comment{the pivot}
  \State $L \gets l$ \Comment{the left mark}
  \For{$R \in [l+1, u]$} \Comment{iterate on the right mark}
    \If{$\lnot (p < A[R])$} \Comment{negate of $<$ is enough for strict weak order}
      \State $L \gets L + 1$
      \State \textproc{Exchange} $A[L] \leftrightarrow A[R]$
    \EndIf
  \EndFor
  \State \textproc{Exchange} $A[L] \leftrightarrow p$
  \State \Return $L + 1$ \Comment{The partition position}
\EndFunction
\end{algorithmic}

Below table shows the steps of partitioning the array $\{ 3, 2, 5, 4, 0, 1, 6, 7\}$.

\begin{tabular}{ | c c c c c c c c | l |}
\hline
(l) {\bf 3} & (r) 2 & 5 & 4 & 0 & 1 & 6 & 7 & initialize, $pivot = 3, l = 1, r = 2$ \\
{\bf 3} & (l)(r) 2 & 5 & 4 & 0 & 1 & 6 & 7 & $2 < 3$, advance $l$, ($r=l$)\\
{\bf 3} & (l) 2 & (r) 5 & 4 & 0 & 1 & 6 & 7 & $5 > 3$, move on \\
{\bf 3} & (l) 2 & 5 & (r) 4 & 0 & 1 & 6 & 7 & $4 > 3$, move on \\
{\bf 3} & (l) 2 & 5 & 4 & (r) 0 & 1 & 6 & 7 & $0 < 3$ \\
{\bf 3} & 2 & (l) 0 & 4 & (r) 5 & 1 & 6 & 7 & Advance $l$, then swap with $r$ \\
{\bf 3} & 2 & (l) 0 & 4 & 5 & (r) 1 & 6 & 7 & $1 < 3$ \\
{\bf 3} & 2 & 0 & (l) 1 & 5 & (r) 4 & 6 & 7 & Advance $l$, then swap with $r$ \\
{\bf 3} & 2 & 0 & (l) 1 & 5 & 4 & (r) 6 & 7 & $6 > 3$, move on \\
{\bf 3} & 2 & 0 & (l) 1 & 5 & 4 & 6 & (r) 7 & $7 > 3$, move on \\
1 & 2 & 0 & 3 & (l+1) 5 & 4 & 6 & 7 & $r$ passes the end, swap pivot and $l$ \\
\hline
\end{tabular}

This version of partition algorithm can be implemented in ANSI C as the following.
\lstset{language=C}
\begin{lstlisting}
int partition(Key* xs, int l, int u) {
    int pivot, r;
    for (pivot = l, r = l + 1; r < u; ++r)
        if (!(xs[pivot] < xs[r])) {
            ++l;
            swap(xs[l], xs[r]);
        }
    swap(xs[pivot], xs[l]);
    return l + 1;
}
\end{lstlisting}

Where \texttt{swap(a, b)} can either be defined as function or a macro. In ISO C++, \texttt{swap(a, b)}
is provided as a function template. the type of the elements can be defined somewhere or abstracted
as a template parameter in ISO C++. We omit these language specific details here.

With this partition method realized, the imperative in-place quick sort can be accomplished as the following.

\begin{algorithmic}[1]
\Procedure{Quick-Sort}{$A, l, u$}
  \If{$l < u$}
    \State $m \gets$ \Call{Partition}{$A, l, u$}
    \State \Call{Quick-Sort}{$A, l, m - 1$}
    \State \Call{Quick-Sort}{$A, m, u$}
  \EndIf
\EndProcedure
\end{algorithmic}

When sort an array, this procedure is called by passing the whole range as the lower and upper bounds.
\textproc{Quick-Sort}($A, 1, |A|$). Note that when $l \geq u$ it means the array slice is either empty,
or just contains only one element, both can be treated as ordered, so
the algorithm does nothing in such cases.

Below ANSI C example program completes the basic in-place quick sort.

\lstset{language=C}
\begin{lstlisting}
void quicksort(Key* xs, int l, int u) {
    int m;
    if (l < u) {
        m = partition(xs, l, u);
        quicksort(xs, l, m - 1);
        quicksort(xs, m, u);
    }
}
\end{lstlisting}

\subsection{Minor improvement in functional partition}
\index{Quick Sort!One pass functional partition}
Before exploring how to improve the partition for basic version quick sort, it's obviously that the
one presented so far can be defined by using folding. Please refer to the appendix A of this book for
definition of folding.

\be
partition(p, L) = fold(f(p), (\phi, \phi), L)
\ee

Where function $f$ compares the element to the pivot with predicate $p$ (which is passed to $f$ as a parameter, so that
$f$ is in curried form, see appendix A for detail. Alternatively, $f$ can be a lexical closure which is in
the scope of $partition$, so that it can access the predicate in this scope.),
and update the result pair accordingly.

\be
f(p, x, (A, B)) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\{ x \} \cup A, B) & p(x) \\
  (A, \{ x \} \cup B) & otherwise(\lnot p(x))
  \end{array}
\right.
\ee

Note we actually use pattern-matching style definition. In environment without pattern-matching support,
the pair $(A, B)$ should be represented by a variable, for example $P$, and use access functions
to extract its first and second parts.

The example Haskell program needs to be modified accordingly.

\lstset{language=Haskell}
\begin{lstlisting}
sort [] = []
sort (x:xs) = sort small ++ [x] ++ sort big where
  (small, big) = foldr f ([], []) xs
  f a (as, bs) = if a <= x then (a:as, bs) else (as, a:bs)
\end{lstlisting}

\subsubsection{Accumulated partition}
\index{Quick Sort!Accmulated partition}
The partition algorithm by using folding actually accumulates to the result pair of lists $(A, B)$. That
if the element is not greater than the pivot, it's accumulated to $A$, otherwise to $B$. We can explicitly
express it which save spaces and is friendly for tail-recursive call optimization (refer to the appendix A
of this book for detail).

\be
partition(p, L, A, B) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (A, B) & L = \phi \\
  partition(p, L', \{ l_1 \} \cup A, B) & p(l_1) \\
  partition(p, L', A, \{ l_1 \} \cup B) & otherwise
  \end{array}
\right.
\ee

Where $l_1$ is the first element in $L$ if $L$ isn't empty, and $L'$ contains the rest elements except for
$l_1$, that $L' = \{ l_2, l_3, ...\}$ for example.
The quick sort algorithm then uses this accumulated partition function by passing the $\lambda_x x \leq pivot$
as the partition predicate.

\be
sort(L) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & L = \phi \\
  sort(A) \cup \{ l_1 \} \cup sort(B) & otherwise
  \end{array}
\right.
\ee

Where $A, B$ are computed by the accumulated partition function defined above.

\[
(A, B) = partition(\lambda_x x \leq l_1, L', \phi, \phi)
\]

\subsubsection{Accumulated quick sort}
\index{Quick Sort!Accumulated quick sort}
Observe the recursive case in the last quick sort definition. the list concatenation operations $sort(A) \cup \{l_1\} \cup sort(B)$
actually are proportion to the length of the list to be concatenated. Of course we can use some general solutions
introduced in the appendix A of this book to improve it. Another way is to change the sort algorithm to accumulated
manner. Something like below:

\[
sort'(L, S) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  S & L = \phi \\
  ... & otherwise
  \end{array}
\right.
\]

Where $S$ is the accumulator, and we call this version by passing empty list as the accumulator to start sorting:
$sort(L) = sort'(L, \phi)$.
The key intuitive is that after the partition finishes, the two sub lists need to be recursively sorted. We can
first recursively sort the list contains the elements which are greater than the pivot, then link the pivot in front
of it and use it as an {\em accumulator} for next step sorting.

Based on this idea, the '...' part in above definition can be realized as the following.

\[
sort'(L, S) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  S & L = \phi \\
  sort(A, \{l_1\} \cup sort(B, ?)) & otherwise
  \end{array}
\right.
\]

The problem is what's the accumulator when sorting $B$. There is an important invariant actually, that at
every time, the accumulator $S$ holds the elements have been sorted so far. So that we should sort $B$ by
accumulating to $S$.

\be
sort'(L, S) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  S & L = \phi \\
  sort(A, \{l_1\} \cup sort(B, S)) & otherwise
  \end{array}
\right.
\ee

The following Haskell example program implements the accumulated quick sort algorithm.

\lstset{language=Haskell}
\begin{lstlisting}
asort xs = asort' xs []

asort' [] acc = acc
asort' (x:xs) acc = asort' as (x:asort' bs acc) where
  (as, bs) = part xs [] []
  part [] as bs = (as, bs)
  part (y:ys) as bs | y <= x = part ys (y:as) bs
                    | otherwise = part ys as (y:bs)
\end{lstlisting}

\begin{Exercise}
\begin{itemize}
\item Implement the recursive basic quick sort algorithm in your favorite imperative programming language.
\item Same as the imperative algorithm, one minor improvement is that besides the empty case, we needn't sort the singleton list, implement
this idea in the functional algorithm as well.
\item The accumulated quick sort algorithm developed in this section uses intermediate variable $A, B$. They
can be eliminated by defining the partition function to mutually recursive call the sort function. Implement this
idea in your favorite functional programming language. Please don't refer to the downloadable example program
along with this book before you try it.
\end{itemize}
\end{Exercise}

\section{Performance analysis for quick sort}
\index{Quick Sort!Performance analysis}

Quick sort performs well in practice, however, it's not easy to give theoretical analysis. It needs the tool
of probability to prove the average case performance.

Nevertheless, it's intuitive to calculate the best case and worst case performance. It's obviously that
the best case happens when every partition divides the sequence into two slices with equal size. Thus
it takes $O(\lg n)$ recursive calls as shown in figure \ref{fig:qsort-best}.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.6]{img/qsort-best.ps}
 \caption{In the best case, quick sort divides the sequence into two slices with same length.}
 \label{fig:qsort-best}
\end{figure}

There are total $O(\lg n)$ levels of recursion. In the first level, it executes one partition, which
 processes $n$ elements; In the second level, it executes partition two times, each processes $n/2$
elements, so the total time in the second level bounds to $2 O(n/2) = O(n)$ as well. In the third
level, it executes partition four times, each processes $n/4$ elements. The total time in the third
level is also bound to $O(n)$; ... In the last level, there are $n$ small slices each contains a
single element, the time is bound to $O(n)$. Summing all the time in each level gives the total
performance of quick sort in best case as $O(n \lg n)$.

However, in the worst case, the partition process unluckily divides the sequence to two slices
with unbalanced lengths in most time. That one slices with length $O(1)$, the other is $O(n)$.
Thus the recursive time degrades to $O(n)$. If we draw a similar figure, unlike in the best
case, which forms a balanced binary tree, the worst case degrades into a very unbalanced tree
that every node has only one child, while the other is empty. The binary tree turns to be
a linked list with $O(n)$ length. And in every level, all the elements are processed, so the
total performance in worst case is $O(n^2)$, which is as same poor as insertion sort and
selection sort.

Let's consider when the worst case will happen. One special case is that
all the elements (or most of the elements) are same. Nico Lomuto's partition method deals
with such sequence poor. We'll see how to solve this problem by introducing other
partition algorithm in the next section.

The other two obvious cases which lead to worst case happen when the sequence has already in
ascending or descending order. Partition the ascending sequence makes an empty sub list
before the pivot, while the list after the pivot contains all the rest elements.
Partition the descending sequence gives an opponent result.

There are other cases which lead quick sort performs poor. There is no completely satisfied solution
which can avoid the worst case. We'll see some engineering practice in next section which can
make it very seldom to meet the worst case.

\subsection{Average case analysis $\star$}
\index{Quick Sort!Average case analysis}
In average case, quick sort performs well. There is a vivid example that even the partition
divides the list every time to two lists with length 1 to 9. The performance is still bound
to $O(n \lg n)$ as shown in \cite{CLRS}.

This subsection need some mathematic background, reader can safely skip to next part.

There are two methods to proof the average case performance, one uses an important fact
that the performance is proportion to the total comparing operations during quick sort \cite{CLRS}.
Different with the selections sort that every two elements have been compared. Quick sort
avoid many unnecessary comparisons. For example suppose a partition operation on list
$\{ a_1, a_2, a_3, ..., a_n\}$. Select $a_1$ as the pivot, the partition builds two sub lists
$A = \{x_1, x_2, ..., x_k\}$ and $B = \{ y_1, y_2, ..., y_{n-k-1} \}$.
In the rest time of quick sort, The element in $A$ will never be compared with any elements in $B$.

Denote the final sorted result as $\{ a_1, a_2, ..., a_n \}$,
this indicates that if element $a_i < a_j$, they will not be compared
any longer if and only if some element $a_k$ where $a_i < a_k < a_j$ has ever been selected as pivot
before $a_i$ or $a_j$ being selected as the pivot.

That is to say, the only chance that $a_i$ and $a_j$ being compared is either $a_i$ is chosen
as pivot or $a_j$ is chosen as pivot before any other elements in ordered range
$a_{i+1} < a_{i+2} < ... < a_{j-1}$ are selected.

Let $P(i, j)$ represent the probability that $a_i$ and $a_j$ being compared. We have:

\be
P(i, j) = \frac{2}{j - i + 1}
\ee

Since the total number of compare operation can be given as:

\be
C(n) = \sum_{i=1}^{n-1}\sum_{j=i+1}^{n} P(i, j)
\ee

Note the fact that if we compared $a_i$ and $a_j$, we won't compare $a_j$ and $a_i$ again in
the quick sort algorithm, and we never compare $a_i$ onto itself. That's why we set the upper
bound of $i$ to $n-1$; and lower bound of $j$ to $i+1$.

Substitute the probability, it yields:

\be
\begin{array}{rl}
C(n) & = \displaystyle \sum_{i=1}^{n-1}\sum_{j = i+1}^{n} \frac{2}{j - i + 1} \\
     & = \displaystyle \sum_{i=1}^{n-1}\sum_{k=1}^{n-i} \frac{2}{k+1} \\
\end{array}
\ee

Using the harmonic series \cite{wiki-harmonic}

\[
H_n = 1 + \frac{1}{2} + \frac{1}{3} + .... = \ln n + \gamma + \epsilon_n
\]

\be
C(n) = \sum_{i=1}^{n-1} O(\lg n) = O(n \lg n)
\ee

The other method to prove the average performance is to use the recursive fact that
when sorting list of length $n$, the partition splits the list into two
sub lists with length $i$ and $n-i-1$. The partition process itself takes $cn$
time because it examine every element with the pivot. So we have the following
equation.

\be
T(n) = T(i) + T(n-i-1) + c n
\ee

Where $T(n)$ is the total time when perform quick sort on list of length $n$.
Since $i$ is equally like to be any of $0, 1, ..., n-1$, taking math expectation to
the equation gives:

\be
\renewcommand*{\arraystretch}{1.5}
\begin{array}{rl}
T(n) & = E(T(i)) + E(T(n-i-1)) + c n \\
     & = \displaystyle \frac{1}{n} \sum_{i=0}^{n-1}T(i) + \frac{1}{n} \sum_{i=0}^{n-1}T(n-i-1) + cn \\
     & = \displaystyle \frac{1}{n} \sum_{i=0}^{n-1}T(i) + \frac{1}{n} \sum_{j=0}^{n-1}T(j) + cn \\
     & = \displaystyle \frac{2}{n} \sum_{i=0}^{b-1}T(i) + cn
\end{array}
\ee

Multiply by $n$ to both sides, the equation changes to:

\be
n T(n) = 2 \sum_{i=0}^{n-1} T(i) + c n^2
\label{eq:ntn}
\ee

Substitute $n$ to $n-1$ gives another equation:

\be
(n-1) T(n-1) = 2 \sum_{i=0}^{n-2} T(i) + c (n-1)^2
\label{eq:n1tn1}
\ee

Subtract equation (\ref{eq:ntn}) and (\ref{eq:n1tn1}) can eliminate all the $T(i)$ for $0 \leq i < n-1$.

\be
n T(n) = (n + 1) T(n-1) + 2cn - c
\ee

As we can drop the constant time $c$ in computing performance. The equation can be one more step changed like
below.

\be
\frac{T(n)}{n+1} = \frac{T(n-1)}{n} + \frac{2c}{n+1}
\ee

Next we assign $n$ to $n-1$, $n-2$, ..., which gives us $n-1$ equations.

\[
\frac{T(n-1)}{n} = \frac{T(n-2)}{n-1} + \frac{2c}{n}
\]

\[
\frac{T(n-2)}{n-1} = \frac{T(n-3)}{n-2} + \frac{2c}{n-1}
\]

\[
...
\]

\[
\frac{T(2)}{3} = \frac{T(1)}{2} + \frac{2c}{3}
\]

Sum all them up, and eliminate the same components in both sides, we can deduce to a function of $n$.

\be
\frac{T(n)}{n+1} = \frac{T(1)}{2} + 2c \sum_{k=3}^{n+1} \frac{1}{k}
\ee

Using the harmonic series mentioned above, the final result is:

\be
O(\frac{T(n)}{n+1}) = O(\frac{T(1)}{2} + 2c \ln n + \gamma + \epsilon_n) = O(\lg n)
\ee

Thus

\be
O(T(n)) = O(n \lg n)
\ee

\begin{Exercise}
\begin{itemize}
\item Why Lomuto's methods performs poor when there are many duplicated elements?
\end{itemize}
\end{Exercise}

% ================================================================
% Minor Improvement for quick sort
% ================================================================

\section{Engineering Improvement}
\index{Quick Sort!Engineering improvement}

Quick sort performs well in most cases as mentioned in previous section. However, there
does exist the worst cases which downgrade the performance to quadratic. If the data is randomly
prepared, such case is rare, however, there are some particular sequences which lead to
the worst case and these kinds of sequences are very common in practice.

In this section, some engineering practices are introduces which either help to avoid poor performance
in handling some special input data with improved partition algorithm, or try to uniform
the possibilities among cases.

\subsection{Engineering solution to duplicated elements}
\index{Quick Sort!Handle duplicated elements}
As presented in the exercise of above section, N. Lomuto's partition method isn't good at handling
sequence with many duplicated elements. Consider a sequence with $n$ equal elements like: $\{x, x, ..., x\}$.
There are actually two methods to sort it.

\begin{enumerate}
\item The normal basic quick sort: That we select an arbitrary element, which is $x$ as the pivot, partition
it to two sub sequences, one is $\{x, x, ..., x \}$, which contains $n-1$ elements, the other is empty.
then recursively sort the first one; this is obviously quadratic $O(n^2)$ solution.
\item The other way is to only pick those elements strictly smaller than $x$, and strictly greater than $x$.
Such partition results two empty sub sequences, and $n$ elements equal to the pivot.
Next we recursively sort the sub sequences contains the smaller and the bigger elements, since both of them are empty, the recursive call returns immediately;
The only thing left is to concatenate the sort results in front of and after the list of elements which are equal to the
pivot.
\end{enumerate}

The latter one performs in $O(n)$ time if all elements are equal. This indicates an important improvement
for partition. That instead of binary partition (split to two sub lists and a pivot), ternary partition (split
to three sub lists) handles duplicated elements better.

We can define the ternary quick sort as the following.

\be
sort(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & L = \phi \\
  sort(S) \cup sort(E) \cup sort(G) & otherwise
  \end{array}
\right.
\ee

Where $S, E, G$ are sub lists contains all elements which are less than, equal to, and greater than the pivot
respectively.

\[
\begin{array}{l}
S = \{ x | x \in L, x < l_1 \} \\
E = \{ x | x \in L, x = l_1 \} \\
G = \{ x | x \in L, l_1 < x \}
\end{array}
\]

The basic ternary quick sort can be implemented in Haskell as the following example code.

\lstset{language=Haskell}
\begin{lstlisting}
sort [] = []
sort (x:xs) = sort [a | a<-xs, a<x] ++
              x:[b | b<-xs, b==x] ++ sort [c | c<-xs, c>x]
\end{lstlisting}

Note that the comparison between elements must support abstract `less-than' and
`equal-to' operations. The basic version of ternary sort takes linear $O(n)$ time to
concatenate the three sub lists. It can be improved by using the standard
techniques of accumulator.

Suppose function $sort'(L, A)$ is the accumulated ternary quick sort definition, that $L$ is the sequence
to be sorted, and the accumulator $A$ contains the intermediate sorted result so far.
We initialize the sorting with an empty accumulator: $sort(L) = sort'(L, \phi)$.

It's easy to give the trivial edge cases like below.

\[
sort'(L, A) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  A & L = \phi \\
  ... & otherwise
  \end{array}
\right.
\]

For the recursive case, as the ternary partition splits to three sub lists $S, E, G$, only $S$ and $G$
need recursive sort, $E$ contains all elements equal to the pivot, which is in correct order thus
needn't to be sorted any more. The idea is to sort $G$ with accumulator $A$, then concatenate it behind
$E$, then use this result as the new accumulator, and start to sort $S$:

\be
sort'(L, A) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  A & L = \phi \\
  sort(S, E \cup sort(G, A)) & otherwise
  \end{array}
\right.
\ee

The partition can also be realized with accumulators. It is similar to what has been developed for the
basic version of quick sort. Note that we can't just pass only one predication for pivot comparison.
It actually needs two, one for less-than, the other for equality testing. For the sake of brevity,
we pass the pivot element instead.

\be
partition(p, L, S, E, G) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (S, E, G) & L = \phi \\
  partition(p, L', \{l_1\} \cup S, E, G) & l_1 < p \\
  partition(p, L', S, \{l_1\} \cup E, G) & l_1 = p \\
  partition(p, L', S, E, \{ l_1 \} \cup G) & p < l_1
  \end{array}
\right.
\ee

Where $l_1$ is the first element in $L$ if $L$ isn't empty, and $L'$ contains all rest elements except for $l_1$.
Below Haskell program implements this algorithm. It starts the recursive sorting immediately in the edge
case of parition.

\lstset{language=Haskell}
\begin{lstlisting}
sort xs = sort' xs []

sort' []     r = r
sort' (x:xs) r = part xs [] [x] [] r where
    part [] as bs cs r = sort' as (bs ++ sort' cs r)
    part (x':xs') as bs cs r | x' <  x = part xs' (x':as) bs cs r
                             | x' == x = part xs' as (x':bs) cs r
                             | x' >  x = part xs' as bs (x':cs) r
\end{lstlisting}

Richard Bird developed another version in \cite{fp-pearls}, that instead of concatenating the
recursively sorted results, it uses a list of sorted sub lists, and performs concatenation
finally.

\lstset{language=Haskell}
\begin{lstlisting}
sort xs = concat $ pass xs []

pass [] xss = xss
pass (x:xs) xss = step xs [] [x] [] xss where
    step [] as bs cs xss = pass as (bs:pass cs xss)
    step (x':xs') as bs cs xss | x' <  x = step xs' (x':as) bs cs xss
                               | x' == x = step xs' as (x':bs) cs xss
                               | x' >  x = step xs' as bs (x':cs) xss
\end{lstlisting} %$

\subsubsection{2-way partition}
\index{Quick Sort!2-way partition}
The cases with many duplicated elements can also be handled imperatively. Robert Sedgewick presented a partition
method \cite{qsort-impl}, \cite{pearls}
which holds two pointers. One moves from left to right, the other moves from right to left. The two pointers
are initialized as the left and right boundaries of the array.

When start partition, the left most element is selected as the pivot. Then the left pointer $i$
keeps advancing to right until it meets any element which is not less than the pivot; On the other hand\footnote{We don't use `then' because it's quite OK to perform the two scans in parallel.}, The right pointer $j$ repeatedly scans to
left until it meets any element which is not greater than the pivot.

At this time, all elements before the left pointer $i$ are strictly less than the pivot, while all
elements after the right pointer $j$ are greater than the pivot. $i$ points to an element which is
 either greater than or equal to the pivot; while $j$ points to an element which is either less than or
equal to the pivot, the situation at this stage is illustrated in figure \ref{fig:partition-2-way} (a).

In order to partition all elements less than or equal to the pivot to the left, and the others to the right,
we can exchange the two elements pointed by $i$, and $j$. After that the scan can be resumed. We repeat this process until either $i$ meets $j$, or they overlap.

At any time point during partition. There is invariant that all elements before $i$ (including the one
pointed by $i$) are not greater than
the pivot; while all elements after $j$ (including the one pointed by $j$) are not less than the pivot.
The elements between $i$ and $j$ haven't been examined yet. This invariant is shown in figure \ref{fig:partition-2-way} (b).

\begin{figure}[htbp]
   \centering
   \subfloat[When pointer $i$, and $j$ stop]{\includegraphics[scale=0.5]{img/partition-2-way-inner.ps}} \\
   \subfloat[Partition invariant]{\includegraphics[scale=0.5]{img/partition-2-way.ps}} \\
   \caption{Partition a range of array by using the left most element as the pivot.}
   \label{fig:partition-2-way}
\end{figure}

After the left pointer $i$ meets the right pointer $j$, or they overlap each other, we need one extra exchanging
to move the pivot located at the first position to the correct place which is pointed by $j$. Next, the
elements between the lower bound and $j$ as well as the sub slice between $i$ and the upper bound of the array
are recursively sorted.

This algorithm can be described as the following.

\begin{algorithmic}[1]
\Procedure{Sort}{$A, l, u$} \Comment{sort range $[l, u)$}
  \If{$u - l > 1$} \Comment{More than 1 element for non-trivial case}
    \State $i \gets l$, $j \gets u$
    \State $pivot \gets A[l]$
    \Loop
      \Repeat
        \State $i \gets i + 1$
      \Until{$A[i] \geq pivot$} \Comment{Need handle error case that $i \geq u$ in fact.}
      \Repeat
        \State $j \gets j - 1$
      \Until{$A[j] \leq pivot$} \Comment{Need handle error case that $j < l$ in fact.}
      \If{$j < i$}
        \State break
      \EndIf
      \State \textproc{Exchange} $A[i] \leftrightarrow A[j]$
    \EndLoop
    \State \textproc{Exchange} $A[l] \leftrightarrow A[j]$ \Comment{Move the pivot}
    \State \Call{Sort}{$A, l, j$}
    \State \Call{Sort}{$A, i, u$}
  \EndIf
\EndProcedure
\end{algorithmic}

Consider the extreme case that all elements are equal, this in-place quick sort will partition
the list to two equal length sub lists although it takes $\frac{n}{2}$ unnecessary swaps.
As the partition is balanced, the overall performance is $O(n \lg n)$, which avoid downgrading
to quadratic. The following ANSI C example program implements this algorithm.

\lstset{language=C}
\begin{lstlisting}
void qsort(Key* xs, int l, int u) {
    int i, j, pivot;
    if (l < u - 1) {
        pivot = i = l; j = u;
        while (1) {
            while (i < u && xs[++i] < xs[pivot]);
            while (j >=l && xs[pivot] < xs[--j]);
            if (j < i) break;
            swap(xs[i], xs[j]);
        }
        swap(xs[pivot], xs[j]);
        qsort(xs, l, j);
        qsort(xs, i, u);
    }
}
\end{lstlisting}

Comparing this algorithm with the basic version based on N. Lumoto's partition method, we can find
that it swaps fewer elements, because it skips those have already in proper sides of the pivot.

\subsubsection{3-way partition}
\index{Quick Sort!3-way partition}
It's obviously that, we should avoid those unnecessary swapping for the duplicated elements. What's more,
the algorithm can be developed with the idea of ternary sort (as known as 3-way partition in some
materials), that all the elements which are strictly less than the pivot are put to the left sub slice,
while those are greater than the pivot are put to the right. The middle part holds all the elements
which are equal to the pivot. With such ternary partition, we need only recursively sort the ones
which differ from the pivot. Thus in the above extreme case, there aren't any elements need further
sorting. So the overall performance is linear $O(n)$.

The difficulty is how to do the 3-way partition. Jon Bentley and Douglas McIlroy developed a solution
which keeps those elements equal to the pivot at the left most and right most sides as shown in
figure \ref{fig:partition-3-way} (a) \cite{3-way-part} \cite{opt-qs}.

\begin{figure}[htbp]
   \centering
   \subfloat[Invariant of 3-way partition]{\includegraphics[scale=0.5]{img/partition-3-way.ps}} \\
   \subfloat[Swapping the equal parts to the middle]{\includegraphics[scale=0.5]{img/partition-3-way-end.ps}} \\
   \caption{3-way partition.}
   \label{fig:partition-3-way}
\end{figure}

The majority part of scan process is as same as the one developed by Robert Sedgewick, that
$i$ and $j$ keep advancing toward each other until they meet any element which is greater then or equal
to the pivot for $i$, or less than or equal to the pivot for $j$ respectively.
At this time, if $i$ and $j$ don't meet each other or overlap, they are not only
exchanged, but also examined if the elements pointed by them are identical to the pivot.
Then necessary exchanging happens between $i$ and $p$, as well as $j$ and $q$.

By the end of the partition process, the elements equal to the pivot need to be swapped to the middle
part from the left and right ends. The number of such extra exchanging operations are proportion to the
number of duplicated elements. It's zero operation if elements are unique which there is no overhead
in the case. The final partition result is shown in figure \ref{fig:partition-3-way} (b). After that
we only need recursively sort the `less-than' and `greater-than' sub slices.

This algorithm can be given by modifying the 2-way partition as below.

\begin{algorithmic}[1]
\Procedure{Sort}{$A, l, u$}
  \If{$u - l > 1$}
    \State $i \gets l$, $j \gets u$
    \State $p \gets l$, $q \gets u$ \Comment{points to the boundaries for equal elements}
    \State $pivot \gets A[l]$
    \Loop
      \Repeat
        \State $i \gets i + 1$
      \Until{$A[i] \geq pivot$} \Comment{Skip the error handling for $i \geq u$}
      \Repeat
        \State $j \gets j - 1$
      \Until{$A[j] \leq pivot$} \Comment{Skip the error handling for $j < l$}
      \If{$j \leq i$}
        \State break \Comment{Note the difference form the above algorithm}
      \EndIf
      \State \textproc{Exchange} $A[i] \leftrightarrow A[j]$
      \If{$A[i] = pivot$} \Comment{Handle the equal elements}
        \State $p \gets p + 1$
        \State \textproc{Exchange} $A[p] \leftrightarrow A[i]$
      \EndIf
      \If{$A[j] = pivot$}
        \State $q \gets q - 1$
        \State \textproc{Exchange} $A[q] \leftrightarrow A[j]$
      \EndIf
    \EndLoop
    \If{$i = j \land A[i] = pivot$} \Comment{A special case}
      \State $j \gets j - 1$, $i \gets i + 1$
    \EndIf
    \For{$k$ from $l$ to $p$} \Comment{Swap the equal elements to the middle part}
      \State \textproc{Exchange} $A[k] \leftrightarrow A[j]$
      \State $j \gets j - 1$
    \EndFor
    \For{$k$ from $u-1$ down-to $q$}
      \State \textproc{Exchange} $A[k] \leftrightarrow A[i]$
      \State $i \gets i + 1$
    \EndFor
    \State \Call{Sort}{$A, l, j + 1$}
    \State \Call{Sort}{$A, i, u$}
  \EndIf
\EndProcedure
\end{algorithmic}

This algorithm can be translated to the following ANSI C example program.

\lstset{language=C}
\begin{lstlisting}
void qsort2(Key* xs, int l, int u) {
    int i, j, k, p, q, pivot;
    if (l < u - 1) {
        i = p = l; j = q = u; pivot = xs[l];
        while (1) {
            while (i < u && xs[++i] < pivot);
            while (j >= l && pivot < xs[--j]);
            if (j <= i) break;
            swap(xs[i], xs[j]);
            if (xs[i] == pivot) { ++p; swap(xs[p], xs[i]); }
            if (xs[j] == pivot) { --q; swap(xs[q], xs[j]); }
        }
        if (i == j && xs[i] == pivot) { --j, ++i; }
        for (k = l; k <= p; ++k, --j) swap(xs[k], xs[j]);
        for (k = u-1; k >= q; --k, ++i) swap(xs[k], xs[i]);
        qsort2(xs, l, j + 1);
        qsort2(xs, i, u);
    }
}
\end{lstlisting}

It can be seen that the the algorithm turns to be a bit complex when it evolves to 3-way partition.
There are some tricky edge cases should be handled with caution. Actually, we just need a ternary
partition algorithm. This remind us the N. Lumoto's method, which is straightforward enough
to be a start point.

The idea is to change the invariant a bit. We still select the first element as the pivot,
as shown in figure \ref{fig:partition-3-way-lumoto},
at any time, the left most section contains elements which are strictly less than the pivot;
the next section contains the elements equal to the pivot; the right most section holds all the
elements which are strictly greater than the pivot. The boundaries of three sections are marked
as $i$, $k$, and $j$ respectively.
The rest part, which is between $k$ and $j$ are elements haven't been scanned yet.

At the beginning of this algorithm, the `less-than' section is empty; the `equal-to' section
contains only one element, which is the pivot; so that $i$ is initialized to the lower bound
of the array, and $k$ points to the element next to $i$. The `greater-than' section is also
initialized as empty, thus $j$ is set to the upper bound.

\begin{figure}[htbp]
   \centering
   \includegraphics[scale=0.5]{img/partition-3-way-lumoto.ps}
   \caption{3-way partition based on N. Lumoto's method.}
   \label{fig:partition-3-way-lumoto}
\end{figure}

When the partition process starts, the element pointed by $k$ is examined. If it's equal to
the pivot, $k$ just advances to the next one; If it's greater than the pivot, we swap it with
the last element in the unknown area, so that the length of `greater-than' section increases
by one. It's boundary $j$ moves to the left. Since we don't know if the elements swapped to $k$
is still greater than the pivot, it should be examined again repeatedly. Otherwise, if the
element is less than the pivot, we can exchange it with the first one in the `equal-to' section
to resume the invariant. The partition algorithm stops when $k$ meets $j$.

\begin{algorithmic}[1]
\Procedure{Sort}{$A, l, u$}
  \If{$u - l > 1$}
    \State $i \gets l$, $j \gets u$, $k \gets l + 1$
    \State $pivot \gets A[i]$
    \While{$k < j$}
      \While{$pivot < A[k]$}
        \State $j \gets j - 1$
        \State \textproc{Exchange} $A[k] \leftrightarrow A[j]$
      \EndWhile
      \If{$A[k] < pivot$}
        \State \textproc{Exchange} $A[k] \leftrightarrow A[i]$
        \State $i \gets i + 1$
      \EndIf
      \State $k \gets k + 1$
    \EndWhile
    \State \Call{Sort}{$A, l, i$}
    \State \Call{Sort}{$A, j, u$}
  \EndIf
\EndProcedure
\end{algorithmic}

Compare this one with the previous 3-way partition quick sort algorithm, it's more
simple at the cost of more swapping operations. Below ANSI C program implements this
algorithm.

\lstset{language=C}
\begin{lstlisting}
void qsort(Key* xs, int l, int u) {
    int i, j, k; Key pivot;
    if (l < u - 1) {
        i = l; j = u; pivot = xs[l];
        for (k = l + 1; k < j; ++k) {
            while (pivot < xs[k]) { --j; swap(xs[j], xs[k]); }
            if (xs[k] < pivot) { swap(xs[i], xs[k]); ++i; }
        }
        qsort(xs, l, i);
        qsort(xs, j, u);
    }
}
\end{lstlisting}

\begin{Exercise}
\begin{itemize}
\item All the quick sort imperative algorithms given in this section use the first element as the pivot,
another method is to choose the
last one as the pivot. Realize the quick sort algorithms, including the basic version, Sedgewick version, and
ternary (3-way partition) version by using this approach.
\end{itemize}
\end{Exercise}

\section{Engineering solution to the worst case}
Although the ternary quick sort (3-way partition) solves the issue for duplicated elements, it can't handle
some typical worst cases. For example if many of the elements in the sequence are ordered, no matter it's
in ascending or descending order, the partition result will be two unbalanced sub sequences, one with few elements,
the other contains all the rest.

Consider the two extreme cases, $\{ x_1 < x_2 < ... < x_n\}$ and $\{ y_1 > y_2 > ... > y_n\}$. The partition
results are shown in figure \ref{fig:worst-cases-1}.

\begin{figure}[htbp]
   \centering
   \subfloat[The partition tree for $\{x_1 < x_2 < ... < x_n\}$, There aren't any elements less than or equal to the pivot (the first element) in every partition.]{\hspace{.3\textwidth} \includegraphics[scale=0.5]{img/unbalanced.ps} \hspace{.3\textwidth}} \\
   \subfloat[The partition tree for $\{y_1 > y_2 > ... > y_n\}$, There aren't any elements greater than or equal to the pivot (the first element) in every partition.]{\includegraphics[scale=0.5]{img/unbalanced-2.ps}} \\
   \caption{The two worst cases.}
   \label{fig:worst-cases-1}
\end{figure}

It's easy to give some more worst cases, for example, $\{ x_m, x_{m-1}, ..., x_2, x_1, x_{m+1}, x_{m+2}, ... x_n\}$
where $\{ x_1 < x_2 < ... < x_n \}$;
Another one is $\{x_n, x_1, x_{n-1}, x_2, ... \}$. Their partition result trees are shown
in figure \ref{fig:worst-cases-2}.

\begin{figure}[htbp]
   \centering
   \subfloat[Except for the first partition, all the others are unbalanced.]{\includegraphics[scale=0.4]{img/unbalanced-3.ps}} \\
   \subfloat[A zig-zag partition tree.]{\includegraphics[scale=0.5]{img/unbalanced-zigzag.ps}} \\
   \caption{Another two worst cases.}
   \label{fig:worst-cases-2}
\end{figure}

Observing that the bad partition happens easily when blindly choose the first element as the pivot,
there is a popular work around suggested by Robert Sedgwick in \cite{qsort-impl}. Instead of
selecting the fixed position in the sequence, a small sampling helps to find a pivot which
has lower possibility to cause a bad partition. One option is to examine the first element, the
middle, and the last one, then choose the median of these three element. In the worst case,
it can ensure that there is at least one element in the shorter partitioned sub list.

Note that there is one tricky in real-world implementation. Since the index is typically represented
in limited length words, it may cause overflow when calculating the middle index by
the naive expression \texttt{(l + u) / 2}. In order to avoid this issue, it can be accessed
as \texttt{l + (u - l) / 2}. There are two methods to find the median, one needs at most three
comparisons \cite{3-way-part}; the other is to move the minimum value to the first location, the maximum value
to the last location, and the median value to the meddle location by swapping. After that
we can select the middle as the pivot.
Below algorithm illustrated the second idea before calling the partition procedure.

\begin{algorithmic}[1]
\Procedure{Sort}{$A, l, u$}
  \If{$u - l > 1$}
    \State $m \gets \lfloor \frac{l + u}{2} \rfloor$ \Comment{Need handle overflow error in practice}
    \If{$A[m] < A[l]$} \Comment{Ensure $A[l] \leq A[m]$}
      \State \textproc{Exchange} $A[l] \leftrightarrow A[m]$
    \EndIf
    \If{$A[u-1] < A[l]$} \Comment{Ensure $A[l] \leq A[u-1]$}
      \State \textproc{Exchange} $A[l] \leftrightarrow A[u-1]$
    \EndIf
    \If{$A[u-1] < A[m]$} \Comment{Ensure $A[m] \leq A[u-1]$}
      \State \textproc{Exchange} $A[m] \leftrightarrow A[u-1]$
    \EndIf
    \State \textproc{Exchange} $A[l] \leftrightarrow A[m]$
    \State $(i, j) \gets $ \Call{Partition}{$A, l, u$}
    \State \Call{Sort}{$A, l, i$}
    \State \Call{Sort}{$A, j, u$}
  \EndIf
\EndProcedure
\end{algorithmic}

It's obviously that this algorithm performs well in the 4 special worst cases given above.
The imperative implementation of median-of-three is left as exercise to the reader.

However, in purely functional settings, it's expensive to randomly access the middle and the last
element. We can't directly translate the imperative median selection algorithm. The idea of
taking a small sampling and then finding the median element as pivot can be realized alternatively
by taking the first 3. For example, in the following Haskell program.

\lstset{language=Haskell}
\begin{lstlisting}
qsort [] = []
qsort [x] = [x]
qsort [x, y] = [min x y, max x y]
qsort (x:y:z:rest) = qsort (filter (< m) (s:rest)) ++ [m] ++ qsort (filter (>= m) (l:rest)) where
    xs = [x, y, z]
    [s, m, l] = [minimum xs, median xs, maximum xs]
\end{lstlisting}

Unfortunately, none of the above 4 worst cases can be well handled by this program, this is because
the sampling is not good. We need telescope, but not microscope to profile the whole list to be
partitioned. We'll see the functional way to solve the partition problem later.

Except for the median-of-three, there is another popular engineering practice to get good partition
result. instead of always taking the first element or the last one as the pivot. One alternative is
to randomly select one. For example as the following modification.

\begin{algorithmic}[1]
\Procedure{Sort}{$A, l, u$}
  \If{$u - l > 1$}
    \State \textproc{Exchange} $A[l] \leftrightarrow A[$ \Call{Random}{$l, u$} $]$
    \State $(i, j) \gets $ \Call{Partition}{$A, l, u$}
    \State \Call{Sort}{$A, l, i$}
    \State \Call{Sort}{$A, j, u$}
  \EndIf
\EndProcedure
\end{algorithmic}

The function \textproc{Random}($l, u$) returns a random integer $i$ between $l$ and $u$, that
$l \leq i < u$. The element at this position is exchanged with the first one, so that it is
selected as the pivot for the further partition. This algorithm is called {\em random quick sort} \cite{CLRS}.

Theoretically, neither median-of-three nor random quick sort can avoid the worst case completely.
If the sequence to be sorted is randomly distributed, no matter choosing the first one as the
pivot, or the any other arbitrary one are equally in effect. Considering the underlying data
structure of the sequence is singly linked-list in functional setting, it's expensive to
strictly apply the idea of random quick sort in purely functional approach.

Even with this bad news, the engineering improvement still makes sense in real world programming.

\section{Other engineering practice}
\index{Quick Sort!Insertion sort fall-back}
There is some other engineering practice which doesn't focus on solving the bad partition issue.
Robert Sedgewick observed that when the list to be sorted is short, the overhead introduced by
quick sort is relative expense, on the other hand, the insertion sort performs better in such
case \cite{pearls}, \cite{3-way-part}. Sedgewick, Bentley and McIlroy tried different
threshold, as known as `Cut-Off', that when
there are lesson than `Cut-Off' elements, the sort algorithm falls back to insertion sort.

\begin{algorithmic}[1]
\Procedure{Sort}{$A, l, u$}
  \If{$u - l > $ \textproc{Cut-Off}}
    \State \Call{Quick-Sort}{$A, l, u$}
  \Else
    \State \Call{Insertion-Sort}{$A, l, u$}
  \EndIf
\EndProcedure
\end{algorithmic}

The implementation of this improvement is left as exercise to the reader.

\begin{Exercise}
\begin{itemize}
\item Can you figure out more quick sort worst cases besides the four given in this section?
\item Implement median-of-three method in your favorite imperative programming language.
\item Implement random quick sort in your favorite imperative programming language.
\item Implement the algorithm which falls back to insertion sort when the length of list is
small in both imperative and functional approach.
\end{itemize}
\end{Exercise}

\section{Side words}
It's sometimes called `true quick sort' if the implementation equipped with most of
the engineering practice we introduced, including insertion sort fall-back with cut-off,
in-place exchanging, choose the pivot by median-of-three method, 3-way-partition.

The purely functional one, which express the idea of quick sort perfect can't
take all of them. Thus someone think the functional quick sort is essentially
tree sort.

Actually, quick sort does have close relationship with tree sort. Richard Bird
shows how to derive quick sort from binary tree sort by deforestation \cite{algo-fp}.

Consider a binary search tree creation algorithm called $unfold$. Which turns a
list of elements into a binary search tree.

\be
unfold(L) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & L = \phi \\
  tree(T_l, l_1, T_r) & otherwise
  \end{array}
\right.
\ee

Where

\be
\begin{array}{l}
T_l = unfold( \{ a | a \in L', a \leq l_1\} ) \\
T_r = unfold( \{ a | a \in L', l_1 < a \} )
\end{array}
\ee

The interesting point is that, this algorithm creates tree in a different
way as we introduced in the chapter of binary search tree. If the list to be unfold
is empty, the result is obviously an empty tree. This is the trivial edge case;
Otherwise, the algorithm set the first element $l_1$ in the list as the key of the
node, and recursively creates its left and right children. Where the elements
used to form the left child are those which are less than or equal to the
key in $L'$, while the rest elements which are greater than the key are used to form
the right child.

Remind the algorithm which turns a binary search tree to a list by in-order
traversing:

\be
toList(T) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & T = \phi \\
  toList(left(T)) \cup \{ key(T) \} \cup toList(right(T)) & otherwise
  \end{array}
\right.
\ee

We can define quick sort algorithm by composing these two functions.

\be
quickSort = toList \cdot unfold
\ee

The binary search tree built in the first step of applying $unfold$ is the intermediate
result. This
result is consumed by $toList$ and dropped after the second step. It's quite possible to
eliminate this intermediate result, which leads to the basic version of quick sort.

The elimination of the intermediate binary search tree is called {\em deforestation}.
This concept is based on Burstle-Darlington's work \cite{slpj}.

% ================================================================
% Merge Sort
% ================================================================

\section{Merge sort}
\index{Merge Sort}
Although quick sort performs perfectly in average cases, it can't avoid the worst case no matter what
engineering practice is applied. Merge sort, on the other kind, ensure the performance is bound to
$O(n \lg n)$ in all the cases. It's particularly useful in theoretical algorithm design and analysis.
Another feature is that merge sort is friendly for linked-space settings, which is suitable for
sorting nonconsecutive stored sequences. Some functional programming and dynamic programming environments
adopt merge sort as the standard library sorting solution, such as Haskel, Python and Java (later than
Java 7).

In this section, we'll first brief the intuitive idea of merge sort, provide a basic version.
After that, some variants of merge sort will be given including nature merge sort, and bottom-up
merge sort.

\subsection{Basic version}
\index{Merge Sort!Basic version}
Same as quick sort, the essential idea behind merge sort is also divide and conquer. Different
from quick sort, merge sort enforces the divide to be strictly balanced, that it always splits the
sequence to be sorted at the middle point. After that, it recursively sort the sub sequences
and merge the sorted two sequences to the final result. The algorithm can be described as the
following.

In order to sort a sequence $L$,
\begin{itemize}
\item Trivial edge case: If the sequence to be sorted is empty, the result is obvious empty;
\item Otherwise, split the sequence at the middle position, recursively sort the two sub sequences
and merge the result.
\end{itemize}

The basic merge sort algorithm can be formalized with the following equation.

\be
sort(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & L = \phi \\
  merge(sort(L_1), sort(L_2)) & otherwise, (L_1, L_2) = splitAt(\lfloor \frac{|L|}{2} \rfloor, L)
  \end{array}
\right.
\ee

\subsubsection{Merge}
\index{Merge Sort!Merge}
There are two `black-boxes' in the above merge sort definition, one is the $splitAt$ function,
which splits a list at a given position; the other is the $merge$ function, which can
merge two sorted lists into one.

As presented in the appendix of this book, it's trivial to realize $splitAt$
in imperative settings by using random access. However, in functional settings, it's typically
realized as a linear algorithm:

\be
splitAt(n, L) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\phi, L) & n = 0 \\
  (\{l_1\} \cup A, B) & otherwise, (A, B) = splitAt(n - 1, L')
  \end{array}
\right.
\ee

Where $l_1$ is the first element of $L$, and $L'$ represents the rest elements except of $l_1$ if $L$
isn't empty.

The idea of merge can be illustrated as in figure \ref{fig:merge}. Consider two lines of kids.
The kids have already stood in order of their heights. that the shortest one stands at the
first, then a taller one, the tallest one stands at the end of the line.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.3]{img/merge.eps}
 \caption{Two lines of kids pass a door.}
 \label{fig:merge}
\end{figure}

Now let's ask the kids to pass a door one by one, every time there can be at most one kid
pass the door. The kids must pass this door in the order of their height. The one can't
pass the door before all the kids who are shorter than him/her.

Since the two lines of kids have already been `sorted', the solution is to ask the first
two kids, one from each line, compare their height, and let the shorter kid pass the door;
Then they repeat this step until one line is empty, after that, all the rest kids can
pass the door one by one.

This idea can be formalized in the following equation.

\be
merge(A, B) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  A & B = \phi \\
  B & A = \phi \\
  \{a_1\} \cup merge(A', B) & a_1 \leq b_1 \\
  \{b_1\} \cup merge(A, B') & otherwise
  \end{array}
\right.
\ee

Where $a_1$ and $b_1$ are the first elements in list $A$ and $B$; $A'$ and $B'$ are the
rest elements except for the first ones respectively. The first two cases are trivial
edge cases. That merge one sorted list with an empty list results the same sorted list;
Otherwise, if both lists are non-empty, we take the first elements from the two lists,
compare them, and use the minimum as the first one of the result, then recursively
merge the rest.

With $merge$ defined, the basic version of merge sort can be implemented like the following
Haskell example code.

\lstset{language=Haskell}
\begin{lstlisting}
msort [] = []
msort [x] = [x]
msort xs = merge (msort as) (msort bs) where
  (as, bs) = splitAt (length xs `div` 2) xs

merge xs [] = xs
merge [] ys = ys
merge (x:xs) (y:ys) | x <= y = x : merge xs (y:ys)
                    | x >  y = y : merge (x:xs) ys
\end{lstlisting}

Note that, the implementation differs from the algorithm definition that it treats the singleton
list as trivial edge case as well.

Merge sort can also be realized imperatively. The basic version can be developed as the below algorithm.

\begin{algorithmic}[1]
\Procedure{Sort}{$A$}
  \If{$|A| > 1$}
    \State $m \gets \lfloor \frac{|A|}{2} \rfloor$
    \State $X \gets$ \Call{Copy-Array}{$A[1...m]$}
    \State $Y \gets$ \Call{Copy-Array}{$A[m+1...|A|]$}
    \State \Call{Sort}{$X$}
    \State \Call{Sort}{$Y$}
    \State \Call{Merge}{$A, X, Y$}
  \EndIf
\EndProcedure
\end{algorithmic}

When the array to be sorted contains at least two elements, the non-trivial sorting process starts.
It first copy the first half to a new created array $X$, and the second half to a second new array $Y$.
Recursively sort them; and finally merge the sorted result back to $A$.

This version uses the same amount of extra spaces of $A$. This is because the \textproc{Merge} algorithm
isn't in-place at the moment. We'll introduce the imperative in-place merge sort in later section.

The merge process almost does the same thing as the functional definition. There is a verbose version
and a simplified version by using sentinel.

The verbose merge algorithm continuously checks the element from the two input arrays, picks the smaller one
and puts it back to the result array $A$, it then advances along the arrays respectively until either
one input array is exhausted. After that, the algorithm appends the rest of the elements in the other
input array to $A$.

\begin{algorithmic}[1]
\Procedure{Merge}{$A, X, Y$}
  \State $i \gets 1, j\gets 1, k\gets 1$
  \State $m \gets |X|, n \gets |Y|$
  \While{$i \leq m \land j \leq n$}
    \If{$X[i] < Y[j]$}
      \State $A[k] \gets X[i]$
      \State $i \gets i + 1$
    \Else
      \State $A[k] \gets Y[j]$
      \State $j \gets j + 1$
    \EndIf
    \State $k \gets k + 1$
  \EndWhile
  \While{$i \leq m$}
    \State $A[k] \gets X[i]$
    \State $k \gets k + 1$
    \State $i \gets i + 1$
  \EndWhile
  \While{$j \leq n$}
    \State $A[k] \gets Y[j]$
    \State $k \gets k + 1$
    \State $j \gets j + 1$
  \EndWhile
\EndProcedure
\end{algorithmic}

Although this algorithm is a bit verbose, it can be short in some programming environment with enough tools
to manipulate array. The following Python program is an example.

\lstset{language=Python}
\begin{lstlisting}
def msort(xs):
    n = len(xs)
    if n > 1:
        ys = [x for x in xs[:n/2]]
        zs = [x for x in xs[n/2:]]
        ys = msort(ys)
        zs = msort(zs)
        xs = merge(xs, ys, zs)
    return xs

def merge(xs, ys, zs):
    i = 0
    while ys != [] and zs != []:
        xs[i] = ys.pop(0) if ys[0] < zs[0] else zs.pop(0)
        i = i + 1
    xs[i:] = ys if ys !=[] else zs
    return xs
\end{lstlisting}

\subsubsection{Performance}
\index{Merge Sort!Performance analysis}
Before dive into the improvement of this basic version, let's analyze the performance of merge sort.
The algorithm contains two steps, divide step, and merge step. In divide step, the sequence to be
sorted is always divided into two sub sequences with the same length. If we draw a similar partition tree
as what we did for quick sort, it can be found this tree is a perfectly balanced binary tree as shown in
figure \ref{fig:qsort-best}. Thus the height of this tree is $O(\lg n)$. It means the recursion depth
of merge sort is bound to $O(\lg n)$. Merge happens in every level. It's intuitive to analyze the
merge algorithm, that it compare elements from two input sequences in pairs, after one sequence is fully examined
the rest one is copied one by one to the result, thus it's a linear algorithm proportion to the length of
the sequence. Based on this facts, denote $T(n)$ the time for sorting the sequence with length $n$,
we can write the recursive time cost as below.

\be
\renewcommand*{\arraystretch}{2}
\begin{array}{rl}
T(n) & = \displaystyle T(\frac{n}{2}) + T(\frac{n}{2}) + c n \\
     & = \displaystyle 2 T(\frac{n}{2}) + c n
\end{array}
\ee

It states that the cost consists of three parts: merge sort the first half takes $T(\frac{n}{2})$,
merge sort the second half takes also $T(\frac{n}{2})$, merge the two results takes $c n$, where $c$
is some constant. Solve this equation gives the result as $O(n \lg n)$.

Note that, this performance doesn't vary in all cases, as merge sort always uniformly divides the input.

Another significant performance indicator is space occupation. However, it varies a lot in different
merge sort implementation. The detail space bounds analysis will be explained in every detailed variants
later.

For the basic imperative merge sort, observe that it demands same amount of spaces as the input array
in every recursion, copies the original elements to them for recursive sort, and these spaces can
be released after this level of recursion. So the peak space requirement happens when the recursion
enters to the deepest level, which is $O(n \lg n)$.

The functional merge sort consume much less than this amount, because the underlying data structure
of the sequence is linked-list. Thus it needn't extra spaces for merge\footnote{The complex effects
caused by lazy evaluation is ignored here, please refer to \cite{algo-fp} for detail}.
The only spaces requirement is for book-keeping the stack for recursive calls. This can be
seen in the later explanation of even-odd split algorithm.

\subsubsection{Minor improvement}
\index{Merge Sort!Work area allocation}
We'll next improve the basic merge sort bit by bit for both the functional and imperative realizations.
The first observation is that the imperative merge algorithm is a bit verbose. \cite{CLRS} presents
an elegant simplification by using positive $\infty$ as the sentinel. That we append $\infty$ as
the last element to the both ordered arrays for merging\footnote{For sorting in monotonic non-increasing order,
$-\infty$ can be used instead}. Thus we needn't test which array is not exhausted. Figure \ref{fig:merge-with-sentinel}
illustrates this idea.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.8]{img/merge-with-sentinel.ps}
 \caption{Merge with $\infty$ as sentinels.}
 \label{fig:merge-with-sentinel}
\end{figure}

\begin{algorithmic}[1]
\Procedure{Merge}{$A, X, Y$}
  \State \Call{Append}{$X, \infty$}
  \State \Call{Append}{$Y, \infty$}
  \State $i \gets 1, j\gets 1$
  \For{$k \gets$ from 1 to $|A|$}
    \If{$X[i] < Y[j]$}
      \State $A[k] \gets X[i]$
      \State $i \gets i + 1$
    \Else
      \State $A[k] \gets Y[j]$
      \State $j \gets j + 1$
    \EndIf
  \EndFor
\EndProcedure
\end{algorithmic}

The following ANSI C program imlements this idea. It embeds the merge inside. \texttt{INF} is defined
as a big constant number with the same type of \texttt{Key}. Where the type can either be defined elsewhere
or we can abstract the type information by passing the comparator as parameter. We skip these
implementation and language details here.

\lstset{language=C}
\begin{lstlisting}
void msort(Key* xs, int l, int u) {
    int i, j, m;
    Key *as, *bs;
    if (u - l > 1) {
        m = l + (u - l) / 2;  /* avoid int overflow */
        msort(xs, l, m);
        msort(xs, m, u);
        as = (Key*) malloc(sizeof(Key) * (m - l + 1));
        bs = (Key*) malloc(sizeof(Key) * (u - m + 1));
        memcpy((void*)as, (void*)(xs + l), sizeof(Key) * (m - l));
        memcpy((void*)bs, (void*)(xs + m), sizeof(Key) * (u - m));
        as[m - l] = bs[u - m] = INF;
        for (i = j = 0; l < u; ++l)
            xs[l] = as[i] < bs[j] ? as[i++] : bs[j++];
        free(as);
        free(bs);
    }
}
\end{lstlisting}

Running this program takes much more time than the quick sort. Besides the major reason we'll explain later,
one problem is that this version frequently allocates and releases memories for merging. While memory
allocation is one of the well known bottle-neck in real world as mentioned by Bentley in \cite{pearls}.
One solution to address this issue is to allocate another array with the same size to the original one
as the working area. The recursive sort for the first and second halves needn't allocate any more
extra spaces, but use the working area when merging. Finally, the algorithm copies
the merged result back.

This idea can be expressed as the following modified algorithm.

\begin{algorithmic}[1]
\Procedure{Sort}{A}
  \State $B \gets $ \Call{Create-Array}{$|A|$}
  \State \Call{Sort'}{$A, B, 1, |A|$}
\EndProcedure
\Statex
\Procedure{Sort'}{$A, B, l, u$}
  \If{$u - l > 0$}
    \State $m \gets \lfloor \frac{l + u}{2} \rfloor$
    \State \Call{Sort'}{$A, B, l, m$}
    \State \Call{Sort'}{$A, B, m + 1, u$}
    \State \Call{Merge'}{$A, B, l, m, u$}
  \EndIf
\EndProcedure
\end{algorithmic}

This algorithm duplicates another array, and pass it along with the original array to be sorted
to \textproc{Sort'} algorithm. In real implementation, this working area should be released
either manually, or by some automatic tool such as GC (Garbage collection).
The modified algorithm \textproc{Merge'} also accepts a working area as parameter.

\begin{algorithmic}[1]
\Procedure{Merge'}{$A, B, l, m, u$}
  \State $i \gets l, j \gets m + 1, k \gets l$
  \While{$i \leq m \land j \leq u$}
    \If{$A[i] < A[j]$}
      \State $B[k] \gets A[i]$
      \State $i \gets i + 1$
    \Else
      \State $B[k] \gets A[j]$
      \State $j \gets j + 1$
    \EndIf
    \State $k \gets k + 1$
  \EndWhile
  \While{$i \leq m$}
    \State $B[k] \gets A[i]$
    \State $k \gets k + 1$
    \State $i \gets i + 1$
  \EndWhile
  \While{$j \leq u$}
    \State $B[k] \gets A[j]$
    \State $k \gets k + 1$
    \State $j \gets j + 1$
  \EndWhile
  \For{$i \gets$ from $l$ to $u$} \Comment{Copy back}
    \State $A[i] \gets B[i]$
  \EndFor
\EndProcedure
\end{algorithmic}

By using this minor improvement, the space requirement reduced to $O(n)$ from $O(n \lg n)$.
The following ANSI C program implements this minor improvement. For illustration purpose,
we manually copy the merged result back to the original array in a loop. This can also
be realized by using standard library provided tool, such as \texttt{memcpy}.

\lstset{language=C}
\begin{lstlisting}
void merge(Key* xs, Key* ys, int l, int m, int u) {
    int i, j, k;
    i = k = l; j = m;
    while (i < m && j < u)
        ys[k++] = xs[i] < xs[j] ? xs[i++] : xs[j++];
    while (i < m)
        ys[k++] = xs[i++];
    while (j < u)
        ys[k++] = xs[j++];
    for(; l < u; ++l)
        xs[l] = ys[l];
}

void msort(Key* xs, Key* ys, int l, int u) {
    int m;
    if (u - l > 1) {
        m = l + (u - l) / 2;
        msort(xs, ys, l, m);
        msort(xs, ys, m, u);
        merge(xs, ys, l, m, u);
    }
}

void sort(Key* xs, int l, int u) {
    Key* ys = (Key*) malloc(sizeof(Key) * (u - l));
    kmsort(xs, ys, l, u);
    free(ys);
}
\end{lstlisting}

This new version runs faster than the previous one. In my test machine, it speeds up about 20\% to 25\% when sorting
100,000 randomly generated numbers.

The basic functional merge sort can also be fine tuned. Observe that, it splits the list at the middle point. However,
as the underlying data structure to represent list is singly linked-list, random access at a given position is
a linear operation (refer to appendix A for detail). Alternatively, one can split the list in an even-odd manner.
That all the elements in even position are collected in one sub list, while all the odd elements are collected
in another. As for any list, there are either same amount of elements in even and odd positions, or they
differ by one. So this divide strategy always leads to well splitting, thus the performance can be ensured
to be $O(n \lg n)$ in all cases.

The even-odd splitting algorithm can be defined as below.

\be
split(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\phi, \phi) & L = \phi \\
  (\{ l_1 \}, \phi) & |L| = 1 \\
  (\{ l_1 \} \cup A, \{ l_2 \} \cup B) & otherwise, (A, B) = split(L'')
  \end{array}
\right.
\ee

When the list is empty, the split result are two empty lists; If there is only one element in the list, we put this
single element, which is at position 1, to the odd sub list, the even sub list is empty; Otherwise, it means
there are at least two elements in the list, We pick the first one to the odd sub list, the second one to the
even sub list, and recursively split the rest elements.

All the other functions are kept same, the modified Haskell program is given as the following.

\lstset{language=Haskell}
\begin{lstlisting}
split [] = ([], [])
split [x] = ([x], [])
split (x:y:xs) = (x:xs', y:ys') where (xs', ys') = split xs
\end{lstlisting}

\section{In-place merge sort}
\index{Merge Sort!In-place merge sort}
One drawback for the imperative merge sort is that it requires extra spaces for merging, the basic version without
any optimization needs $O(n \lg n)$ in peak time, and the one by allocating a working area needs $O(n)$.

It's nature for people to seek the in-place version merge sort, which can reuse the original array without allocating
any extra spaces. In this section, we'll introduce some solutions to realize imperative in-place merge sort.

\subsection{Naive in-place merge}
\index{Merge Sort!Naive in-place merge}
The first idea is straightforward. As illustrated in figure \ref{fig:merge-in-place-naive}, sub list $A$, and $B$
are sorted, when performs in-place merge, the invariant ensures that all elements before $i$ are merged, so that
they are in non-decreasing order; every time we compare the $i$-th and the $j$-th elements. If the $i$-th is less
than the $j$-th, the marker $i$ just advances one step to the next. This is the easy case. Otherwise, it
means that the $j$-th element is the next merge result, which should be put in front of $i$. In order
to achieve this, all elements between $i$ and $j$, including the $i$-th should be shift to the end by one cell.
We repeat this process till all the elements in $A$ and $B$ are put to the correct positions.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.8]{img/merge-in-place-naive.ps}
 \caption{Naive in-place merge}
 \label{fig:merge-in-place-naive}
\end{figure}

\begin{algorithmic}[1]
\Procedure{Merge}{$A, l, m, u$}
  \While{$l \leq m \land m \leq u$}
    \If{$A[l] < A[m]$}
      \State $l \gets l + 1$
    \Else
      \State $x \gets A[m]$
      \For{$i \gets m $ down-to $l+1$} \Comment{Shift}
        \State $A[i] \gets A[i-1]$
      \EndFor
      \State $A[l] \gets x$
    \EndIf
  \EndWhile
\EndProcedure
\end{algorithmic}

However, this naive solution downgrades merge sort overall performance to quadratic $O(n^2)$! This is because
that array shifting is a linear operation. It is proportion to the length of elements in
the first sorted sub array which haven't been compared so far.

The following ANSI C program based on this algorithm runs very slow, that it takes about 12 times slower than
the previous version when sorting 10,000 random numbers.

\lstset{language=C}
\begin{lstlisting}
void naive_merge(Key* xs, int l, int m, int u) {
    int i; Key y;
    for(; l < m && m < u; ++l)
        if (!(xs[l] < xs[m])) {
            y = xs[m++];
            for (i = m - 1; i > l; --i) /* shift */
                xs[i] = xs[i-1];
            xs[l] = y;
        }
}

void msort3(Key* xs, int l, int u) {
    int m;
    if (u - l > 1) {
        m = l + (u - l) / 2;
        msort3(xs, l, m);
        msort3(xs, m, u);
        naive_merge(xs, l, m, u);
    }
}
\end{lstlisting}

\subsection{in-place working area}
\index{Merge Sort!In-place working area}
In order to implement the in-place merge sort in $O(n \lg n)$ time, when sorting a sub array, the rest part of
the array must be reused as working area for merging. As the elements stored in the working area, will be sorted
later, they can't be overwritten. We can modify the previous algorithm, which duplicates extra spaces for merging,
a bit to achieve this. The idea is that, every time when we compare the first elements in the two sorted sub
arrays, if we want to put the less element to the target position in the working area, we in-turn exchange what
sored in the working area with this element. Thus after merging, the two sub arrays store what the working area
previously contains. This idea can be illustrated in figure \ref{fig:merge-workarea}.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.8]{img/merge-workarea.ps}
 \caption{Merge without overwriting working area.}
 \label{fig:merge-workarea}
\end{figure}

In our algorithm, both the two sorted sub arrays, and the working area for merging are parts of the
original array to be sorted. we need supply the following arguments when merging: the start points and end
points of the sorted sub arrays, which can be represented as ranges; and the start point of the working
area. The following algorithm for example, uses $[a, b)$ to indicate the range include $a$,
exclude $b$. It merges sorted range $[i, m)$ and range $[j, n)$ to the working area starts from $k$.

\begin{algorithmic}[1]
\Procedure{Merge}{$A, [i, m), [j, n), k$}
  \While{$i < m \land j < n$}
    \If{$A[i] < A[j]$}
      \State \textproc{Exchange} $A[k] \leftrightarrow A[i]$
      \State $i \gets i + 1$
    \Else
      \State \textproc{Exchange} $A[k] \leftrightarrow A[j]$
      \State $j \gets j + 1$
    \EndIf
    \State $k \gets k + 1$
  \EndWhile
  \While{$i < m$}
    \State \textproc{Exchange} $A[k] \leftrightarrow A[i]$
    \State $i \gets i + 1$
    \State $k \gets k + 1$
  \EndWhile
  \While{$j < m$}
    \State \textproc{Exchange} $A[k] \leftrightarrow A[j]$
    \State $j \gets j + 1$
    \State $k \gets k + 1$
  \EndWhile
\EndProcedure
\end{algorithmic}

Note that, the following two constraints must be satisfied when merging:

\begin{enumerate}
\item The working area should be within the bounds of the array. In other words, it should be big
enough to hold elements exchanged in without causing any out-of-bound error;
\item The working area can be overlapped with either of the two sorted arrays, however, it should
be ensured that there are not any unmerged elements being overwritten;
\end{enumerate}

This algorithm can be implemented in ANSI C as the following example.

\lstset{language=C}
\begin{lstlisting}
void wmerge(Key* xs, int i, int m, int j, int n, int w) {
    while (i < m && j < n)
        swap(xs, w++, xs[i] < xs[j] ? i++ : j++);
    while (i < m)
        swap(xs, w++, i++);
    while (j < n)
        swap(xs, w++, j++);
}
\end{lstlisting}

With this merging algorithm defined, it's easy to imagine a solution, which can sort
half of the array; The next question is, how to deal with the rest of the unsorted part
stored in the working area as shown in figure \ref{fig:merge-in-place-start}?

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.8]{img/merge-in-place-start.ps}
 \caption{Half of the array is sorted.}
 \label{fig:merge-in-place-start}
\end{figure}

One intuitive idea is to recursively sort another half of the working area, thus there are
only $\frac{1}{4}$ elements haven't been sorted yet. Which is shown in figure \ref{fig:merge-in-place-quater}.
The key point at this stage is that we must merge the sorted $\frac{1}{4}$ elements $B$
with the sorted $\frac{1}{2}$ elements $A$ sooner or later.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.8]{img/merge-in-place-quater.ps}
 \caption{$A$ and $B$ must be merged at sometime.}
 \label{fig:merge-in-place-quater}
\end{figure}

Is the working area left, which only holds $\frac{1}{4}$ elements, big enough for merging
$A$ and $B$? Unfortunately, it isn't in the settings shown in figure \ref{fig:merge-in-place-quater}.

However, the second constraint mentioned before gives us a hint, that we can exploit
it by arranging the working area to overlap with either sub array if we can ensure
the unmerged elements won't be overwritten under some well designed merging schema.

Actually, instead of making the second half of the working area be sorted, we can make the first
half be sorted, and put the working area between the two sorted arrays as shown in figure
\ref{fig:merge-in-place-setup} (a).
This setup effects arranging the working area to overlap with the sub array $A$. This idea
is proposed in \cite{msort-in-place}.

\begin{figure}[htbp]
 \centering
 \subfloat[]{\includegraphics[scale=0.8]{img/merge-in-place-setup.ps}} \\
 \subfloat[]{\includegraphics[scale=0.8]{img/merge-in-place-merged-quater.ps}}
 \caption{Merge $A$ and $B$ with the working area.}
 \label{fig:merge-in-place-setup}
\end{figure}

Let's consider two extreme cases:

\begin{enumerate}
\item All the elements in $B$ are less than any element in $A$. In this case, the merge algorithm
finally moves the whole contents of $B$ to the working area; the cells of $B$ holds what previously
stored in the working area; As the size of area is as same as $B$, it's OK to exchange their contents;
\item All the elements in $A$ are less than any element in $B$. In this case, the merge algorithm
continuously exchanges elements between $A$ and the working area. After all the previous $\frac{1}{4}$
cells in the working area are filled with elements from $A$, the algorithm starts to overwrite the
first half of $A$. Fortunately, the contents being overwritten are not those unmerged elements.
The working area is in effect advances toward the end of the array, and finally moves to the right
side; From this time point, the merge algorithm starts exchanging contents in $B$ with the working area.
The result is that the working area moves to the left most side which is shown in figure \ref{fig:merge-in-place-setup} (b).
\end{enumerate}

We can repeat this step, that always sort the second half of the unsorted part, and exchange
the sorted sub array to the first half as working area. Thus we keep reducing the working area
from $\frac{1}{2}$ of the array, $\frac{1}{4}$
of the array, $\frac{1}{8}$ of the array, ... The scale of the merge problem keeps reducing.
When there is only one element left in the working area, we needn't sort it any more since
the singleton array is sorted by nature. Merging a singleton array to the other is equivalent
to insert the element. In practice, the algorithm can finalize the last few
elements by switching to insertion sort.

The whole algorithm can be described as the following.

\begin{algorithmic}[1]
\Procedure{Sort}{$A, l, u$}
  \If{$u - l > 0$}
    \State $m \gets \lfloor \frac{l + u}{2} \rfloor$
    \State $w \gets l + u - m$
    \State \Call{Sort'}{$A, l, m, w$} \Comment{The second half contains sorted elements}
    \While{$w - l > 1$}
      \State $u' \gets w$
      \State $w \gets \lceil \frac{l + u'}{2} \rceil$ \Comment{Ensure the working area is big enough}
      \State \Call{Sort'}{$A, w, u', l$} \Comment{The first half holds the sorted elements}
      \State \Call{Merge}{$A, [l, l + u' - w], [u', u], w$}
    \EndWhile
    \For{$i \gets w$ down-to $l$} \Comment{Switch to insertion sort}
      \State $j \gets i$
      \While{$j \leq u \land A[j] < A[j-1]$}
        \State \textproc{Exchange} $A[j] \leftrightarrow A[j-1]$
        \State $j \gets j + 1$
      \EndWhile
    \EndFor
  \EndIf
\EndProcedure
\end{algorithmic}

Note that in order to satisfy the first constraint, we must ensure the working area is big enough to hold
all exchanged in elements, that's way we round it by ceiling when sort the second half of the working area.
Note that we actually pass the ranges including the end points to the algorithm \textproc{Merge}.

Next, we develop a \text{Sort'} algorithm, which mutually recursive call \text{Sort} and exchange the result
to the working area.

\begin{algorithmic}[1]
\Procedure{Sort'}{$A, l, u, w$}
  \If{$u - l > 0$}
    \State $m \gets \lfloor \frac{l + u}{2} \rfloor$
    \State \Call{Sort}{$A, l, m$}
    \State \Call{Sort}{$A, m+1, u$}
    \State \Call{Merge}{$A, [l, m], [m+1, u], w$}
  \Else \Comment{Exchange all elements to the working area}
    \While{$l \leq u$}
      \State \textproc{Exchange} $A[l] \leftrightarrow A[w]$
      \State $l \gets l + 1$
      \State $w \gets w + 1$
    \EndWhile
  \EndIf
\EndProcedure
\end{algorithmic}

Different from the naive in-place sort, this algorithm doesn't shift the array during merging. The main
algorithm reduces the unsorted part in sequence of $\frac{n}{2}, \frac{n}{4}, \frac{n}{8}, ...$, it takes $O(\lg n)$ steps to complete
sorting. In every step, It recursively sorts half of the rest elements, and performs linear time merging.

Denote the time cost of sorting $n$ elements as $T(n)$, we have the following equation.

\be
T(n) = T(\frac{n}{2}) + c \frac{n}{2} + T(\frac{n}{4}) + c \frac{3n}{4} + T(\frac{n}{8}) + c \frac{7n}{8} + ...
\ee

Solving this equation by using telescope method, gets the result $O(n \lg n)$. The detailed process is
left as exercise to the reader.

The following ANSI C code completes the implementation by using the example \texttt{wmerge} program given
above.

\lstset{language=C}
\begin{lstlisting}
void imsort(Key* xs, int l, int u);

void wsort(Key* xs, int l, int u, int w) {
    int m;
    if (u - l > 1) {
        m = l + (u - l) / 2;
        imsort(xs, l, m);
        imsort(xs, m, u);
        wmerge(xs, l, m, m, u, w);
    }
    else
        while (l < u)
            swap(xs, l++, w++);
}

void imsort(Key* xs, int l, int u) {
    int m, n, w;
    if (u - l > 1) {
        m = l + (u - l) / 2;
        w = l + u - m;
        wsort(xs, l, m, w); /* the last half contains sorted elements */
        while (w - l > 2) {
            n = w;
            w = l + (n - l + 1) / 2; /* ceiling */
            wsort(xs, w, n, l);  /* the first half contains sorted elements */
            wmerge(xs, l, l + n - w, n, u, w);
        }
        for (n = w; n > l; --n) /*switch to insertion sort*/
            for (m = n; m < u && xs[m] < xs[m-1]; ++m)
                swap(xs, m, m - 1);
    }
}
\end{lstlisting}

However, this program doesn't run faster than the version we developed in previous section, which doubles
the array in advance as working area. In my machine, it is about 60\% slower when sorting 100,000 random
numbers due to many swap operations.

\subsection{In-place merge sort vs. linked-list merge sort}
\index{Merge Sort!Linked-list merge sort}
The in-place merge sort is still a live area for research. In order to save the extra spaces for merging,
some overhead has be introduced, which increases the complexity of the merge sort algorithm. However, if
the underlying data structure isn't array, but linked-list, merge can be achieved without any extra spaces
as shown in the even-odd functional merge sort algorithm presented in previous section.

In order to make it clearer, we can develop a purely imperative linked-list merge sort solution.
The linked-list can be defined as a record type as shown in appendix A like below.

\lstset{language=C}
\begin{lstlisting}
struct Node {
    Key key;
    struct Node* next;
};
\end{lstlisting}

We can define an auxiliary function for node linking. Assume the list to be linked isn't empty, it
can be implemented as the following.

\lstset{language=C}
\begin{lstlisting}
struct Node* link(struct Node* x, struct Node* ys) {
    x->next = ys;
    return x;
}
\end{lstlisting}

One method to realize the imperative even-odd splitting, is to initialize two empty sub lists.
Then iterate the list to be split. Every time, we link the current node in front of the
first sub list, then exchange the two sub lists. So that, the second sub list will be linked
at the next time iteration. This idea can be illustrated as below.

\begin{algorithmic}[1]
\Function{Split}{$L$}
  \State $(A, B) \gets (\phi, \phi)$
  \While{$L \neq \phi$}
    \State $p \gets L$
    \State $L \gets $ \Call{Next}{$L$}
    \State $A \gets $ \Call{Link}{$p, A$}
    \State \textproc{Exchange} $A \leftrightarrow B$
  \EndWhile
  \State \Return $(A, B)$
\EndFunction
\end{algorithmic}

The following example ANSI C program implements this splitting algorithm embedded.

\lstset{language=C}
\begin{lstlisting}
struct Node* msort(struct Node* xs) {
    struct Node *p, *as, *bs;
    if (!xs || !xs->next) return xs;

    as = bs = NULL;
    while(xs) {
        p = xs;
        xs = xs->next;
        as = link(p, as);
        swap(as, bs);
    }
    as = msort(as);
    bs = msort(bs);
    return merge(as, bs);
}
\end{lstlisting}

The only thing left is to develop the imperative merging algorithm for linked-list. The idea
is quite similar to the array merging version. As long as neither of the sub lists is exhausted,
we pick the less one, and append it to the result list. After that, it just need link the
non-empty one to the tail the result, but not a looping for copying. It needs some carefulness
to initialize the result list, as its head node is the less one among the two sub lists.
One simple method is to use a dummy sentinel head, and drop it before returning. This implementation
detail can be given as the following.

\lstset{language=C}
\begin{lstlisting}
struct Node* merge(struct Node* as, struct Node* bs) {
    struct Node s, *p;
    p = &s;
    while (as && bs) {
        if (as->key < bs->key) {
            link(p, as);
            as = as->next;
        }
        else {
            link(p, bs);
            bs = bs->next;
        }
        p = p->next;
    }
    if (as)
        link(p, as);
    if (bs)
        link(p, bs);
    return s.next;
}
\end{lstlisting}

\begin{Exercise}
\begin{itemize}
\item Proof the performance of in-place merge sort is bound to $O(n \lg n)$.
\end{itemize}
\end{Exercise}

\section{Nature merge sort}
\index{Merge Sort!Nature merge sort}
Knuth gives another way to interpret the idea of divide and conquer merge sort. It just likes
burn a candle in both ends \cite{TAOCP}. This leads to the nature merge sort algorithm.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.3]{img/burn-candle-2-ends.eps}
 \caption{Burn a candle from both ends}
 \label{fig:burn-candle}
\end{figure}

For any given sequence, we can always find a non-decreasing sub sequence starts at any position.
One particular case is that we can find such a sub sequence from the left-most position. The following
table list some examples, the non-decreasing sub sequences are in bold font.

\begin{tabular}{ | l |}
\hline
{\bf 15 } , 0, 4, 3, 5, 2, 7, 1, 12, 14, 13, 8, 9, 6, 10, 11 \\
{\bf 8, 12, 14 }, 0, 1, 4, 11, 2, 3, 5, 9, 13, 10, 6, 15, 7 \\
{\bf 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15 } \\
\hline
\end{tabular}

The first row in the table illustrates the worst case, that the second element is less than the first one,
so the non-decreasing sub sequence is a singleton list, which only contains the first element;
The last row shows the best case, the the sequence is ordered, and the non-decreasing list is the whole;
The second row shows the average case.

Symmetrically, we can always find a non-decreasing sub sequence from the end of the sequence
to the left. This indicates us that we can merge the two non-decreasing sub sequences, one
from the beginning, the other form the ending to a longer sorted sequence. The advantage of
this idea is that, we utilize the nature ordered sub sequences, so that we needn't recursive
sorting at all.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.8]{img/nature-merge-sort.ps}
 \caption{Nature merge sort}
 \label{fig:nature-merge-sort}
\end{figure}

Figure \ref{fig:nature-merge-sort} illustrates this idea. We starts the algorithm by scanning
from both ends, finding the longest non-decreasing sub sequences respectively. After that,
these two sub sequences are merged to the working area. The merged result starts from beginning.
Next we repeat this step, which goes on scanning toward the center of the original sequence.
This time we merge the two ordered sub sequences to the right hand of the working area toward
the left. Such setup is easy for the next round of scanning. When all the elements
in the original sequence have been scanned and merged to the target, we switch to use the
elements stored in the working area for sorting, and use the previous sequence as new working area.
Such switching happens repeatedly in each round. Finally, we copy all elements from the
working area to the original array if necessary.

The only question left is when this algorithm stops. The answer is that when we start
a new round of scanning, and find that the longest non-decreasing sub list spans to the
end, which means the whole list is ordered, the sorting is done.

Because this kind of merge sort proceeds the target sequence in two ways, and uses the
nature ordering of sub sequences, it's named {\em nature two-way merge sort}. In order
to realize it, some carefulness must be paid. Figure \ref{fig:nature-msort-invariant}
shows the invariant during the nature merge sort. At anytime, all elements before marker
$a$ and after marker $d$ have been already scanned and merged. We are trying to
span the non-decreasing sub sequence $[a, b)$ as long as possible, at the same time,
we span the sub sequence from right to left to span $[c, d)$ as long as possible as well.
The invariant for the working area is shown in the second row. All elements before
$f$ and after $r$ have already been processed (Note that they may contain several
ordered sub sequences). For the odd times (1, 3, 5, ...), we merge $[a, b)$ and $[c, d)$
from $f$ toword right; while for the even times (2, 4, 6, ...), we merge the two
sorted sub sequences after $r$ toward left.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.8]{img/nature-msort-invariant.ps}
 \caption{Invariant during nature merge sort}
 \label{fig:nature-msort-invariant}
\end{figure}

For imperative realization, the sequence is represented by array. Before sorting starts,
we duplicate the array to create a working area. The pointers $a, b$ are initialized to
point the left most position, while $c, d$ point to the right most position. Pointer $f$
starts by pointing to the front of the working area, and $r$ points to the rear
position.

\begin{algorithmic}[1]
\Function{Sort}{$A$}
  \If{$|A| > 1$}
    \State $n \gets |A|$
    \State $B \gets$ \Call{Create-Array}{$n$}  \Comment{Create the working area}
    \Loop
      \State $[a, b) \gets [1, 1)$
      \State $[c, d) \gets [n+1, n+1)$
      \State $f \gets 1, r \gets n$ \Comment{front and rear pointers to the working area}
      \State $t \gets $ False \Comment{merge to front or rear}
      \While{$b < c$} \Comment{There are still elements for scan}
        \Repeat \Comment{Span $[a, b)$}
          \State $b \gets b + 1$
        \Until{$b \geq c \lor A[b] < A[b-1]$}

        \Repeat \Comment{Span $[c, d)$}
          \State $c \gets c - 1$
        \Until{$c \leq b \lor A[c-1] < A[c]$}

        \If{$c < b$} \Comment{Avoid overlap}
          \State $c \gets b$
        \EndIf

        \If{$b - a \geq n$} \Comment{Done if $[a, b)$ spans to the whole array}
          \State \Return $A$
        \EndIf

        \If{$t$} \Comment{merge to front}
          \State $f \gets$ \Call{Merge}{$A, [a, b), [c, d), B, f, 1$}
        \Else \Comment{merge to rear}
          \State $r \gets$ \Call{Merge}{$A, [a, b), [c, d), B, r, -1$}
        \EndIf
        \State $a \gets b, d \gets c$
        \State $t \gets \lnot t$ \Comment{Switch the merge direction}
      \EndWhile
      \State \textproc{Exchange} $A \leftrightarrow B$ \Comment{Switch working area}
    \EndLoop
  \EndIf
  \State \Return $A$
\EndFunction
\end{algorithmic}

The merge algorithm is almost as same as before except that we need pass a parameter
to indicate the direction for merging.

\begin{algorithmic}[1]
\Function{Merge}{$A, [a, b), [c, d), B, w, \Delta$}
  \While{$a < b \land c < d$}
    \If{$A[a] < A[d-1]$}
      \State $B[w] \gets A[a]$
      \State $a \gets a + 1$
    \Else
      \State $B[w] \gets A[d-1]$
      \State $d \gets d - 1$
    \EndIf
    \State $w \gets w + \Delta$
  \EndWhile
  \While{$a < b$}
    \State $B[w] \gets A[a]$
    \State $a \gets a + 1$
    \State $w \gets w + \Delta$
  \EndWhile
  \While{$c < d$}
    \State $B[w] \gets A[d-1]$
    \State $d \gets d - 1$
    \State $w \gets w + \Delta$
  \EndWhile
  \State \Return $w$
\EndFunction
\end{algorithmic}

The following ANSI C program implements this two-way nature merge sort algorithm. Note that it
doesn't release the allocated working area explicitly.

\lstset{language=C}
\begin{lstlisting}
int merge(Key* xs, int a, int b, int c, int d, Key* ys, int k, int delta) {
    for(; a < b && c < d; k += delta )
        ys[k] = xs[a] < xs[d-1] ? xs[a++] : xs[--d];
    for(; a < b; k += delta)
        ys[k] = xs[a++];
    for(; c < d; k += delta)
        ys[k] = xs[--d];
    return k;
}

Key* sort(Key* xs, Key* ys, int n) {
    int a, b, c, d, f, r, t;
    if(n < 2)
        return xs;
    for(;;) {
        a = b = 0;
        c = d = n;
        f = 0;
        r = n-1;
        t = 1;
        while(b < c) {
            do {      /* span [a, b) as much as possible */
                ++b;
            } while( b < c && xs[b-1] <= xs[b] );
            do{      /* span [c, d) as much as possible */
                --c;
            } while( b < c && xs[c] <= xs[c-1] );
            if( c < b )
                c = b;   /* eliminate overlap if any */
            if( b - a >= n)
                return xs;          /* sorted */
            if( t )
                f = merge(xs, a, b, c, d, ys, f, 1);
            else
                r = merge(xs, a, b, c, d, ys, r, -1);
            a = b;
            d = c;
            t = !t;
        }
        swap(&xs, &ys);
    }
    return xs; /*can't be here*/
}
\end{lstlisting}

The performance of nature merge sort depends on the actual ordering of the sub arrays. However, it in fact performs
well even in the worst case. Suppose that we are unlucky when scanning the array, that the length of the non-decreasing
sub arrays are always 1 during the first round scan. This leads to the result working area with merged ordered sub
arrays of length 2. Suppose that we are unlucky again in the second round of scan, however, the previous results
ensure that the non-decreasing sub arrays in this round are no shorter than 2, this time, the working area will
be filled with merged ordered sub arrays of length 4, ... Repeat this we get the length of the non-decreasing sub arrays
doubled in every round, so there are at most $O(\lg n)$ rounds, and in every round we scanned all the elements.
The overall performance for this worst case is bound to $O(n \lg n)$. We'll go back to this interesting phenomena
in the next section about bottom-up merge sort.

In purely functional settings however, it's not sensible to scan list from both ends since the underlying data
structure is singly linked-list. The nature merge sort can be realized in another approach.

Observe that the list to be sorted is consist of several non-decreasing sub lists, that we can pick every two
of such sub lists and merge them to a bigger one. We repeatedly pick and merge, so that the number of the
non-decreasing sub lists halves continuously and finally there is only one such list, which is the sorted
result. This idea can be formalized in the following equation.

\be
sort(L) = sort'(group(L))
\ee

Where function $group(L)$ groups the elements in the list into non-decreasing sub lists. This function can be described like
below, the first two are trivial edge cases.

\begin{itemize}
\item If the list is empty, the result is a list contains an empty list;
\item If there is only one element in the list, the result is a list contains a singleton list;
\item Otherwise, The first two elements are compared, if the first one is less than or equal to the second,
it is linked in front of the first sub list of the recursive grouping result; or a singleton list contains
the first element is set as the first sub list before the recursive result.
\end{itemize}

\be
group(L) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \{ L \} & |L| \leq 1 \\
  \{ \{ l_1 \} \cup L_1, L_2, ... \} & l_1 \leq l_2, \{ L_1, L_2, ...\} = group(L') \\
  \{ \{ l_1 \}, L_1, L_2, ... \} & otherwise
  \end{array}
\right.
\ee

It's quite possible to abstract the grouping criteria as a parameter to develop a generic grouping function,
for instance, as the following Haskell code \footnote{There is a `groupBy' function provided in
the Haskell standard library 'Data.List'. However, it doesn't fit here, because it accepts an equality
testing function as parameter, which must satisfy the properties of reflexive, transitive, and
symmetric. but what we use here, the less-than or equal to operation doesn't conform to symetric. Refer
to appendix A of this book for detail.}.

\lstset{language=Haskell}
\begin{lstlisting}
groupBy' :: (a->a->Bool) ->[a] ->[[a]]
groupBy' _ [] = [[]]
groupBy' _ [x] = [[x]]
groupBy' f (x:xs@(x':_)) | f x x' = (x:ys):yss
                         | otherwise = [x]:r
  where
    r@(ys:yss) = groupBy' f xs
\end{lstlisting}

Different from the $sort$ function, which sorts a list of elements, function $sort'$ accepts a list of
sub lists which is the result of grouping.

\be
sort'(\mathbb{L}) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & \mathbb{L} = \phi \\
  L_1 & \mathbb{L} = \{ L_1 \} \\
  sort'(mergePairs(\mathbb{L})) & otherwise
  \end{array}
\right.
\ee

The first two are the trivial edge cases. If the list to be sorted is empty, the result is obviously empty;
If it contains only one sub list, then we are done. We need just extract this single sub list as result;
For the recursive case, we call a function $mergePairs$ to merge every two sub lists, then recursively call
$sort'$.

The next undefined function is $mergePairs$, as the name indicates, it repeatedly merges pairs of non-decreasing
sub lists into bigger ones.

\be
mergePairs(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  L & |L| \leq 1 \\
  \{ merge(L_1, L_2) \} \cup mergePairs(L'') & otherwise
  \end{array}
\right.
\ee

When there are less than two sub lists in the list, we are done; otherwise, we merge the first two sub lists $L_1$ and $L_2$,
and recursively merge the rest of pairs in $L''$. The type of
the result of $mergePairs$ is list of lists, however, it will be flattened by $sort'$ function finally.

The $merge$ function is as same as before. The complete example Haskell program is given as below.

\lstset{language=Haskell}
\begin{lstlisting}
mergesort = sort' . groupBy' (<=)

sort' [] = []
sort' [xs] = xs
sort' xss = sort' (mergePairs xss) where
  mergePairs (xs:ys:xss) = merge xs ys : mergePairs xss
  mergePairs xss = xss
\end{lstlisting}

Alternatively, observing that we can first pick two sub lists, merge them to an intermediate result, then repeatedly
pick next sub list, and merge to this ordered result we've gotten so far until all the rest sub lists are merged.
This is a typical folding algorithm as introduced in appendix A.

\be
sort(L) = fold(merge, \phi, group(L))
\ee

Translate this version to Haskell yields the folding version.

\lstset{language=Haskell}
\begin{lstlisting}
mergesort' = foldl merge [] . groupBy' (<=)
\end{lstlisting}

\begin{Exercise}
\begin{itemize}
  \item Is the nature merge sort algorithm realized by folding is equivalent with the one by using $mergePairs$ in terms
of performance? If yes, prove it; If not, which one is faster?
\end{itemize}
\end{Exercise}

\section{Bottom-up merge sort}
\index{Merge Sort!Bottom-up merge sort}
The worst case analysis for nature merge sort raises an interesting topic, instead of realizing merge sort in
top-down manner, we can develop a bottom-up version. The great advantage is that, we needn't do book keeping
any more, so the algorithm is quite friendly for purely iterative implementation.

The idea of bottom-up merge sort is to turn the sequence to be sorted into $n$ small sub sequences each contains
only one element. Then we merge every two of such small sub sequences, so that we get $\frac{n}{2}$ ordered
sub sequences each with length 2; If $n$ is odd number, we left the last singleton sequence untouched.
We repeatedly merge these pairs, and finally we get the sorted result. Knuth names this variant as
`straight two-way merge sort' \cite{TAOCP}. The bottom-up merge sort is illustrated in figure \ref{fig:bottom-up-msort}

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.6]{img/bottom-up-msort.ps}
 \caption{Bottom-up merge sort}
 \label{fig:bottom-up-msort}
\end{figure}

Different with the basic version and even-odd version, we needn't explicitly split the list to be sorted
in every recursion. The whole list is split into $n$ singletons at the very beginning, and we merge these
sub lists in the rest of the algorithm.

\be
sort(L) = sort'(wraps(L))
\ee

\be
wraps(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & L = \phi \\
  \{ \{l_1\} \} \cup wraps(L') & otherwise
  \end{array}
\right.
\ee

Of course $wraps$ can be implemented by using mapping as introduced in appendix A.

\be
sort(L) = sort'(map(\lambda_x \cdot \{ x \}, L))
\ee

We reuse the function $sort'$ and $mergePairs$ which are defined in section of nature merge sort. They
repeatedly merge pairs of sub lists until there is only one.

Implement this version in Haskell gives the following example code.

\lstset{language=Haskell}
\begin{lstlisting}
sort = sort' . map (\x->[x])
\end{lstlisting}

This version is based on what Okasaki presented in \cite{okasaki-book}. It is quite similar to the nature merge sort
only differs in the way of grouping. Actually, it can be deduced as a special case (the worst case) of
nature merge sort by the following equation.

\be
sort(L)= sort'(groupBy(\lambda_{x, y} \cdot False, L))
\ee

That instead of spanning the non-decreasing sub list as long as possible, the predicate always evaluates to false,
so the sub list spans only one element.

Similar with nature merge sort, bottom-up merge sort can also be defined by folding. The detailed implementation
is left as exercise to the reader.

Observing the bottom-up sort, we can find it's in tail-recursion call manner, thus it's quite easy
to translate into purely iterative algorithm without any recursion.

\begin{algorithmic}[1]
\Function{Sort}{$A$}
  \State $B \gets \phi$
  \For{$\forall a \in A$}
    \State $B \gets$ \Call{Append}{$\{ a \}$}
  \EndFor
  \State $N \gets |B|$
  \While{$N > 1$}
    \For{$i \gets $ from $1$ to $\lfloor \frac{N}{2} \rfloor$}
      \State $B[i] \gets$ \Call{Merge}{$B[2i -1], B[2i]$}
    \EndFor
    \If{\Call{Odd}{$N$}}
      \State $B[\lceil \frac{N}{2} \rceil] \gets B[N]$
    \EndIf
    \State $N \gets \lceil \frac{N}{2} \rceil$
  \EndWhile
  \If{$B = \phi$}
    \State \Return $\phi$
  \EndIf
  \State \Return $B[1]$
\EndFunction
\end{algorithmic}

The following example Python program implements the purely iterative bottom-up merge sort.

\lstset{language=Python}
\begin{lstlisting}
def mergesort(xs):
    ys = [[x] for x in xs]
    while len(ys) > 1:
        ys.append(merge(ys.pop(0), ys.pop(0)))
    return [] if ys == [] else ys.pop()

def merge(xs, ys):
    zs = []
    while xs != [] and ys !=[]:
        zs.append(xs.pop(0) if xs[0] < ys[0] else ys.pop(0))
    return zs + (xs if xs !=[] else ys)
\end{lstlisting}

The Python implementation combines multiple rounds of merging by consuming
the pair of lists on the head, and appending the merged result to the tail. This greatly simply
the logic of handling odd sub lists case as shown in the above pseudo code.

\begin{Exercise}
\begin{itemize}
\item Implement the functional bottom-up merge sort by using folding.
\item Implement the iterative bottom-up merge sort only with array indexing. Don't use any library
supported tools, such as list, vector etc.
\end{itemize}
\end{Exercise}

\section{Parallelism}
\index{Parallel merge sort}
\index{Parallel quick sort}
We mentioned in the basic version of quick sort, that the two sub sequences can be sorted in
parallel after the divide phase finished. This strategy is also applicable for merge sort.
Actually, the parallel version quick sort and morege sort, do not only distribute
the recursive sub sequences sorting into two parallel processes, but divide the sequences into
$p$ sub sequences, where $p$ is the number of processors. Idealy, if we can achieve
sorting in $T'$ time with parallelism, which satisifies $O(n \lg n) = p T'$. We say it
is linear speed up, and the algorithm is parallel optimal.

However, a straightforward parallel extension to the sequential quick sort algorithm
which samples several pivots, divides $p$ sub sequences, and independently
sorts them in parallel, isn't optimal. The bottleneck exists in the
divide phase, which we can only achieve $O(n)$ time in average case.

The straightforward parallel extension to merge sort, on the other hand, block
at the merge phase. Both parallel merge sort and quick sort in practice need
good designs in order to achieve the optimal speed up. Actually, the divide and
conquer nature makes merge sort and quick sort relative easy for parallelisim.
Richard Cole found the $O(\lg n)$ parallel merge sort algorithm with $n$ processors
in 1986 in \cite{para-msort}.

Parallelism is a big and complex
topic which is out of the scope of this elementary book. Readers can refer to
\cite{para-msort} and \cite{para-qsort} for details.

\section{Short summary}
In this chapter, two popular divide and conquer sorting methods, quick sort and merge sort are introduced.
Both of them meet the upper performance limit of the comparison based sorting algorithms $O(n \lg n)$.
Sedgewick said that quick sort is the greatest algorithm invented in the 20th century. Almost
all programming environments adopt quick sort as the default sorting tool. As time goes on,
some environments, especially those manipulate abstract sequence which is dynamic and not based on
pure array switch to merge sort as the general purpose sorting tool\footnote{Actually, most of
them are kind of hybrid sort, balanced with insertion sort to achieve good performance when the
sequence is short}.

The reason for this interesting phenomena can be partly explained by the treatment in this chapter.
That quick sort performs perfectly in most cases, it needs fewer swapping than most other algorithms.
However, the quick sort algorithm is based on swapping, in purely functional settings, swapping isn't
the most efficient way due to the underlying data structure is singly linked-list, but not vectorized
array. Merge sort, on the other hand, is friendly in such environment, as it costs constant spaces,
and the performance can be ensured even in the worst case of quick sort, while the latter downgrade
to quadratic time. However, merge sort doesn't performs as well as quick sort in purely imperative
settings with arrays. It either needs extra spaces for merging, which is sometimes unreasonable, for
example in embedded system with limited memory, or causes many overhead swaps by in-place workaround.
In-place merging is till an active research area.

Although the title of this chapter is `quick sort vs. merge sort', it's not the case that one
algorithm has nothing to do with the other. Quick sort can be viewed as the optimized version of
tree sort as explained in this chapter. Similarly, merge sort can also be deduced from tree sort
as shown in \cite{sort-deriving}.

There are many ways to categorize sorting algorithms, such as in \cite{TAOCP}. One way is to
from the point of view of easy/hard partition, and easy/hard merge \cite{algo-fp}.

Quick sort, for example, is quite easy for merging, because all the elements in the sub
sequence before the pivot are no greater than any one after the pivot. The merging for
quick sort is actually trivial sequence concatenation.

Merge sort, on the other hand, is more complex in merging than quick sort. However, it's
quite easy to divide no matter what concrete divide method is taken:
simple divide at the middle point, even-odd splitting, nature splitting, or bottom-up
straight splitting. Compare to merge sort, it's more difficult for quick sort to
achieve a perfect dividing. We show that in theory, the worst case can't be completely
avoided, no matter what engineering practice is taken, median-of-three, random quick sort,
3-way partition etc.

We've shown some elementary sorting algorithms in this book till this chapter, including
insertion sort, tree sort, selection sort, heap sort, quick sort and merge sort. Sorting
is still a hot research area in computer science. At the time when this chapter is written,
people are challenged by the buzz word `big data', that the traditional convenient method
can't handle more and more huge data within reasonable time and resources.
Sorting a sequence of hundreds of Gigabytes becomes a routine in some fields.

\begin{Exercise}
  \begin{itemize}
    \item Design an algorithm to create binary search tree by using merge sort strategy.
  \end{itemize}
\end{Exercise}

\begin{thebibliography}{99}

\bibitem{TAOCP}
Donald E. Knuth. ``The Art of Computer Programming, Volume 3: Sorting and Searching (2nd Edition)''. Addison-Wesley Professional; 2 edition (May 4, 1998) ISBN-10: 0201896850 ISBN-13: 978-0201896855

\bibitem{CLRS}
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein.
``Introduction to Algorithms, Second Edition''. ISBN:0262032937. The MIT Press. 2001

\bibitem{qsort-impl}
Robert Sedgewick. ``Implementing quick sort programs''. Communication of ACM. Volume 21, Number 10. 1978. pp.847 - 857.

\bibitem{pearls}
Jon Bentley. ``Programming pearls, Second Edition''. Addison-Wesley Professional; 1999. ISBN-13: 978-0201657883

\bibitem{3-way-part}
Jon Bentley, Douglas McIlroy. ``Engineering a sort function''. Software Practice and experience VOL. 23(11), 1249-1265 1993.

\bibitem{opt-qs}
Robert Sedgewick, Jon Bentley. ``Quicksort is optimal''. http://www.cs.princeton.edu/~rs/talks/QuicksortIsOptimal.pdf

\bibitem{fp-pearls}
Richard Bird. ``Pearls of functional algorithm design''. Cambridge University Press. 2010. ISBN, 1139490605, 9781139490603

\bibitem{algo-fp}
Fethi Rabhi, Guy Lapalme. ``Algorithms: a functional programming approach''. Second edition. Addison-Wesley, 1999. ISBN: 0201-59604-0

\bibitem{slpj}
Simon Peyton Jones. ``The Implementation of functional programming languages''. Prentice-Hall International, 1987. ISBN: 0-13-453333-X

\bibitem{msort-in-place}
Jyrki Katajainen, Tomi Pasanen, Jukka Teuhola. ``Practical in-place mergesort''. Nordic Journal of Computing, 1996.

\bibitem{okasaki-book}
Chris Okasaki. ``Purely Functional Data Structures''. Cambridge university press, (July 1, 1999), ISBN-13: 978-0521663502

\bibitem{sort-deriving}
Jos\`{e} Bacelar Almeida and Jorge Sousa Pinto. ``Deriving Sorting Algorithms''. Technical report, Data structures and Algorithms. 2008.

\bibitem{para-msort}
Cole, Richard (August 1988). ``Parallel merge sort''. SIAM J. Comput. 17 (4): 770C785. doi:10.1137/0217049. (August 1988)

\bibitem{para-qsort}
Powers, David M. W. ``Parallelized Quicksort and Radixsort with Optimal Speedup'', Proceedings of International Conference on Parallel Computing Technologies. Novosibirsk. 1991.

\bibitem{wiki-qs}
Wikipedia. ``Quicksort''. http://en.wikipedia.org/wiki/Quicksort

\bibitem{wiki-sweak-order}
Wikipedia. ``Strict weak order''. http://en.wikipedia.org/wiki/Strict\_weak\_order

\bibitem{wiki-total-order}
Wikipedia. ``Total order''. http://en.wokipedia.org/wiki/Total\_order

\bibitem{wiki-harmonic}
Wikipedia. ``Harmonic series (mathematics)''. http://en.wikipedia.org/wiki/Harmonic\_series\_(mathematics)

\end{thebibliography}

\ifx\wholebook\relax\else
\end{document}
\fi

% LocalWords:  ZF Lumoto's Burstle Okasaki
