\ifx\wholebook\relax \else
% ------------------------ 

\documentclass{article}
%------------------- Other types of document example ------------------------
%
%\documentclass[twocolumn]{IEEEtran-new}
%\documentclass[12pt,twoside,draft]{IEEEtran}
%\documentstyle[9pt,twocolumn,technote,twoside]{IEEEtran}
%
%-----------------------------------------------------------------------------
%\input{../common.tex}
\input{../common-en.tex}

\setcounter{page}{1}

\begin{document}

\fi
%--------------------------

% ================================================================
%                 COVER PAGE
% ================================================================

\title{Searching}

\author{Larry~LIU~Xinyu
\thanks{{\bfseries Larry LIU Xinyu } \newline
  Email: liuxinyu95@gmail.com \newline}
  }

\markboth{Searching}{AlgoXY}

\maketitle

\ifx\wholebook\relax
\chapter{Searching}
\numberwithin{Exercise}{chapter}
\fi

% ================================================================
%                 Introduction
% ================================================================
\section{Introduction}
\label{introduction} 
Searching is a quite big and important area. Computer turns many hard
searching problem realistic, which is almost impossible for human begins.
A modern industry robot can even search and pick the correct gadget from
the pipeline for assembly; A GPS car navigator can search among the
map, for the best route to a specific place. The modern mobile phone
does not only equipped with such map navigator, but can also search
for internet shopping with the best price.

This chapter just scratch the surface of elementary searching. One
good thing that computer offers is the brute-force scanning for a
certain result in a large sequences. The divide and conquers
search strategy will be briefed with two problems, one is to find
the $k$-th big one among a list of unsorted elements; the other
is the popular binary search among a list of sorted elements.
We'll also introduce the extension of binary search for searching
among multiple-dimension data.

Text matching is also very important in our daily life, two well-known
searching algorithms, Knuth-Morris-Pratt (KMP) and Boyer-Moore algorithm
will be introduced, They set good example for another searching strategy:
information reusing.

Besides sequence search, some elementary methods for searching
solution of some interesting problems will be introduced. They
were mostly well studied in the early phase of AI (artificial
intelligence), including the basic DFS (Depth first search), 
and BFS (Breadth first search).

Finally, Dynamic programming will be briefed for searching
optimal solutions, and we'll also introduce about greedy
algorithm which is applicable for some special cases.

All algorithms will be realized in both imperative and functional
approaches.

% ================================================================
% Sequence search
% ================================================================
\section{Sequence search}
Although modern computer offers fast speed for brute-force searching,
and even if the Moore's law could be strictly followed, the grows of
huge data is too fast to be handled well in this way. We've seen
a vivid example in the introduction chapter of this book.
It's why people study the computer search algorithms. 

\subsection{Divide and conquer search}
One solution is to use divide and conquer approach. That if we can
repeatedly scale down the search domain, the data being dropped needn't
be examined at all. This will definately speed up the search.

\subsubsection{$k$-selection problem}
\index{Selection algorithm}
Consider a problem of finding the $k$-th smallest element among $n$ data.
The most straightforward idea is to find the minimum one first, then
drop it and find the second minimum element among the rest. Repeat
this minimum finding and dropping $k$ steps will find the
$k$-th smallest one. Finding the minimum element among $n$ data
costs linear $O(n)$ time. Thus this method performs $O(kn)$ time,
if $k$ is much smaller than $n$.

Another method is to use the `heap' data structure we've introduced.
No matter what concreate heap is used, binary heap with implicit array,
Fibonacci heap or others, a top element accessing followed by a 
popping is typically bound $O(lg n)$ time. Thus this method, as
formalized in euqation (\ref{eq:kth-heap1}) and (\ref{eq:kth-heap2}) performs in $O(k \lg n)$ time, if
$k$ is much smaller than $n$.
    
\be
top(k, L) = find(k, heapify(L))
\label{eq:kth-heap1}
\ee

\be
find(k, H) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  top(H) & k = 0 \\
  find(k-1, pop(H)) & otherwise
  \end{array}
\right.
\label{eq:kth-heap2}
\ee

However, heap adds some complexity to the solution. Is there
any simple, fast method to find the $k$-th element?

The divide and conquer strategy can help us. If we can divide all the
elements into two sub lists $A$ and $B$, and ensure all the elements in 
$A$ is not greater than any elements in $B$, we can scale down the
problem by following this method\footnote{This actually demands a more accurate
definition of the $k$-th smallest in $L$: It's equal to the $k$-the element
of $L'$, where $L'$ is a permutation of $L$, and $L'$ is in monotonic non-decreasing order.}:

\begin{enumerate}
\item Compare the length of sub list $A$ and $k$;
\item If $k < |A|$, the $k$-th smallest one must contained in $A$, we can drop $B$ and {\em further search} in $A$;
\item If $|A| < k$, the $k$-th smallest one must contained in $B$, we can drop $A$ and {\em further serach} the $(k-|A|)$-th
smallest one in $B$.
\end{enumerate}

Note that the {\em italic font} emphasizes the fact of recursion. The ideal case always divide
the list into two equaly big sub lists $A$ and $B$, so that we can halve the problem each time.
Such ideal case leads to a performance of $O(n)$ linear time. 

Thus the key problem is how to realize dividing, which collect the first $m$ smallest elements in one sub list,
and put the rest in another.

This reminds us the partition algorithm in quick sort, which moves all the elements smaller than the
pivot infront of it, and moves those greater than the pivot after it. Based on this idea, we can
refine a divide and conquer $k$-selection algorithm, which is called quick selection algorithm.

\begin{enumerate}
\item Randomly select an element (the first for instance) as the pivot;
\item Moves all elements which aren't greater than the pivot in a sub list $A$; and moves the rest to sub list $B$;
\item Compare the length of $A$ with $k$, if $|A| = k - 1$, then the pivot is the $k$-th smallest one;
\item If $|A| > k - 1$, recusively find the $k$-th smallest one among $A$;
\item Otherwise, recursively find the $(k - |A|)$-th smallest one among $B$;
\end{enumerate}

This algorithm can be formalized in below equation. Suppose $0 < k \leq |L|$ , where $L$ is a non-empty list of elements.
Denote $l_1$ as the first element in $L$. It is chosen as the pivot; $L'$ contains the rest elements except for $l_1$.
$(A, B) = partition(\lambda_x \cdot x \leq l_1, L')$ partitions $L'$ by using the same algorithm defined
in the chapter of quick sort.

\be
top(k, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  l_1 & |A| = k - 1 \\
  top(k - 1 - |A|, B) & |A| < k - 1 \\
  top(k, A) & otherwise
  \end{array}
\right.  
\ee

\be
partition(p, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\Phi, \Phi) & L = \Phi \\
  (\{ l_1 \} \cup A, B) & p(l_1), (A, B) = partition(p, L') \\
  (A, \{ l_1 \} \cup B) & \lnot p(l_1)
  \end{array}
\right.  
\ee

The following Haskell example program implements this algorithm.

\lstset{language=Haskell}
\begin{lstlisting}
top n (x:xs) | len == n - 1 = x
             | len < n - 1 = top (n - len - 1) bs
             | otherwise = top n as
    where                           
      (as, bs) = partition (<=x) xs
      len = length as
\end{lstlisting}

The partition function is provided in Haskell standard library, the detailed implementation
can be referred to previous chapter about quick sort.

The lucky case is that, the $k$-th smallest element is selected as the pivot at be very beginning.
The partition function examines the whole list, and finds that there are $k-1$ elements not greater
than the pivot, we are done after $O(n)$ time. The worst case is that either the maximum or the
minimum elements are selected as the pivot every time. The partition always produces an empty
sub list, that either $A$ or $B$ is empty. If we always pick the minimum as the pivot, the
performance is bound to $O(kn)$. If we always pick the maximum as the pivot, the performance
is $O((n-k)n)$. If $k$ is much less than $n$, it downgrads to quadratic $O(n^2)$ time.

The best case (not the lucky case), is that the pivot always partition the list perfectly.
The length of $A$ is nearly same as the length of $B$. In such case, the list is halved
every time. It needs about $O(\lg n)$ partitions, each partition takes linear time proportion
to the length of the halved list. This can be expressed as 
$O(n + \frac{n}{2} + \frac{n}{4} + ... + \frac{n}{2^m})$, where $m$ is the smallest number satisfies
$\frac{n}{2^m} < k$. Summing the series leads to the result of $O(n)$.

The average case analysis needs tool of mathematical expectation. It's quite similiar to the
proof given in previous chapter of quick sort. It's left as an exercise to the reader.

Similar as quick sort, this divide and conquer selection algorithm performs well most time
in practice. We can take the same engineering practice such as media-of-three, or randomly
pivoting as we did for quick sort. Below is the imperative realization for example.

\begin{algorithmic}[1]
\Function{Top}{$k, A, l, u$}
  \State \textproc{Exchange} $A[l] \leftrightarrow A[$ \Call{Random}{$l, u$} $]$ \Comment{Randomly select in $[l, u]$}
  \State $p \gets$ \Call{Partition}{$A, l, u$}
  \If{$p - l + 1 = k$}
    \State \Return $A[p]$
  \EndIf
  \If{$k < p - l + 1$}
    \State \Return \Call{Top}{$k, A, l, p-1$}
  \EndIf
  \State \Return \Call{Top}{$k - p + l - 1, A, p + 1, u$}
\EndFunction
\end{algorithmic}

This algorithm searches the $k$-th smallest element in range of $[l, u]$ for array $A$. The boundaries
are included. It first randomly selects a position, and swaps it with the first one. Then this element
is chosen as the pivot for partitioning. The partition algorithm in-place moves elements and 
returns the position where the pivot being moved. If the pivot is just located at the $k$-th, then
we are done; if there are more than $k-1$ elements not greater than the pivot, the algorithm 
recursively search the $k$-th smallest one in range $[l, p-1]$; otherwise, $k$ is deduced by the
number of elements before the pivot, and recusively search the range after the pivot $[p+1, u]$.

There are many methods to realize the partition algorithm, below one is based on N. Lumoto's method.
Other realizations are left as exercises to the reader.

\begin{algorithmic}[1]
\Function{Partition}{A, l, u}
  \State $p \gets A[l]$ 
  \State $L \gets l$ 
  \For{$R \in [l+1, u]$} 
    \If{$\lnot (p < A[R])$} 
      \State $L \gets L + 1$
      \State \textproc{Exchange} $A[L] \leftrightarrow A[R]$
    \EndIf
  \EndFor
  \State \textproc{Exchange} $A[L] \leftrightarrow p$
  \State \Return $L$
\EndFunction
\end{algorithmic}

Below ANSI example program implements this algorithm. Note that it handles the special case that
either the array is empty, or $k$ is out of the boundaries of the array. It returns -1 to indicate
the search failure.

\lstset{language=C}
\begin{lstlisting}
int partition(Key* xs, int l, int u) {
    int r, p = l;
    for (r = l + 1; r < u; ++r)
        if (!(xs[p] < xs[r]))
            swap(xs, ++l, r);
    swap(xs, p, l);
    return l;
}

/* The result is stored in xs[k], returns k if u-l >=k, otherwise -1 */
int top(int k, Key* xs, int l, int u) {
    int p;
    if (l < u) {
        swap(xs, l, rand() % (u - l) + l);
        p = partition(xs, l, u);
        if (p - l + 1 == k) 
            return p;
        return (k < p - l + 1) ? top(k, xs, l, p) : 
                                 top(k- p + l - 1, xs, p + 1, u);
    }
    return -1;
}
\end{lstlisting}

There is a method proposed by Blum, Floyd, Pratt, Rivest and Tarjan in 1973, which ensure the worst case performance
being bound to $O(n)$ \cite{CLRS}, \cite{median-of-median}. It divides the list into small groups. Each group cnotains
no more than 5 elements. The median of each group among these 5 elements are identified quickly. Then there are $\frac{n}{5}$
median elements selected. We repeat this step, and divide them again into groups of 5, and recursively select the
{\em median of meidan}. It's obviously that the final `true' median can be found in $O(\lg n)$ time. This is the 
best pivot for partitioning the list. Next, we halve the list by this pivot and recursively seach for the $k$-th
smallest one. The performance can be calculated as the following.

\be
T(n) = c_1 lg n + c_2 n + T(\frac{n}{2})
\ee

Where $c_1$ and $c_2$ are constant factors for the median of median and parition computation respectively. Solve this
equation by using telescope method or the master theory in \cite{CLRS} gives the linear $O(n)$ performance. The detailed
algorithm realization is left as exercise to the reader.

In case we just want to pick the top $k$ smallest elements, but don't care about the order of them, the algorithm can
be adjusted a little bit to fit.

\be
tops(k, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & k = 0 \lor L = \Phi \\
  A & |A| = k \\
  A \cup \{ l_1 \} \cup tops(k - |A| - 1, B) & |A| < k \\
  tops(k, A) & otherwise
  \end{array}
\right.  
\ee

Where $A$, $B$ has the same meaning as before that, $(A, B) = partition(\lambda_x \cdot x \leq l_1, L')$ if $L$ isn't
empty. The relative Haskell program is given as below.

\lstset{language=Haskell}
\begin{lstlisting}
tops _ [] = []
tops 0 _  = []
tops n (x:xs) | len ==n = as
              | len < n  = as ++ [x] ++ tops (n-len-1) bs
              | otherwise = tops n as
    where
      (as, bs) = partition (<= x) xs
      len = length as
\end{lstlisting}

\subsubsection{binary search}
Another popular devide and conquer algorithm is binary search. We've shown it in the chapter about insertion sort.
When I was in school, the teacher who taught math played a magic to me, He asked me to consider a natural number
less than 1000. Then he asked me 10 questions, I only saied `yes' or `no', and finally he guessed my number.
He typically asks questions like the following:

\begin{itemize}
\item Is it an even number?
\item Is it a prime number?
\item Are all digits same?
\item Can it be divided by 3?
\item ...
\end{itemize}

Most of the time he guessed the number within 10 questions. My classmates and I all thought it's unbelievable.

This game will not be so interesting if it downgrades to a popular TV program, that the price of a product
is hidden, and you must figure out the exact price in 30 seconds. The host of the program tells you if
your guess is higher or lower to the fact. If you win, the product is yours. The best strategy is
to use similar divide and conquer approach to perfrom a binary search. So it's common to find such
conversation between the player and the host:

\begin{itemize}
\item P: 1000;
\item H: High;
\item P: 500;
\item H: Low;
\item P: 750;
\item H: Low;
\item P: 890;
\item H: Low;
\item P: 990;
\item H: Bingo.
\end{itemize}

My math teacher told us that, because the number we considered is within 1000, if he can halve the
numbers every time by designing good questions, the number will be found in 10 questions. This is because
$2^{10} = 1024 > 1000$. However, it would be boring to just ask is it higher than 500, is it lower
than 250, ... Actually, the question `is it even' is very good, because it always
halve the number.

Come back to the binary search algorithm, it is only applicable to a sequence of orderred number.
I've seen programmers tried to apply it to unsorted arrays, and took several hours to figure out
why it doesn't work. The idea is quite straightforward, in order to find a number $x$ in a
orderred sequence $A$, we first check middle point number, compare it with $x$, if
they are same, then we are done; If $x$ is smaller, as $A$ is orderred, we need only
recursively search it among the first half; or we search it among the second half.
Once $A$ gets empty and we haven't found $x$ yet, it means $x$ doesn't exsit.

Before formalizing this algorithm. There is a suprising fact need to be noted. Donald Knuth stated that
`Although the basic idea of binary search is comparatively straightforward, 
the details can be surprisingly tricky¡­'. Jon Bentley pointed out that most binary search implementation
contains errors, and even the version given by him in the first version of `Programming pearls' contains
an error undetected over twenty years \cite{programming-pearls}.

There are two kinds of realization, one is recursive, the other is iterative. The recursive solution
is as same as what we described. Suppose the lower and upper bounderies of the array 
are $l$ and $u$ inclusive.

\begin{algorithmic}[1]
\Function{Binary-Search}{$x, A, l, u$}
  \If{$u < l$}
    \State Not found error
  \Else
     \State $m \gets l + \lfloor \frac{u - l}{2} \rfloor$ \Comment{avoid overflow of $\lfloor \frac{l+u}{2} \rfloor$}
     \If{$A[m] = x$}
       \State \Return $m$
     \EndIf
     \If{$x < A[m]$}
       \State \Return \Call{Binary-Search}{x, A, l, m - 1}
     \Else
       \State \Return \Call{Binary-Search}{x, A, m + 1, u}
     \EndIf
  \EndIf
\EndFunction
\end{algorithmic}

As the comment highlights, if the integer is presented with limited words, we can't merely use $\lfloor \frac{l+u}{2} \rfloor$
because it may cause overflow if $l$ and $u$ are big.

Binary search can also be realized in iterative manner, that we keep updating the boundaries according to the middle point
comparison result.

\begin{algorithmic}[1]
\Function{Binary-Search}{$x, A, l, u$}
  \While{$l < u$}
    \State $m \gets l + \lfloor \frac{u - l}{2} \rfloor$
    \If{$A[m] = x$}
      \State \Return $m$
    \EndIf
    \If{$x < A[m]$}
      \State $u \gets m - 1$
    \Else
      \State $l \gets m + 1$
    \EndIf
  \EndWhile
\EndFunction
\end{algorithmic}

The implementation is very good exercise, we left it to the reader. Please try all kinds of methods to verify your program.

Since the array is halved every time, the performance of binary search is bound to $O(\lg n)$ time.

In purely functional settings, the list is represented with singly linked-list. It's linear time to randomly access the
element for a given position. Binary search doesn't make sense in such case. However, it good to analysis what the perfromance
will downgrade to. Consider the following equation.

\[
bsearch(x, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  Err & L = \Phi \\
  b_1 & x = b_1, (A, B) = splitAt(\lfloor \frac{|L|}{2} \rfloor, L) \\
  bsearch(x, A) & B = \Phi \lor x < b_1 \\
  bsearch(x, B') & otherwise
  \end{array}
\right.
\]

The $splitAt$ function takes $O(n)$ time to divide the list into two subs $A$ and $B$
(see the appendix A, and the chapter about merge sort for detail).
If $B$ isn't empty, denote the first element in it is $b_1$, and the rest as $B'$.
If $x$ is equal to $b_1$, the search returns; Otherwise if it is less than $b_1$, as
the list is sorted, we need recursively search in $A$, otherwise, we search in $B$.
If the list is empty, we raise error to indicate search failure.

As we always split the list in the middle point, the number of elements halves in each
recursion. In every recursive call, we takes linear time for splitting. The splitting
function only traverses the first half of the linked-list, Thus the
total time can be expressed as.

\[
T(n) = c \frac{n}{2} + c \frac{n}{4} + c \frac{n}{8} + ...
\]

This results $O(n)$ time, which is as same as the brute force search from head to tail:

\[
search(x, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  Err & L = \Phi \\
  l_1 & x = l_1 \\
  search(x, L') & otherwise
  \end{array}
\right.
\]

As we mentioned in the chapter about insertion sort, the functional approach of binary search
is through binary search tree. The ordered sequence is represented in a binary search tree (
self balanced tree if neccessary), which offers logarithm time search \footnote{Some readers
may argue that array should be used instead of linked-list, for example in Haskell. This book
only deals with purely functional sequences in finger-tree. Different from the 
Haskell array, it can't support constant time random accessing}.

\subsubsection{2 demension bineary search? Dijkstra}

\begin{Exercise}
\begin{itemize}
\item Prove that the average case of the divide and conquer solution to $k$-selection problem is $O(n)$. Please refer to previous chapter about quick sort.
\item Implement the imperative $k$-selection problem with 2-way partition, and median-of-three pivot selection.
\item Implement the imperative $k$-selection problem to handle duplicated elements effectively.
\item Realize the median-of-median $k$-selection algorithm and implement it in your favorite programming language.
\item The $tops(k, L)$ algorithm uses list concatenation likes $A \cup \{ l_1 \} \cup tops(k - |A| - 1, B)$. It is linear operation which is proportion to the length of the list to be concatenated. Modify the algorithm so that the sub lists are concatenated by one pass.
\item The author considered another divide and conquer solution for the $k$-selection problem. It finds the maximum of the
first $k$ elements and the minimum of the rest. Denote them as $x$, and $y$. If $x$ is smaller than $y$, it means that 
all the first $k$ elements are smaller than the rest, so that they are exactly the top $k$ smallest; Otherwise, There
some elements in the first $k$ should be moved.
\begin{algorithmic}
\Procedure{Tops}{$k, A$}
  \State $l \gets 1$
  \State $u \gets |A|$
  \Loop
    \State $i \gets$ \Call{Max-At}{$A[l..k]$}
    \State $j \gets$ \Call{Min-At}{$A[k+1..u]$}
    \If{$A[i] < A[j]$}
      \State break
    \EndIf
    \State \textproc{Exchange} $A[l] \leftrightarrow A[j]$
    \State \textproc{Exchange} $A[k+1] \leftrightarrow A[i]$
    \State $l \gets$ \Call{Partition}{$A, l, k$}
    \State $u \gets$ \Call{Partition}{$A, k+1, u$}
  \EndLoop
\EndProcedure
\end{algorithmic}
\item Implement the binary search algorithm in both recursive and iterative manner, and try to verify your version
automatically. You can generate randomized data, test your program with the binary search invariant and against the
one provided in your standard library.
\end{itemize}
\end{Exercise}

Explain why this algorithm works? What's the performance of it?

\subsection{Information reuse}

\subsubsection{KMP}

\subsubsection{Boyer-moore}

\section{Solution searching}
\subsection{DFS and BFS}

\subsection{Search the optimal solution}

\subsection{Dynamic programming}

\subsection{Grady algorithm}

\section{Short summary} 
summary

\begin{thebibliography}{99}

\bibitem{TAOCP}
Donald E. Knuth. ``The Art of Computer Programming, Volume 3: Sorting and Searching (2nd Edition)''. Addison-Wesley Professional; 2 edition (May 4, 1998) ISBN-10: 0201896850 ISBN-13: 978-0201896855

\bibitem{CLRS}
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein. 
``Introduction to Algorithms, Second Edition''. ISBN:0262032937. The MIT Press. 2001

\bibitem{median-of-median}
M. Blum, R.W. Floyd, V. Pratt, R. Rivest and R. Tarjan, "Time bounds for selection," J. Comput. System Sci. 7 (1973) 448-461.

\bibitem{programming-pearls}
Jon Bentley. ``Programming pearls, Second Edition''. Addison-Wesley Professional; 1999. ISBN-13: 978-0201657883

\end{thebibliography}

\ifx\wholebook\relax\else
\end{document}
\fi

